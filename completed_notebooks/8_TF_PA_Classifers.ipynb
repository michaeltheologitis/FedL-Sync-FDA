{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996933fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45873535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 10000\n",
    "d = 5\n",
    "noise_factor = 0.\n",
    "\n",
    "# Create (noisy) testing data for binary classification.\n",
    "X, y = make_classification(\n",
    "    n_samples=n, \n",
    "    n_features=d,\n",
    "    n_informative=d,\n",
    "    n_redundant=0, \n",
    "    n_classes=2,\n",
    "    class_sep=-1,\n",
    "    flip_y=noise_factor\n",
    ")\n",
    "\n",
    "# We will work with label values -1, +1 and not 0, +1 (convert)\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc3d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the data to TensorFlow tensors\n",
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99a6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the X and y tensors into a single dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train_tensor))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_tensor, y_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963ea1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLIENTS = 10\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 5\n",
    "SHUFFLE_BUFFER = 1000\n",
    "PREFETCH_BUFFER = 32\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da7bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the training dataset\n",
    "preprocessed_train_dataset = preprocess(train_dataset)\n",
    "\n",
    "# Preprocess the testing dataset\n",
    "preprocessed_test_dataset = preprocess(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91d808",
   "metadata": {},
   "source": [
    "## PA-Classiers (binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e391eb",
   "metadata": {},
   "source": [
    "![PA](images/PA_binary_classifiers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67db5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(model, batch, C=0.01):\n",
    "    \n",
    "    @tf.function\n",
    "    def t_pa1(x_batch, loss_batch, C):\n",
    "        # shape=(batchsize,1) where each instance is ||x||^2, x in x_batch\n",
    "        norm_batch = tf.expand_dims(tf.reduce_sum(tf.square(x_batch), axis=1), axis=1)\n",
    "\n",
    "        # PA-1 : Learning rate t for each instance x, with shape=(batchsize,1)\n",
    "        t_batch = tf.maximum(C, tf.divide(loss_batch, norm_batch))\n",
    "\n",
    "        return t_batch\n",
    "    \n",
    "    @tf.function\n",
    "    def t_pa2(x_batch, loss_batch, C):\n",
    "        # shape=(batchsize,1) where each instance is ||x||^2, x in x_batch\n",
    "        norm_batch = tf.expand_dims(tf.reduce_sum(tf.square(x_batch), axis=1), axis=1)\n",
    "        \n",
    "        # PA-2 : Learning rate t for each instance x, with shape=(batchsize,1)\n",
    "        t_batch = tf.divide(loss_batch, tf.add(0.5/C, norm_batch))\n",
    "        \n",
    "        return t_batch\n",
    "    \n",
    "    x_batch, y_batch = batch\n",
    "\n",
    "    # from shape (d,) make it (d,1)\n",
    "    y_batch = tf.expand_dims(y_batch, axis=1)\n",
    "\n",
    "    # dot(w, x) for the batch (each instance of x in x_batch) with with shape=(batchsize, 1)\n",
    "    weights_dot_x_batch = tf.matmul(x_batch, model)\n",
    "\n",
    "    # Prediction batch with shape=(batchsize, 1)\n",
    "    y_pred_batch = tf.sign(weights_dot_x_batch)\n",
    "\n",
    "    # Suffer loss for each prediction (of instance) in the batch with shape=(batchsize,1)\n",
    "    loss_batch = tf.maximum(0., 1. - tf.multiply(y_batch, weights_dot_x_batch))\n",
    "\n",
    "    # PA-1 : Learning rate t for each instance x, with shape=(batchsize,1)\n",
    "    t_batch = t_pa1(x_batch, loss_batch, C)\n",
    "\n",
    "    # each instance is y*t*x, where y,t scalars and x in x_batch. shape=(batchsize,d)\n",
    "    t_y_x_batch = tf.multiply(t_batch, tf.multiply(y_batch, x_batch))\n",
    "    \n",
    "    # !!!! Update with mean t*y*x\n",
    "    t_y_x_update = tf.expand_dims(tf.reduce_mean(t_y_x_batch, axis=0) ,axis=1)\n",
    "\n",
    "    # Update\n",
    "    model.assign_add(t_y_x_update)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357153c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef87299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 -r 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.Variable(tf.zeros(shape=(d, 1)), trainable=True, name='weights', dtype=tf.float32)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in iter(preprocessed_train_dataset):\n",
    "        train_on_batch(model, batch, C=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e50d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the accuracy accumulator\n",
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "# Evaluate the model on each batch of the preprocessed testing dataset\n",
    "for batch in iter(preprocessed_test_dataset):\n",
    "    x_batch, y_batch = batch\n",
    "\n",
    "    # from shape (d,) make it (d,1)\n",
    "    y_batch = tf.expand_dims(y_batch, axis=1)\n",
    "\n",
    "    # Compute the predicted labels for the batch using the trained model\n",
    "    predictions = tf.sign(tf.matmul(x_batch, model))\n",
    "\n",
    "    test_accuracy.update_state(y_batch, predictions)\n",
    "\n",
    "# Compute the overall accuracy\n",
    "print(f'Test accuracy: {test_accuracy.result().numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
