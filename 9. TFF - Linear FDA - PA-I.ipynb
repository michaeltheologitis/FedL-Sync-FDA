{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16945920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5666e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.50.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db373c",
   "metadata": {},
   "source": [
    "## Create Binary Classification data with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8936c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 100000 \n",
    "d = 20 \n",
    "noise_factor = 0.05\n",
    "test_size = 0.1 # % of n\n",
    "\n",
    "seed = 2\n",
    "\n",
    "# Create (noisy) testing data for binary classification.\n",
    "X, y = make_classification(\n",
    "    n_samples=n, \n",
    "    n_features=d,\n",
    "    n_informative=d,\n",
    "    n_redundant=0, \n",
    "    n_classes=2,\n",
    "    class_sep=-1,\n",
    "    flip_y=noise_factor,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# We will work with label values -1, +1 and not 0, +1 (convert)\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180164a",
   "metadata": {},
   "source": [
    "## Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d1b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the data to TensorFlow tensors\n",
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc001d5",
   "metadata": {},
   "source": [
    "## Prepare data for Tensorflow Federated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2921d7d",
   "metadata": {},
   "source": [
    "We have the training and testing Tensors holding our data. TFF expects for each client an `OrderedDict` containing `y` and `x` data. Hence, we preprocess our Tensors to follow this convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e2091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLIENTS = 3\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = int(n / NUM_CLIENTS)\n",
    "BATCHES_PER_STEP = 1 # Batches per Step, i.e, How many batches until we check RTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb39a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches per client: 1041\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of batches per client: {int(n / (NUM_CLIENTS*BATCH_SIZE))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5c09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "\n",
    "# Create a dictionary with the slices for each client\n",
    "client_slices_train = {}\n",
    "slices_test = {}\n",
    "\n",
    "n_test = int(n - n*test_size)\n",
    "\n",
    "for i in range(NUM_CLIENTS):\n",
    "    # Compute the indices for this client's slice\n",
    "    start_idx = int(i * n_test / NUM_CLIENTS)\n",
    "    end_idx = int((i + 1) * n_test / NUM_CLIENTS)\n",
    "\n",
    "    # Get the slice for this client\n",
    "    X_client_train = X_train_tensor[start_idx:end_idx]\n",
    "    y_client_train = y_train_tensor[start_idx:end_idx]\n",
    "    \n",
    "    client_data_train = collections.OrderedDict([('y', y_client_train), ('x', X_client_train)])\n",
    "    \n",
    "    # Combine the slices into a single dataset\n",
    "    client_slices_train[f'client_{i}'] = client_data_train\n",
    "\n",
    "slices_test = collections.OrderedDict([('y', y_test_tensor), ('x', X_test_tensor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c178a1f",
   "metadata": {},
   "source": [
    "For a sanity check let's see inside `client_slices_train` for the first x,y tuple of the 'first' client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f169ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([ 2.8520453 ,  4.1043606 ,  0.72331977,  0.630671  ,  1.0472858 ,\n",
       "       -2.4497042 , -2.0924962 , -2.9751914 , -2.8719494 ,  0.63433236,\n",
       "        1.8827676 ,  3.525443  , -1.7605263 , -2.8830485 ,  2.4992065 ,\n",
       "        1.4187863 ,  5.275925  ,  0.7632305 , -3.8827038 , -0.09312054],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_slices_train['client_0']['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e19090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_slices_train['client_0']['y'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192acbc",
   "metadata": {},
   "source": [
    "Now, a client with `client_id` has it's single Tensor holding instances in`client_slices_train[client_id]['x']` and labels in `client_slices_train[client_id]['y']`. Let's take a step back from TFF. Having this data scheme, we can create a client's Tensorflow dataset using `from_tensor_slices` function passing the client's id as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c683cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
    "\n",
    "def create_tf_dataset_for_client(client_id):\n",
    "    return tf.data.Dataset.from_tensor_slices(client_slices_train[client_id]) \\\n",
    "        .shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) \\\n",
    "        .take(BATCHES_PER_STEP)\n",
    "\n",
    "def create_tf_dataset_for_test():\n",
    "    return tf.data.Dataset.from_tensor_slices(slices_test).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef737537",
   "metadata": {},
   "source": [
    "For TFF we need to construct Federated data for clients, i.e., `tff.simulation.datasets.ClientData`. We can use the `from_clients_and_tf_fn` function that takes as argument the `client_ids` : a list of strings corresponding to client ids, and a `serializable_dataset_fn` : a function that takes a `client_id` from the above list, and returns a `tf.data.Dataset`. It's obvious how we proceed with the code (using the above function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86b6c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_train_federated_dataset = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "    client_ids=list(client_slices_train.keys()),\n",
    "    serializable_dataset_fn=lambda client_id: create_tf_dataset_for_client(client_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1acb22b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client_0', 'client_1', 'client_2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_federated_dataset.client_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a74b7",
   "metadata": {},
   "source": [
    "**Note**: Cross-device federated learning does not use client IDs or perform any tracking of clients. However in simulation experiments using centralized test data the experimenter may select specific clients to be processed per round. The concept of a client ID is only available at the preprocessing stage when preparing input data for the simulation and is not part of the TensorFlow Federated core APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde8820",
   "metadata": {},
   "source": [
    "Now, `preprocessed_train_federated_dataset` holds logic on how each client constructs its dataset. Note that so `client_slices_train` has already been materialized and lies in this context's memory.\n",
    "\n",
    "One way (the simplest) to feed federated data to TFF in a simulation is simply as a Python list, with each element of the list holds the data of an individual client, whether as a list or preferably as a `tf.data.Dataset`. Since we already created an interface that provides the latter we will use it. Here is a helper function that will construct a list of datasets from the set of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c68b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_federated_data():    \n",
    "    return [\n",
    "        preprocessed_train_federated_dataset.create_tf_dataset_for_client(client)\n",
    "        for client in preprocessed_train_federated_dataset.client_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f39ef",
   "metadata": {},
   "source": [
    "**Important Note**: Firstly, we used `sklearn` to create the binary classification data eagerly, i.e., we were forced to materialize it into memory. In simulation, in general it is more sound to push preprocessing logic into each client, i.e., each client constructs its own dataset (from the same underlying distribution) or reads from a file or something else and he, himself processes the data as needed. This is the best approach and uses the TFF distributed engine the best way. But in our case this was illogical to happen since we are forced to construct the dataset in memory anyway. For example, we could have stored each client's data inside some serialized file (`client_0.tfrecord` for the first client and so on) and push logic where each clients diserializes and processes its own data but this would be silly and slower when testing. For a small example that showcases this scenario see *TFF - Introduction - Federated Core API - Part 3(examples).ipynb*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b3cc5",
   "metadata": {},
   "source": [
    "## TFF Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f51dd",
   "metadata": {},
   "source": [
    "Let's start with a simple float32 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3016a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT32_TYPE = tff.TensorType(dtype=tf.float32, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86f60f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(FLOAT32_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "090749e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_FLOAT32 = tff.FederatedType(FLOAT32_TYPE, tff.CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72cbef65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{float32}@CLIENTS'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CLIENT_FLOAT32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f7d8f",
   "metadata": {},
   "source": [
    "1-dimensional tensor (vector) of length 1 with elements of type float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b813ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT32_VECTOR_TYPE = tff.TensorType(dtype=tf.float32, shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fdf9a8",
   "metadata": {},
   "source": [
    "The local client state $ S_i(t) $ as defined in the unpublished paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "168f7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_STATE = tff.FederatedType(FLOAT32_VECTOR_TYPE, tff.CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e006455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{float32[1]}@CLIENTS'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CLIENT_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3c683",
   "metadata": {},
   "source": [
    "First, let's define the type of input as a TFF named tuple. Since the size of data batches may vary, we set the batch dimension to None to indicate that the size of this dimension is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47b88ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SPEC = collections.OrderedDict(\n",
    "    y=tf.TensorSpec(shape=[None], dtype=tf.float32),\n",
    "    x=tf.TensorSpec(shape=[None, d], dtype=tf.float32)\n",
    ")\n",
    "BATCH_TYPE = tff.to_type(BATCH_SPEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d69a37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<y=float32[?],x=float32[?,20]>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(BATCH_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a844eba",
   "metadata": {},
   "source": [
    "Every client holds a sequence of batches so the we define the client data type as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bd7c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOCAL_DATA_TYPE = tff.SequenceType(BATCH_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f9890c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<y=float32[?],x=float32[?,20]>*'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(LOCAL_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dedbfb",
   "metadata": {},
   "source": [
    "Let's now define the TFF type of the model which is simply a `tf.Variable` with shape (d, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1625384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_TYPE = tff.TensorType(dtype=tf.float32, shape=(d, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4db97c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32[20,1]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbc608",
   "metadata": {},
   "source": [
    "Since the server holds the 'global' model we need to create the Federated Type, defined as the tuple of a member: An instance of `tff.Type`, and a placement: The specification of placement of the member comonents (where this type is hosted at, for example, at `tff.SERVER` or `tff.CLIENTS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93453747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SERVER_MODEL_TYPE = tff.type_at_server(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "424c7703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32[20,1]@SERVER'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(SERVER_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd0413",
   "metadata": {},
   "source": [
    "Following, the same logic, we create the Federated Type of each client's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5a56be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLIENT_DATA_TYPE = tff.type_at_clients(LOCAL_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cc37383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{<y=float32[?],x=float32[?,20]>*}@CLIENTS'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CLIENT_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe9341",
   "metadata": {},
   "source": [
    "We will also need to define the client models at the CLIENTS (for FDA later, to be cont...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dd51ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_MODEL_TYPE = tff.type_at_clients(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47df0ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{float32[20,1]}@CLIENTS'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CLIENT_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af966b0",
   "metadata": {},
   "source": [
    "## Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2427e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def accuracy(model, dataset):\n",
    "    \n",
    "    @tf.function\n",
    "    def _batch_accuracy(model, batch):\n",
    "        x_batch, y_batch = batch['x'], tf.expand_dims(batch['y'], axis=1)\n",
    "\n",
    "        # dot(w, x) for the batch (each instance of x in x_batch) with with shape=(batchsize, 1)\n",
    "        weights_dot_x_batch = tf.matmul(x_batch, model)\n",
    "\n",
    "        # Prediction batch with shape=(batchsize, 1)\n",
    "        y_pred_batch = tf.sign(weights_dot_x_batch)\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_batch, y_batch), tf.float32))\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    # We take advantage of AutoGraph (convert Python code to TensorFlow-compatible graph code automatically)\n",
    "    acc, num_batches = 0., 0.\n",
    "    for batch in dataset:\n",
    "        acc += _batch_accuracy(model, batch)\n",
    "        num_batches += 1\n",
    "        \n",
    "    acc = acc / num_batches\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7db2f5a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@tff.tf_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
    "def accuracy_fn(model, dataset):\n",
    "    model = tf.Variable(initial_value=model)\n",
    "    return accuracy(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecea1204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<model=float32[20,1],dataset=<y=float32[?],x=float32[?,20]>*> -> float32)'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(accuracy_fn.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791b516",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379134b",
   "metadata": {},
   "source": [
    "### Server Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b4f4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.tf_computation(MODEL_TYPE)\n",
    "def server_update_fn(clients_aggr_model):\n",
    "    model = tf.Variable(initial_value=clients_aggr_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74689db",
   "metadata": {},
   "source": [
    "**Note**: This abstraction for this simple jupyter (where the model is a `tf.Variable`) is not necessary. We create this abstraction since it is common practice generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e2160",
   "metadata": {},
   "source": [
    "### Client train - PA-I Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f3b8a",
   "metadata": {},
   "source": [
    "![PA](images/PA_binary_classifiers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5a9fb",
   "metadata": {},
   "source": [
    "Each client trains on its own dataset (which is a sequence of batches). Hence, we create the training process, currently a PA-1 Classifier. The input of `client_train` is the client model materialized inside its client and its dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32494451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def client_train(model, C, dataset):\n",
    "    \n",
    "    @tf.function\n",
    "    def _train_on_batch(model, C, batch):\n",
    "\n",
    "        x_batch, y_batch = batch['x'], tf.expand_dims(batch['y'], axis=1)\n",
    "\n",
    "        # dot(w, x) for the batch (each instance of x in x_batch) with with shape=(batchsize, 1)\n",
    "        weights_dot_x_batch = tf.matmul(x_batch, model)\n",
    "\n",
    "        # Prediction batch with shape=(batchsize, 1)\n",
    "        y_pred_batch = tf.sign(weights_dot_x_batch)\n",
    "\n",
    "        # Suffer loss for each prediction (of instance) in the batch with shape=(batchsize,1)\n",
    "        loss_batch = tf.maximum(0., 1. - tf.multiply(y_batch, weights_dot_x_batch))\n",
    "\n",
    "        # shape=(batchsize,1) where each instance is ||x||^2, x in x_batch\n",
    "        norm_batch = tf.expand_dims(tf.reduce_sum(tf.square(x_batch), axis=1), axis=1)\n",
    "        \n",
    "        # PA-1 : Learning rate t for each instance x, with shape=(batchsize,1)\n",
    "        t_batch = tf.maximum(C, tf.divide(loss_batch, norm_batch))\n",
    "\n",
    "        # each instance is y*t*x, where y,t scalars and x in x_batch. shape=(batchsize,d)\n",
    "        t_y_x_batch = tf.multiply(t_batch, tf.multiply(y_batch, x_batch))\n",
    "\n",
    "        # !!!! Update with mean t*y*x\n",
    "        t_y_x_update = tf.expand_dims(tf.reduce_mean(t_y_x_batch, axis=0) ,axis=1)\n",
    "\n",
    "        # Update\n",
    "        model.assign_add(t_y_x_update)\n",
    "    \n",
    "    for batch in dataset:\n",
    "        _train_on_batch(model, C, batch)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32448c2f",
   "metadata": {},
   "source": [
    "# Functional Dynamic Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e6a7f",
   "metadata": {},
   "source": [
    "We follow the Functional Dynamic Averaging (FDA) scheme. Let the mean model be\n",
    "\n",
    "$$ \\overline{w_t} = \\frac{1}{k} \\sum_{i=1}^{k} w_t^{(i)} $$\n",
    "\n",
    "where $ w_t^{(i)} $ is the model at time $ t $ in some round in the $i$-th learner.\n",
    "\n",
    "Local models are trained independently and cooperatively and we want to monitor the Round Terminating Conditon (**RTC**):\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2  \\leq \\Theta $$\n",
    "\n",
    "where the left-hand side is the **model variance**, and threshold $\\Theta$ is a hyperparameter of the FDA, defined at the beginning of the round; it may change at each round. When the monitoring logic cannot guarantee the validity of RTC, the round terminates. All local models are pulled into `tff.SERVER`, and $\\bar{w_t}$ is set to their average. Then, another round begins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaca834",
   "metadata": {},
   "source": [
    "### Monitoring the RTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62f59b",
   "metadata": {},
   "source": [
    "FDA monitors the RTC by applying techniques from Functionary [Functional Geometric Averaging](http://users.softnet.tuc.gr/~minos/Papers/edbt19.pdf). We first restate the problem of monitoring RTC into the standard distributed stream monitoring formulation. Let\n",
    "\n",
    "$$ S(t) =  \\frac{1}{k} \\sum_{i=1}^{k} S_i(t) $$\n",
    "\n",
    "where $ S(t) \\in \\mathbb{R}^n $ be the \"global state\" of the system and $ S_i(t) \\in \\mathbb{R}^n $ the \"local states\". The goal is to monitor the threshold condition on the global state in the form $ F(S(t)) \\leq \\Theta $ where $ F : \\mathbb{R}^n \\to \\mathbb{R} $ a non-linear function. Let\n",
    "\n",
    "$$ \\Delta_t^{(i)} = w_t^{(i)} - w_{t_0}^{(i)} $$\n",
    "\n",
    "be the update at the $ i $-th learner, that is, the change to the local model at time $t$ since the beginning of the current round at time $t_0$. Let the average update be\n",
    "\n",
    "$$ \\overline{\\Delta_t} = \\frac{1}{k} \\sum_{i=1}^{k} \\Delta_t^{(i)} $$\n",
    "\n",
    "it follows that the variance can be written as\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2 = \\Big( \\frac{1}{k} \\sum_{i=1}^{k} \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\Big) - \\lVert \\overline{\\Delta_t} \\rVert_2^2 $$\n",
    "\n",
    "So, conceptually, if we define\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\bf{x}\n",
    "         \\end{bmatrix}) = v - \\lVert \\bf{x} \\rVert_2^2 $$\n",
    "\n",
    "The RTC is equivalent to condition $$ F(S(t)) \\leq \\Theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01137fad",
   "metadata": {},
   "source": [
    "## 2️⃣ Linear FDA\n",
    "\n",
    "In the linear case, we reduce the update vector to a scalar, $ \\xi \\Delta_t^{(i)} \\in \\mathbb{R}$, where $ \\xi $ is any unit vector.\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\xi \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^2 $$\n",
    "\n",
    "Also, define \n",
    "\n",
    "$$ F(v, x) = v - x^2 $$\n",
    "\n",
    "The RTC is equivalent to condition \n",
    "\n",
    "$$ F(S(t)) \\leq \\Theta $$\n",
    "\n",
    "A random choice of $ \\xi $ is likely to perform poorly (terminate round prematurely), as it wil likely be close to orthogonal to $ \\overline{\\Delta_t} $. A good choice would be a vector $ \\xi $ correlated to $ \\overline{\\Delta_t} $. A heuristic choice is to take $ \\overline{\\Delta_{t_0}} $ (after scaling it to norm 1), i.e., the update vector right before the current round started. All nodes can estimate this without communication, as $ \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} $, the difference of the last two models pushed by the Server. Hence, \n",
    "\n",
    "$$ \\xi = \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7c19cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.tf_computation(MODEL_TYPE, MODEL_TYPE)\n",
    "def ksi_unit_fn(w_t, w_tminus1):\n",
    "    \n",
    "    @tf.function\n",
    "    def _ksi_unit(w_t, w_tminus1):\n",
    "        if tf.reduce_all(tf.equal(w_t, w_tminus1)):\n",
    "            # if equal then ksi becomes a random vector (will only happen in round 1)\n",
    "            ksi = tf.random.normal(shape=w_t.shape)\n",
    "        else:\n",
    "            ksi = w_t - w_tminus1\n",
    "\n",
    "        # Normalize and return\n",
    "        return tf.divide(ksi, tf.norm(ksi))\n",
    "    \n",
    "    return _ksi_unit(w_t, w_tminus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0069b1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<w_t=float32[20,1],w_tminus1=float32[20,1]> -> float32[20,1])'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ksi_unit_fn.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4743f",
   "metadata": {},
   "source": [
    "Using the functions decorated with `tf.function` (context inside Tensorflow) we create the `client_train_fn` with context inside TFF. \n",
    "\n",
    "`initial_model` is the model currently inside each `tff.CLIENT`. This model is different in each CLIENT with the exception in the first step after synchronization.\n",
    "\n",
    "`last_sync_model` is the synchronized model at the start of the current round. \n",
    "\n",
    "`last_last_sync_model` is the synchronized model at the start of the previous round (used for the heuristic for $ \\xi $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "888be5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.tf_computation(MODEL_TYPE, MODEL_TYPE, MODEL_TYPE, FLOAT32_TYPE, LOCAL_DATA_TYPE)\n",
    "def client_train_fn(last_last_sync_model, last_sync_model, initial_model, C, dataset):\n",
    "    \n",
    "    model = client_train(\n",
    "        tf.Variable(initial_value=initial_model), C, dataset\n",
    "    )\n",
    "    \n",
    "    Delta_i = model - last_sync_model # AutoGraph\n",
    "    \n",
    "    #||D(t)_i||^2 , shape = (1,) \n",
    "    Delta_i_norm_squared = tf.reduce_sum(tf.square(Delta_i), axis=0) \n",
    "    \n",
    "    # heuristic unit vector ksi\n",
    "    ksi = ksi_unit_fn(last_sync_model, last_last_sync_model)\n",
    "    \n",
    "    # ksi * Delta_i (* is dot) , shape = ()\n",
    "    ksi_Delta_i = tf.reduce_sum(tf.multiply(ksi, Delta_i))\n",
    "    # shape = (1,)\n",
    "    ksi_Delta_i = tf.expand_dims(ksi_Delta_i, axis=0)\n",
    "    \n",
    "    # shape = (2,)\n",
    "    S_i = tf.concat([Delta_i_norm_squared, ksi_Delta_i], axis=0)\n",
    "    # shape = (2,1)\n",
    "    S_i = tf.reshape(S_i, (2, 1))\n",
    "\n",
    "    return model, S_i, tf.reduce_sum(tf.square(ksi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "349b9ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<last_last_sync_model=float32[20,1],last_sync_model=float32[20,1],initial_model=float32[20,1],C=float32,dataset=<y=float32[?],x=float32[?,20]>*> -> <float32[20,1],float32[2,1]>)'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(client_train_fn.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d317f2",
   "metadata": {},
   "source": [
    "### HEREEEEEEEEEEEEEEEEEEEE (CORRECT ABOVE ALL) Server Average Client Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e18e0",
   "metadata": {},
   "source": [
    "When it is time to synchronize the Clients, the Server averages the Client weights and computes the global model. This is what this function does. Moreover, remember that the Client updates its model using `server_update_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc7bdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation(CLIENT_MODEL_TYPE)\n",
    "def server_update(client_models):\n",
    "    # 4. Compute the mean of the client weights\n",
    "    mean_client_model = tff.federated_mean(client_models)\n",
    "    \n",
    "    # 4. Update the server model\n",
    "    server_model = tff.federated_map(server_update_fn, mean_client_model)\n",
    "    \n",
    "    return server_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61a2a0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'({float32[20,1]}@CLIENTS -> float32[20,1]@SERVER)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(server_update.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6455e",
   "metadata": {},
   "source": [
    "### Client Learning Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556998dc",
   "metadata": {},
   "source": [
    "A Client learning step is an update of the model based on one or more batches. The following `federated_computation` describes the step operation over all the clients. We pass a `federated_dataset` which could be thought as a stream of one or more batches for each client and some more parameters, namely, the last synchronized global model, the current client models, and the parameter `C` of the **PA-I** classifier.\n",
    "\n",
    "Notice that each of those parameters is placed in `tff.CLIENTS`. \n",
    "\n",
    "Moreover, we return the updated Client models `client_models` (think of this as the new state of the distributed system) placed in `tff.CLIENTS` aswell which is logical, each Client updates its own model. Lastly, we return the `client_S_i` as described in the unpublished manuscript ('local state') of each Client, again placed in `tff.CLIENTS`.\n",
    "\n",
    "Note: Do not think of the `return` as a normal programming `return` statement. Here, we describe the change in state in the distributed system, in this case, solely in `tff.CLIENTS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b758bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation(CLIENT_MODEL_TYPE, CLIENT_MODEL_TYPE, CLIENT_FLOAT32, CLIENT_DATA_TYPE)\n",
    "def step(last_sync_client_models, client_models, client_C, federated_dataset):\n",
    "    # 2. 3. Train the client models on their respective datasets\n",
    "    client_models, client_S_i = tff.federated_map(\n",
    "        client_train_fn, \n",
    "        (last_sync_client_models, client_models, client_C, federated_dataset)\n",
    "    )\n",
    "    \n",
    "    return client_models, client_S_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08b11ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<last_sync_client_models={float32[20,1]}@CLIENTS,client_models={float32[20,1]}@CLIENTS,client_C={float32}@CLIENTS,federated_dataset={<y=float32[?],x=float32[?,20]>*}@CLIENTS> -> <{float32[20,1]}@CLIENTS,{float32[1]}@CLIENTS>)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(step.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c0b3b",
   "metadata": {},
   "source": [
    "### Server Computation of 'Global State'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787e99a",
   "metadata": {},
   "source": [
    "As you saw above, each Client has a property 'local state' `client_S_i`. When each round ends, the server should average those local states to compute the 'global state', i.e., the approximation of the **variance** using the *Naive FDA* scheme. The following `federated_computation` describes exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f818b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.federated_computation(CLIENT_STATE)\n",
    "def server_global_state(client_S_i):\n",
    "    \n",
    "    server_S = tff.federated_mean(client_S_i)\n",
    "    \n",
    "    return server_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63f3a7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'({float32[1]}@CLIENTS -> float32[1]@SERVER)'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(server_global_state.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f9f7e",
   "metadata": {},
   "source": [
    "### Round Terminating Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d158204",
   "metadata": {},
   "source": [
    "As explained in the theoretical analysis of the *Naive FDA*, when the approximation of **RTC** does not hold, i.e., \n",
    "\n",
    "$$ F(S(t)) \\gt \\Theta $$\n",
    "\n",
    "we are oblidged to synchronize the Client models since we can no longer guarantee that the variance is bellow the $\\Theta$ threshold, i.e., \n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2  \\leq \\Theta $$\n",
    "\n",
    "The following function checks whether we guarantee that the **RTC** holds (`True`) or not (`False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a06e446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for all FDA. (bool)\n",
    "@tff.tf_computation(FLOAT32_VECTOR_TYPE, FLOAT32_TYPE)\n",
    "def RTC_holds(S_t, THETA):\n",
    "    \"\"\" Returns True if RTC holds (has not been defied). False otherwise (sync must happen)\"\"\"\n",
    "    \n",
    "    @tf.function\n",
    "    def _F(S_t, THETA):\n",
    "        \"\"\" Naive FDA \"\"\"\n",
    "        return S_t <= THETA\n",
    "    \n",
    "    return _F(S_t, THETA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb04701e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<S_t=float32[1],THETA=float32> -> bool[1])'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(RTC_holds.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c5f35",
   "metadata": {},
   "source": [
    "### Help variance function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e382606",
   "metadata": {},
   "source": [
    "A helper function that computes the **Actual Variance** when the approximate **RTC** condition does not hold. We want to see how far off the approximation is from reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eec4adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_spec = tf.TensorSpec(shape=(NUM_CLIENTS, d, 1), dtype=tf.float32)\n",
    "\n",
    "@tf.function(input_signature=[w_spec, w_spec])\n",
    "def variance(w_t, w_sync):\n",
    "    # w_t , w_sync tensors with shape=(NUM_CLIENTS, d, 1)\n",
    "    \n",
    "    # tensor with shape=(NUM_CLIENTS, d, 1)\n",
    "    diff = w_t - w_sync\n",
    "    \n",
    "    # tensor with shape=(NUM_CLIENTS, 1) , For each client ||w_i_t - w_t||^2\n",
    "    dot = tf.reduce_sum(tf.square(diff), axis=1)\n",
    "    \n",
    "    # Variance shape=() , scalar\n",
    "    var = tf.reduce_mean(dot)\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51953c",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a75dc",
   "metadata": {},
   "source": [
    "A very ugly looking `Metrics` class that helps us with printing and storing the round metrics.\n",
    "\n",
    "Skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc049cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, NUM_ROUNDS, THETA, NUM_CLIENTS, BATCHES_PER_STEP, BATCH_SIZE, n, d, test_size):\n",
    "        self.NUM_ROUNDS = NUM_ROUNDS\n",
    "        self.THETA = THETA\n",
    "        self.NUM_CLIENTS = NUM_CLIENTS\n",
    "        self.BATCHES_PER_STEP = BATCHES_PER_STEP\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        self.total_batches_per_client = int(self.n / (self.NUM_CLIENTS*self.BATCH_SIZE))\n",
    "        \n",
    "        self.one_sample_size_b = (self.d+2)*4 # bytes\n",
    "        \n",
    "        self.training_dataset_size_mb = self.one_sample_size_b * (n * (1-test_size)) / 1_000_000 # In mb\n",
    "        \n",
    "        # Total batches for all clients for a single step\n",
    "        self.total_batches_per_step = (self.BATCHES_PER_STEP * self.NUM_CLIENTS)\n",
    "        \n",
    "        self.samples = int(self.n * (1-self.test_size))\n",
    "        \n",
    "        self.all_metrics = []\n",
    "        \n",
    "    \n",
    "    def print_initial_information(self):\n",
    "        print(\"FEDERATED SETTING INFO:\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "\n",
    "        print(\"CLIENTS:\")\n",
    "        print(f'{\"Clients\":<10} {\"Batches per Client\":<20} {\"Batches per Step\":<20}')\n",
    "        print(f'{self.NUM_CLIENTS:<10} {self.total_batches_per_client:<20} {self.BATCHES_PER_STEP:<20}')\n",
    "        print()\n",
    "\n",
    "        print(\"TRAIN DATASET:\")\n",
    "        print(f'{\"x-Dim\":<6} {\"y-classes\":<10} {\"Samples\":<12} {\"Dataset size (MB)\":<20} {\"Samples per Batch\":<20}')\n",
    "        print(f'{d:<6} {2:<10} {int(n * (1-test_size)):<12} {self.training_dataset_size_mb:<20} {self.BATCH_SIZE:<20}')\n",
    "        print()\n",
    "\n",
    "        print(\"ALGORITHM:\")\n",
    "        print(f'{\"Name\":<5} {\"Model Bytes\":<10}')\n",
    "        print(f'{\"PA-I\":<5} {self.d*4:<10}')\n",
    "        print()\n",
    "\n",
    "        print(\"SYNCHRONIZATION:\")\n",
    "        print('Naive FDA')\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        print()\n",
    "        print()   \n",
    "    \n",
    "    def store_and_print_metrics(self, num_round, num_steps, global_state, accuracy, C, var):\n",
    "        metrics = {}\n",
    "\n",
    "        metrics['Round'] = num_round\n",
    "        metrics['Steps'] = num_steps\n",
    "        metrics['Accuracy'] = accuracy\n",
    "        metrics['Global State'] = global_state[0]\n",
    "        metrics['C'] = C\n",
    "        metrics['Actual Variance'] = var\n",
    "\n",
    "        # Total samples seen by all clients. BATCH_SIZE = samples per batch\n",
    "        metrics['Samples'] = self.BATCH_SIZE * (num_steps * self.total_batches_per_step)\n",
    "\n",
    "        # FDA In each step clients return their S_i (4 bytes)\n",
    "        local_states_bytes = num_steps * (self.NUM_CLIENTS * 4)\n",
    "        # Synchronization: Send model to all clients\n",
    "        sync_bytes = self.d * self.NUM_CLIENTS * 4\n",
    "\n",
    "        metrics['Bytes Exchanged'] = local_states_bytes + sync_bytes\n",
    "\n",
    "        self.all_metrics.append(metrics)\n",
    "\n",
    "        # Print the metrics for the current round\n",
    "        self.print_round_metrics()\n",
    "        \n",
    "    def print_round_metrics(self):\n",
    "        metrics = self.all_metrics[-1]\n",
    "        \n",
    "        # Print the metric values in a nicely formatted table\n",
    "        print(f'{\"Round\":<6} {\"Steps\":<6} {\"C\":<5} {\"Accuracy\":<13} {\"Bytes Exchanged\":<20} {\"Samples\":<15} {\"Var Approx\":<15} {\"Var (Actual)\":<15}')\n",
    "\n",
    "        print(f\"{metrics['Round']:<6} {metrics['Steps']:<6} {metrics['C']:<5} {metrics['Accuracy']:<13.5f} {metrics['Bytes Exchanged']:<20} {metrics['Samples']:<15} {metrics['Global State']:<15.6f} {metrics['Actual Variance']:<15.6f}\")\n",
    "        print()\n",
    "    \n",
    "    def print_aggregate_metrics(self):\n",
    "\n",
    "        total_bytes_exchanged = sum(metrics['Bytes Exchanged'] for metrics in self.all_metrics)\n",
    "        total_steps = sum(metrics['Steps'] for metrics in self.all_metrics)\n",
    "        total_samples = sum(metrics['Samples'] for metrics in self.all_metrics)\n",
    "        final_accuracy = self.all_metrics[-1]['Accuracy']\n",
    "\n",
    "        # Remember we pass the dataset many times at random (random batches)\n",
    "        trained_in_size = self.one_sample_size_b * total_samples / 1_000_000 # MB\n",
    "\n",
    "        print()\n",
    "        print('FINAL METRICS:')\n",
    "        print()\n",
    "\n",
    "        # Print the metric values in a nicely formatted table\n",
    "        print(f'{\"Rounds\":<6} {\"Steps\":<8} {\"Samples\":<13} {\"MB Exchanged\":<17} {\"Accuracy\":<13} {\"Trained MB\":<15}')\n",
    "\n",
    "        print(f'{self.NUM_ROUNDS:<6} {total_steps:<8} {total_samples:<13} {total_bytes_exchanged/1_000_000:<17} {final_accuracy:<13.6} {trained_in_size:<20.5f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05037e",
   "metadata": {},
   "source": [
    "## Training Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09197d61",
   "metadata": {},
   "source": [
    "Create the federated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "933033bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_federated_data = create_federated_data()\n",
    "test_dataset = create_tf_dataset_for_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9949269",
   "metadata": {},
   "source": [
    "Initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e2b3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 10\n",
    "THETA = 1.\n",
    "C = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec3b1e",
   "metadata": {},
   "source": [
    "We assume that all Clients start in synchronization, i.e., Server model is zeros and Client models are also zeros.\n",
    "\n",
    "Moreover, notice that `client_models`, `last_sync_client_models`, `client_C` are all defined as lists containing `NUM_CLIENTS` elements. This is the simulation approach of TFF and following the already defined functions above, each element in those lists is assumed to lie in one `tff.CLIENT`. For example, each `tff.CLIENT` has a `C` hyperparameter to be used by **PA-I** classifier on its own model etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cb526fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model of zeros\n",
    "model = tf.Variable(tf.zeros(shape=(d, 1)), trainable=True, name='weights', dtype=tf.float32)\n",
    "\n",
    "# Assume client models are synchronized at the start (Obviously S_t = 0)\n",
    "client_models = [model]*NUM_CLIENTS\n",
    "last_sync_client_models = [model]*NUM_CLIENTS\n",
    "last_last_sync_client_models = [model]*NUM_CLIENTS\n",
    "S_t = [0.]\n",
    "\n",
    "client_C = [C]*NUM_CLIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7248d",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d926259b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEDERATED SETTING INFO:\n",
      "------------------------------------------------------------\n",
      "CLIENTS:\n",
      "Clients    Batches per Client   Batches per Step    \n",
      "3          1041                 1                   \n",
      "\n",
      "TRAIN DATASET:\n",
      "x-Dim  y-classes  Samples      Dataset size (MB)    Samples per Batch   \n",
      "20     2          90000        7.92                 32                  \n",
      "\n",
      "ALGORITHM:\n",
      "Name  Model Bytes\n",
      "PA-I  80        \n",
      "\n",
      "SYNCHRONIZATION:\n",
      "Naive FDA\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "1      1      1     0.77935       252                  96              7.515466        2.153086       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "2      1      1     0.78524       252                  96              13.302562       3.629052       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "3      1      1     0.79423       252                  96              8.732189        2.420318       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "4      1      1     0.80022       252                  96              8.773078        2.559329       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "5      1      1     0.80082       252                  96              8.315991        3.848138       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "6      1      1     0.79862       252                  96              10.924670       4.658895       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "7      1      1     0.79702       252                  96              10.739598       3.871512       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "8      1      1     0.79493       252                  96              6.604635        3.008776       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "9      1      1     0.80122       252                  96              7.414873        3.294093       \n",
      "\n",
      "Round  Steps  C     Accuracy      Bytes Exchanged      Samples         Var Approx      Var (Actual)   \n",
      "10     1      1     0.80062       252                  96              11.303424       3.657010       \n",
      "\n",
      "\n",
      "FINAL METRICS:\n",
      "\n",
      "Rounds Steps    Samples       MB Exchanged      Accuracy      Trained MB     \n",
      "10     10       960           0.00252           0.800619      0.08448             \n"
     ]
    }
   ],
   "source": [
    "metrics = Metrics(NUM_ROUNDS, THETA, NUM_CLIENTS, BATCHES_PER_STEP, BATCH_SIZE, n, d, test_size)\n",
    "metrics.print_initial_information()\n",
    "\n",
    "for r in range(1, NUM_ROUNDS+1):\n",
    "    \n",
    "    num_steps = 0 # Each step() invocation is a step\n",
    "    \n",
    "    while RTC_holds(S_t, THETA): # RTC holds, no sync needed\n",
    "        \n",
    "        # Perform a training step with the current client_models (no sync yet)\n",
    "        client_models, client_S_i = step(\n",
    "            last_sync_client_models, client_models, client_C, train_federated_data\n",
    "        )\n",
    "        \n",
    "        # Compute 'global state' as defined in the manuscript\n",
    "        S_t = server_global_state(client_S_i)\n",
    "        \n",
    "        num_steps += 1\n",
    "    \n",
    "    # RTC defied, sync must happen\n",
    "    \n",
    "        \n",
    "    # Update the server model from the client models.\n",
    "    model = server_update(client_models)\n",
    "    \n",
    "    metrics.store_and_print_metrics(r, num_steps, S_t, accuracy_fn(model, test_dataset), client_C[0], variance(client_models, [model]*NUM_CLIENTS))\n",
    "    \n",
    "    client_models, last_sync_client_models, last_last_sync_client_models, S_t = [model]*NUM_CLIENTS, [model]*NUM_CLIENTS, last_sync_client_models, [0.]\n",
    "\n",
    "    \n",
    "metrics.print_aggregate_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cf782",
   "metadata": {},
   "source": [
    "\n",
    "2. Comments + Check approach (maybe pass string \"Naive FDA\", unentangle functions)\n",
    "5. Wrap in tff.tf_computation or federated. X nope\n",
    "7. shuffle not random seed so we can compare FDA\n",
    "8. fix awful looking metrics cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tff-py39] *",
   "language": "python",
   "name": "conda-env-tff-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
