{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2759993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74531322",
   "metadata": {},
   "source": [
    "## Import EMNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a550108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc37c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c8e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_BATCH_INPUT = (None, 28, 28) # EMNIST dataset (None is used for batch size, as it varies)\n",
    "CNN_INPUT_RESHAPE = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ab8a7",
   "metadata": {},
   "source": [
    "## Prepare data for Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa54408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(256)\n",
    "\n",
    "del X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24a95c",
   "metadata": {},
   "source": [
    "### Slice the Tensors for each Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b27a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_federated_data_for_clients(num_clients):\n",
    "    \n",
    "    # Shard the data across clients CLIENT LEVEL\n",
    "    client_datasets = [\n",
    "        train_dataset.shard(num_clients, i)\n",
    "        for i in range(num_clients)\n",
    "    ]\n",
    "    \n",
    "    return client_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88734523",
   "metadata": {},
   "source": [
    "### Prepare (and restart) Client Dataset - shuffling, batching, prefetching\n",
    "\n",
    "Proper use of `.prefetch` [explained](https://stackoverflow.com/questions/63796936/what-is-the-proper-use-of-tensorflow-dataset-prefetch-and-cache-options).\n",
    "\n",
    "Proper ordering `.shuffle` and `.batch` and `.repeat` [explained](https://stackoverflow.com/questions/50437234/tensorflow-dataset-shuffle-then-batch-or-batch-then-shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec88b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_federated_data_for_test(federated_data, batch_size, num_steps_until_rtc_check, seed=None):\n",
    "    \n",
    "    def process_client_dataset(client_dataset, batch_size, num_steps_until_rtc_check, seed, shuffle_size=512):\n",
    "        return client_dataset.shuffle(shuffle_size, seed=seed).repeat().batch(batch_size)\\\n",
    "            .take(num_steps_until_rtc_check).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "    federated_dataset_prepared = [\n",
    "        process_client_dataset(client_dataset, batch_size, num_steps_until_rtc_check, seed)\n",
    "        for client_dataset in federated_data\n",
    "    ]\n",
    "    return federated_dataset_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a0455",
   "metadata": {},
   "source": [
    "# Miscallenious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d696f",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d6aa1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def variance(cnn_list, cnn_sync):\n",
    "    \n",
    "    squared_distances = [\n",
    "        tf.reduce_sum(tf.square(cnn.trainable_vars_as_vector() - cnn_sync.trainable_vars_as_vector())) \n",
    "        for cnn in cnn_list\n",
    "    ]\n",
    "    \n",
    "    var = tf.reduce_mean(squared_distances)\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88347123",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_dict(fda_name, n_train, dataset_name, input_pixels, seed, epochs, num_clients, \n",
    "                        batch_size, steps_in_one_fda_step, theta, total_fda_steps, num_weights,\n",
    "                        total_rounds, final_accuracy, sketch_width=-1, sketch_depth=-1):\n",
    "    metrics = {\n",
    "            \"fda_name\" : fda_name,\n",
    "            \"theta\" : theta,\n",
    "            \"dataset_name\" : dataset_name, # new\n",
    "            \"input_pixels\" : input_pixels, # new\n",
    "            \"n_train\" : n_train, # new\n",
    "            \"num_weights\" : num_weights, # new\n",
    "            \"seed\" : seed,\n",
    "            \"epochs\" : epochs,\n",
    "            \"num_clients\" : num_clients,\n",
    "            \"batch_size\" : batch_size,\n",
    "            \"steps_in_one_fda_step\" : steps_in_one_fda_step,\n",
    "            \"sketch_width\" : sketch_width,\n",
    "            \"sketch_depth\" : sketch_depth\n",
    "        }\n",
    "    \n",
    "    # one batch bytes\n",
    "    metrics[\"one_sample_bytes\"] = 4 * (metrics[\"input_pixels\"] + 1)  # 4 bytes float32\n",
    "    \n",
    "    # training dataset size\n",
    "    metrics[\"training_dataset_bytes\"] = metrics[\"one_sample_bytes\"] * metrics[\"n_train\"]\n",
    "    \n",
    "    # model bytes\n",
    "    metrics[\"model_bytes\"] = metrics[\"num_weights\"] * 4\n",
    "    \n",
    "    \n",
    "    # local state bytes (i.e. S_i), for one client\n",
    "    if fda_name == \"naive\":\n",
    "        metrics[\"local_state_bytes\"] = 4\n",
    "    elif fda_name == \"linear\":\n",
    "        metrics[\"local_state_bytes\"] = 8\n",
    "    else:\n",
    "        metrics[\"local_state_bytes\"] = sketch_width * sketch_depth * 4 + 4\n",
    "        \n",
    "    # accuracy (already computed in parameter)\n",
    "    metrics[\"final_accuracy\"] = final_accuracy\n",
    "    \n",
    "    # total fda steps from algo\n",
    "    metrics[\"total_fda_steps\"] = total_fda_steps\n",
    "    \n",
    "    # total steps (a single fda step might have many normal SGD steps, batch steps)\n",
    "    metrics[\"total_steps\"] = metrics[\"total_fda_steps\"] * metrics[\"steps_in_one_fda_step\"]\n",
    "    \n",
    "    # total rounds in algo. Reason why we differentiate from the hardcoded NUM_ROUNDS\n",
    "    # is because we might run less rounds in the future (i.e. stop on 10^7 samples idk)\n",
    "    metrics[\"total_rounds\"] = total_rounds\n",
    "    \n",
    "    # bytes exchanged for synchronizing weights (x2 because server sends back)\n",
    "    metrics[\"model_bytes_exchanged\"] = metrics[\"total_rounds\"] * metrics[\"model_bytes\"] \\\n",
    "        * metrics[\"num_clients\"] * 2\n",
    "    \n",
    "    # bytes exchanged for monitoring the variance (communication)\n",
    "    metrics[\"monitoring_bytes_exchanged\"] = metrics[\"local_state_bytes\"] * metrics[\"total_fda_steps\"] \\\n",
    "        * metrics[\"num_clients\"]\n",
    "    \n",
    "    # total communication bytes (for both monitoring and model synchronization)\n",
    "    metrics[\"total_communication_bytes\"] = metrics[\"model_bytes_exchanged\"] + metrics[\"monitoring_bytes_exchanged\"]\n",
    "    \n",
    "    # total seen dataset bytes (across all learning, i.e., all clients)\n",
    "    metrics[\"trained_in_bytes\"] = metrics[\"batch_size\"] * metrics[\"one_sample_bytes\"] \\\n",
    "        * metrics[\"total_steps\"] * metrics[\"num_clients\"]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b8329",
   "metadata": {},
   "source": [
    "## Time-Series Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd016279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_metrics(time_series_data, dataset_name, fda_name, epochs, num_clients, batch_size, \n",
    "                               steps_in_one_fda_step, theta, num_weights, seed=None, sketch_width=-1, \n",
    "                               sketch_depth=-1):\n",
    "    \n",
    "    time_series_metrics = []\n",
    "    \n",
    "    id_tuple = (\n",
    "        dataset_name, fda_name, epochs, num_clients, batch_size,\n",
    "        steps_in_one_fda_step, theta, num_weights, seed, sketch_width, sketch_depth\n",
    "    )\n",
    "    \n",
    "    # tf.cast(total_fda_steps, dtype=tf.float32), estimated_var, actual_var\n",
    "    for round_num, (total_fda_steps, estimated_var, actual_var) in enumerate(time_series_data):\n",
    "        \n",
    "        total_fda_steps = int(total_fda_steps)\n",
    "        \n",
    "        met = {\n",
    "            \"round\" : round_num,\n",
    "            \"total_fda_steps\" : total_fda_steps,\n",
    "            \"total_steps\" : total_fda_steps*steps_in_one_fda_step,\n",
    "            \"actual_var\" : actual_var,\n",
    "            \"estimated_var\" : estimated_var,\n",
    "        }\n",
    "        \n",
    "        time_series_metrics.append(met)\n",
    "        \n",
    "    return {id_tuple: time_series_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e079822",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Net weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc9b44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weights(model):\n",
    "    total_params = 0\n",
    "    for layer in model.layers:\n",
    "        total_params += np.sum([np.prod(weight.shape) for weight in layer.trainable_weights])\n",
    "    return int(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee0c14",
   "metadata": {},
   "source": [
    "## Reseting NN weights for Server-Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8abe642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables):\n",
    "    \n",
    "    server_cnn.set_trainable_variables(starting_trainable_variables)\n",
    "    \n",
    "    for client_cnn in client_cnns:\n",
    "        client_cnn.set_trainable_variables(starting_trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbfbd3",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Net (CNN) - Medium Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3a252",
   "metadata": {},
   "source": [
    "A simple Convolutional Neural Network with a single convolutional layer, followed by a max-pooling layer, and two dense layers for classification. Designed for 28x28 grayscale images. It has 692,352 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac88dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.reshape = layers.Reshape(CNN_INPUT_RESHAPE)\n",
    "        self.conv1 = layers.Conv2D(32, 3, activation='relu')\n",
    "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dense2 = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "        \n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.reshape(inputs)  # Add a channel dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, batch):\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            y_batch_pred = self(x_batch, training=True)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(\n",
    "                y_true=y_batch,\n",
    "                y_pred=y_batch_pred,\n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients to the model's trainable variables (update weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        #self.compiled_metrics.update_state(y_batch, y_batch_pred)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, dataset):\n",
    "\n",
    "        for batch in dataset:\n",
    "            self.step(batch)\n",
    "            \n",
    "    \n",
    "    def set_trainable_variables(self, trainable_vars):\n",
    "        \"\"\" Given `trainable_vars` set our `self.trainable_vars` \"\"\"\n",
    "        for model_var, var in zip(self.trainable_variables, trainable_vars):\n",
    "            model_var.assign(var)\n",
    "\n",
    "            \n",
    "    def trainable_vars_as_vector(self):\n",
    "        return tf.concat([tf.reshape(var, [-1]) for var in self.trainable_variables], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead5a6c",
   "metadata": {},
   "source": [
    "### Helper function to compile and return the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ddd2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_and_built_cnn():\n",
    "    cnn = CNN()\n",
    "    \n",
    "    cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # we have softmax\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')]\n",
    "    )\n",
    "    \n",
    "    cnn.build(CNN_BATCH_INPUT)  # EMNIST dataset (None is used for batch size, as it varies)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f99e0",
   "metadata": {},
   "source": [
    "# Advanced Convolutional Neural Net (CNN) - Large Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259d325",
   "metadata": {},
   "source": [
    "A more complex Convolutional Neural Network with three sets of two convolutional layers, each followed by a max-pooling layer, and two dense layers with dropout for classification. Designed for 28x28 grayscale images. It has 2,592,202 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee5705b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AdvancedCNN, self).__init__()\n",
    "        \n",
    "        self.reshape = layers.Reshape(CNN_INPUT_RESHAPE)\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv4 = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.conv5 = layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv6 = layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(512, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.dense2 = layers.Dense(512, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "        self.dense3 = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.reshape(inputs)  # Add a channel dimension\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.max_pool2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.max_pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, batch):\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            y_batch_pred = self(x_batch, training=True)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(\n",
    "                y_true=y_batch,\n",
    "                y_pred=y_batch_pred,\n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients to the model's trainable variables (update weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        #self.compiled_metrics.update_state(y_batch, y_batch_pred)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, dataset):\n",
    "\n",
    "        for batch in dataset:\n",
    "            self.step(batch)\n",
    "            \n",
    "    \n",
    "    def set_trainable_variables(self, trainable_vars):\n",
    "        \"\"\" Given `trainable_vars` set our `self.trainable_vars` \"\"\"\n",
    "        for model_var, var in zip(self.trainable_variables, trainable_vars):\n",
    "            model_var.assign(var)\n",
    "\n",
    "            \n",
    "    def trainable_vars_as_vector(self):\n",
    "        return tf.concat([tf.reshape(var, [-1]) for var in self.trainable_variables], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1289f0",
   "metadata": {},
   "source": [
    "### Helper function to compile and return the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a705fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_and_built_advanced_cnn():\n",
    "    advanced_cnn = AdvancedCNN()\n",
    "    \n",
    "    advanced_cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # we have softmax\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')]\n",
    "    )\n",
    "    \n",
    "    advanced_cnn.build(CNN_BATCH_INPUT)  # EMNIST dataset (None is used for batch size, as it varies)\n",
    "    \n",
    "    return advanced_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c454f52",
   "metadata": {},
   "source": [
    "### Average NN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7cb196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_client_weights(client_models):\n",
    "    # client_weights[0] the trainable variables of Client 0 (a list of tf.Variable)\n",
    "    client_weights = [model.trainable_variables for model in client_models]\n",
    "\n",
    "    # concise solution. per layer. `layer_weight_tensors` corresponds to a list of tensors of a layer\n",
    "    avg_weights = [\n",
    "        tf.reduce_mean(layer_weight_tensors, axis=0)\n",
    "        for layer_weight_tensors in zip(*client_weights)\n",
    "    ]\n",
    "\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e3dab",
   "metadata": {},
   "source": [
    "### Server - Clients synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666ac14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def synchronize(server_cnn, client_cnns):\n",
    "    # server average\n",
    "    server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "    \n",
    "    # synchronize clients\n",
    "    for client_cnn in client_cnns:\n",
    "        client_cnn.set_trainable_variables(server_cnn.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a13e6",
   "metadata": {},
   "source": [
    "# Functional Dynamic Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2a1b5",
   "metadata": {},
   "source": [
    "We follow the Functional Dynamic Averaging (FDA) scheme. Let the mean model be\n",
    "\n",
    "$$ \\overline{w_t} = \\frac{1}{k} \\sum_{i=1}^{k} w_t^{(i)} $$\n",
    "\n",
    "where $ w_t^{(i)} $ is the model at time $ t $ in some round in the $i$-th learner.\n",
    "\n",
    "Local models are trained independently and cooperatively and we want to monitor the Round Terminating Conditon (**RTC**):\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2  \\leq \\Theta $$\n",
    "\n",
    "where the left-hand side is the **model variance**, and threshold $\\Theta$ is a hyperparameter of the FDA, defined at the beginning of the round; it may change at each round. When the monitoring logic cannot guarantee the validity of RTC, the round terminates. All local models are pulled into `tff.SERVER`, and $\\bar{w_t}$ is set to their average. Then, another round begins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0ccbb",
   "metadata": {},
   "source": [
    "### Monitoring the RTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6e34b",
   "metadata": {},
   "source": [
    "FDA monitors the RTC by applying techniques from Functionary [Functional Geometric Averaging](http://users.softnet.tuc.gr/~minos/Papers/edbt19.pdf). We first restate the problem of monitoring RTC into the standard distributed stream monitoring formulation. Let\n",
    "\n",
    "$$ S(t) =  \\frac{1}{k} \\sum_{i=1}^{k} S_i(t) $$\n",
    "\n",
    "where $ S(t) \\in \\mathbb{R}^n $ be the \"global state\" of the system and $ S_i(t) \\in \\mathbb{R}^n $ the \"local states\". The goal is to monitor the threshold condition on the global state in the form $ F(S(t)) \\leq \\Theta $ where $ F : \\mathbb{R}^n \\to \\mathbb{R} $ a non-linear function. Let\n",
    "\n",
    "$$ \\Delta_t^{(i)} = w_t^{(i)} - w_{t_0}^{(i)} $$\n",
    "\n",
    "be the update at the $ i $-th learner, that is, the change to the local model at time $t$ since the beginning of the current round at time $t_0$. Let the average update be\n",
    "\n",
    "$$ \\overline{\\Delta_t} = \\frac{1}{k} \\sum_{i=1}^{k} \\Delta_t^{(i)} $$\n",
    "\n",
    "it follows that the variance can be written as\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2 = \\Big( \\frac{1}{k} \\sum_{i=1}^{k} \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\Big) - \\lVert \\overline{\\Delta_t} \\rVert_2^2 $$\n",
    "\n",
    "So, conceptually, if we define\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\bf{x}\n",
    "         \\end{bmatrix}) = v - \\lVert \\bf{x} \\rVert_2^2 $$\n",
    "\n",
    "The RTC is equivalent to condition $$ F(S(t)) \\leq \\Theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a0df4",
   "metadata": {},
   "source": [
    "## 1️⃣ Naive FDA\n",
    "\n",
    "In the naive approach, we eliminate the update vector from the local state (i.e. recuce the dimension to 0). Define local state as\n",
    "\n",
    "$$ S_i(t) = \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\in \\mathbb{R}$$ \n",
    "\n",
    "and the identity function\n",
    "\n",
    "$$ F(v) = v $$\n",
    "\n",
    "It is trivial that $ F(S(t)) \\leq \\Theta $ implies the RTC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6875d47",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0cf704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_naive(last_sync_cnn, client_cnn, client_dataset):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - last_sync_cnn.trainable_vars_as_vector()\n",
    "    \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    return Delta_i_euc_norm_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2013ea",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4780782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_naive(S):\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2c6b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_naive(server_cnn, client_cnns, federated_dataset,\n",
    "                                   num_epochs, theta, epoch_fda_steps):\n",
    "    \n",
    "    print(\"retracing naive\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    \n",
    "    \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "    estimated_var = tf.constant(0., shape=(), dtype=tf.float32)  # for time series data\n",
    "    actual_var = tf.constant(0., shape=(), dtype=tf.float32)  # for time series data\n",
    "    time_series_data = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)  # for time series data\n",
    "    \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_naive(S) <= theta:\n",
    "            S_i_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared = steps_naive(server_cnn, client_cnn, client_dataset)\n",
    "                S_i_clients.append(Delta_i_euc_norm_squared)\n",
    "                \n",
    "            S = tf.reduce_mean(S_i_clients)\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "        estimated_var = F_naive(S)\n",
    "        actual_var = variance(client_cnns, server_cnn)\n",
    "        \n",
    "        time_series_data = time_series_data.write(\n",
    "            total_rounds, \n",
    "            (tf.cast(total_fda_steps, dtype=tf.float32), estimated_var, actual_var)\n",
    "        )\n",
    "        \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps, time_series_data.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6d3f1",
   "metadata": {},
   "source": [
    "## 2️⃣ Linear FDA\n",
    "\n",
    "In the linear case, we reduce the update vector to a scalar, $ \\xi \\Delta_t^{(i)} \\in \\mathbb{R}$, where $ \\xi $ is any unit vector.\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\xi \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^2 $$\n",
    "\n",
    "Also, define \n",
    "\n",
    "$$ F(v, x) = v - x^2 $$\n",
    "\n",
    "The RTC is equivalent to condition \n",
    "\n",
    "$$ F(S(t)) \\leq \\Theta $$\n",
    "\n",
    "A random choice of $ \\xi $ is likely to perform poorly (terminate round prematurely), as it wil likely be close to orthogonal to $ \\overline{\\Delta_t} $. A good choice would be a vector $ \\xi $ correlated to $ \\overline{\\Delta_t} $. A heuristic choice is to take $ \\overline{\\Delta_{t_0}} $ (after scaling it to norm 1), i.e., the update vector right before the current round started. All nodes can estimate this without communication, as $ \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} $, the difference of the last two models pushed by the Server. Hence, \n",
    "\n",
    "$$ \\xi = \\frac{\\overline{w_{t_0}} - \\overline{w_{t_{-1}}}}{\\lVert \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} \\rVert_2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbfb12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ksi_unit_fn(w_t0, w_tminus1):\n",
    "    \n",
    "    if tf.reduce_all(tf.equal(w_t0, w_tminus1)):\n",
    "        # if equal then ksi becomes a random vector (will only happen in round 1)\n",
    "        ksi = tf.random.normal(shape=w_t0.shape)\n",
    "    else:\n",
    "        ksi = w_t0 - w_tminus1\n",
    "\n",
    "    # Normalize and return\n",
    "    return tf.divide(ksi, tf.norm(ksi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b66f11",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0ad6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_linear(cnn_tminus, cnn_t0, client_cnn, client_dataset):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - cnn_t0.trainable_vars_as_vector()\n",
    "    \n",
    "    #||D(t)_i||^2 , shape = (1,) \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    # heuristic unit vector ksi\n",
    "    ksi = ksi_unit_fn(cnn_t0.trainable_vars_as_vector(), cnn_tminus.trainable_vars_as_vector())\n",
    "    \n",
    "    # ksi * Delta_i (* is dot) , shape = ()\n",
    "    ksi_Delta_i = tf.reduce_sum(tf.multiply(ksi, Delta_i))\n",
    "    \n",
    "    return Delta_i_euc_norm_squared, ksi_Delta_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c734b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d8dd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_linear(S_1, S_2):\n",
    "    return S_1 - S_2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7e87dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_linear(previous_server_cnn, server_cnn, client_cnns, federated_dataset,\n",
    "                                   num_epochs, theta, epoch_fda_steps):\n",
    "    \n",
    "    print(\"retracing linear\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    S_2 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    \n",
    "    \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "    estimated_var = tf.constant(0., shape=(), dtype=tf.float32)  # for time series data\n",
    "    actual_var = tf.constant(0., shape=(), dtype=tf.float32)  # for time series data\n",
    "    time_series_data = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)  # for time series data\n",
    "    \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_linear(S_1, S_2) <= theta:\n",
    "            euc_norm_squared_clients = []\n",
    "            ksi_delta_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared, ksi_Delta_i = steps_linear(\n",
    "                    previous_server_cnn, server_cnn, client_cnn, client_dataset\n",
    "                )\n",
    "                \n",
    "                euc_norm_squared_clients.append(Delta_i_euc_norm_squared)\n",
    "                ksi_delta_clients.append(ksi_Delta_i)\n",
    "            \n",
    "            S_1 = tf.reduce_mean(euc_norm_squared_clients)\n",
    "            S_2 = tf.reduce_mean(ksi_delta_clients)\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "        # last server model (previous sync)\n",
    "        previous_server_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "        estimated_var = F_linear(S_1, S_2)\n",
    "        actual_var = variance(client_cnns, server_cnn)\n",
    "        \n",
    "        time_series_data = time_series_data.write(\n",
    "            total_rounds, \n",
    "            (tf.cast(total_fda_steps, dtype=tf.float32), estimated_var, actual_var)\n",
    "        )\n",
    "        \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "        S_2 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps, time_series_data.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b28f1a",
   "metadata": {},
   "source": [
    "## 3️⃣ Sketch FDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a6434",
   "metadata": {},
   "source": [
    "An optimal estimator for $ \\lVert \\overline{\\Delta_t} \\rVert_2^2  $ can be obtained by employing AMS sketches. An AMS sketch of a vector $ v \\in \\mathbb{R}^M $ is a $ d \\times m $ real matrix\n",
    "\n",
    "$$ \\Xi = \\text{sk}(v) = \\begin{bmatrix}\n",
    "           \\Xi_1 \\\\\n",
    "           \\Xi_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           \\Xi_d \n",
    "         \\end{bmatrix} $$\n",
    "         \n",
    "where $ d \\cdot m \\ll M$. Operator sk($ \\cdot $) is linear, i.e., let $a, b \\in \\mathbb{R}$ and $v_1, v_2 \\in \\mathbb{R}^N$ then \n",
    "\n",
    "$$ \\text{sk}(a v_1 + b v_2) = a \\; \\text{sk}(v_1) + b \\; \\text{sk}(v_2)  $$\n",
    "\n",
    "Also, sk($ v $) can be computed in $ \\mathcal{O}(dN) $ steps.\n",
    "\n",
    "The interesting property of AMS sketches is that the function \n",
    "\n",
    "$$ M(sk(\\textbf{v})) = \\underset{i=1,...,d}{\\text{median}} \\; \\lVert \\boldsymbol{\\Xi}_i \\rVert_2^2  $$ \n",
    "\n",
    "is an excellent estimator of the Euclidean norm of **v** (within relative $\\epsilon$-error):\n",
    "\n",
    "$$ M(sk(\\textbf{v})) \\; \\in (1 \\pm \\epsilon) \\lVert \\textbf{v} \\rVert_2^2 \\; \\; \\text{with probability at least} \\; (1-\\delta) $$\n",
    "\n",
    "where $m = \\mathcal{O}(\\frac{1}{\\epsilon^2})$ and $d = \\mathcal{O}(\\log \\frac{1}{\\delta})$\n",
    "            \n",
    "Moreover, let $\\boldsymbol{\\Xi} \\in \\mathbb{R}^{d \\times m}$ and $ k \\in \\mathbb{R}$. It can be proven that\n",
    "\n",
    "$$ M( \\frac{1}{k} \\boldsymbol{\\Xi}) = \\frac{1}{k^2} M(\\boldsymbol{\\Xi}) $$\n",
    "\n",
    "Let's investigate a little further on how this helps us. The $i$-th client computes $ sk(\\Delta_t^{(i)}) $ and sends it to the server. Notice\n",
    "\n",
    "$$ M\\big(sk(\\Delta_t^{(1)}) + sk(\\Delta_t^{(2)}) + ... + sk(\\Delta_t^{(k)}) \\big) = M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big)$$\n",
    "\n",
    "Remember that\n",
    "\n",
    "$$ \\overline{\\boldsymbol{\\Delta}}_t = \\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} $$\n",
    "\n",
    "Then\n",
    "            \n",
    "$$ M\\Big( \\text{sk}\\big( \\overline{\\boldsymbol{\\Delta}}_t \\big) \\Big) = M\\Big( \\text{sk}\\big( \\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) = \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) $$\n",
    "\n",
    "\n",
    "Which means that \n",
    "\n",
    "$$ \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) \\in (1 \\pm \\epsilon) \\lVert \\overline{\\boldsymbol{\\Delta}}_t \\rVert_2^2 \\; \\; \\text{w.p. at least} \\; (1-\\delta) $$\n",
    "\n",
    "In the monitoring process it is essential that we do not overestimate $ \\lVert \\overline{\\Delta_t} \\rVert_2^2 $ because we would then underestimate the variance which would potentially result in actual varience exceeding $ \\Theta$ without us noticing it. With this in mind,\n",
    "\n",
    "$$ \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big) \\leq (1+\\epsilon) \\lVert \\overline{\\Delta_t} \\rVert_2^2 \\quad \\text{with probability at least} \\; (1-\\delta)$$\n",
    "\n",
    "Which means\n",
    "\n",
    "$$ \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big) \\leq \\lVert \\overline{\\Delta_t} \\rVert_2^2 \\quad \\text{with probability at least} \\; (1-\\delta)$$\n",
    "\n",
    "Hence, the Server's estimation of $ \\lVert \\overline{\\Delta_t} \\rVert_2^2 $ is\n",
    "\n",
    "$$ \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M\\Big( sk(\\Delta_t^{(1)}) + sk(\\Delta_t^{(2)}) + ... + sk(\\Delta_t^{(k)}) \\big) \\Big) $$\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           sk(\\Delta_t^{(i)})\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^{1+d \\times m} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\Xi\n",
    "         \\end{bmatrix}) = v - \\frac{1}{(1+\\epsilon)}  M(\\Xi) \\quad \\text{where} \\quad \\Xi = \\frac{1}{k} \\sum_{i=1}^{k} sk(\\Delta_t^{(i)}) $$\n",
    "\n",
    "It follows that $ F(S(t)) \\leq \\Theta $ implies that the variance is less or equal to $ \\Theta $ with probability at least $ 1-\\delta $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db738526",
   "metadata": {},
   "source": [
    "## AMS sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e3bfa",
   "metadata": {},
   "source": [
    "We use `ExtensionType` which is the way to go in order to avoid unecessary graph retracing when passing around `AmsSketch` type 'objects'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7f6b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.experimental import ExtensionType\n",
    "\n",
    "class AmsSketch(ExtensionType):\n",
    "    depth: int\n",
    "    width: int\n",
    "    F: tf.Tensor\n",
    "        \n",
    "        \n",
    "    def __init__(self, depth=7, width=1500):\n",
    "        self.depth = depth\n",
    "        self.width = width\n",
    "        self.F = tf.random.uniform(shape=(6, depth), minval=0, maxval=(1 << 31) - 1, dtype=tf.int32)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def hash31(self, x, a, b):\n",
    "\n",
    "        r = a * x + b\n",
    "        fold = tf.bitwise.bitwise_xor(tf.bitwise.right_shift(r, 31), r)\n",
    "        return tf.bitwise.bitwise_and(fold, 2147483647)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def tensor_hash31(self, x, a, b): # GOOD\n",
    "        \"\"\" Assumed that x is tensor shaped (d,) , i.e., a vector (for example, indices, i.e., tf.range(d)) \"\"\"\n",
    "\n",
    "        # Reshape x to have an extra dimension, resulting in a shape of (k, 1)\n",
    "        x_reshaped = tf.expand_dims(x, axis=-1)\n",
    "\n",
    "        # shape=(`v_dim`, 7)\n",
    "        r = tf.multiply(a, x_reshaped) + b\n",
    "\n",
    "        fold = tf.bitwise.bitwise_xor(tf.bitwise.right_shift(r, 31), r)\n",
    "        \n",
    "        return tf.bitwise.bitwise_and(fold, 2147483647)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def tensor_fourwise(self, x):\n",
    "        \"\"\" Assumed that x is tensor shaped (d,) , i.e., a vector (for example, indices, i.e., tf.range(d)) \"\"\"\n",
    "        # 1st use the tensor hash31\n",
    "        in1 = self.tensor_hash31(x, self.F[2], self.F[3])  # (`x_dim`, 7)\n",
    "        \n",
    "        # 2nd (notice we swap the first two params, no change really)\n",
    "        in2 = self.tensor_hash31(x, in1, self.F[4])  # (`x_dim`, 7)\n",
    "        \n",
    "        in3 = self.tensor_hash31(x, in2, self.F[5])  # (`x_dim`, 7)\n",
    "        \n",
    "        in4 = tf.bitwise.bitwise_and(in3, 32768)  # (`x_dim`, 7)\n",
    "        \n",
    "        return 2 * (tf.bitwise.right_shift(in4, 15)) - 1  # (`x_dim`, 7)\n",
    "        \n",
    "        \n",
    "    @tf.function\n",
    "    def fourwise(self, x):\n",
    "\n",
    "        result = 2 * (tf.bitwise.right_shift(tf.bitwise.bitwise_and(self.hash31(self.hash31(self.hash31(x, self.F[2], self.F[3]), x, self.F[4]), x, self.F[5]), 32768), 15)) - 1\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector(self, v):\n",
    "        \"\"\" Extremely efficient computation of sketch with only using tensors. \"\"\"\n",
    "        \n",
    "        sketch = tf.zeros(shape=(self.depth, self.width), dtype=tf.float32)\n",
    "        \n",
    "        len_v = v.shape[0]\n",
    "        \n",
    "        pos_tensor = self.tensor_hash31(tf.range(len_v), self.F[0], self.F[1]) % self.width\n",
    "        \n",
    "        v_expand = tf.expand_dims(v, axis=-1)\n",
    "        \n",
    "        deltas_tensor = tf.multiply(tf.cast(self.tensor_fourwise(tf.range(len_v)), dtype=tf.float32), v_expand)\n",
    "        \n",
    "        range_tensor = tf.range(self.depth)\n",
    "        \n",
    "        # Expand dimensions to create a 2D tensor with shape (1, depth)\n",
    "        range_tensor_expanded = tf.expand_dims(range_tensor, 0)\n",
    "\n",
    "        # Use tf.tile to repeat the range `len_v` times\n",
    "        repeated_range_tensor = tf.tile(range_tensor_expanded, [len_v, 1])\n",
    "        \n",
    "        # shape=(`len_v`, 7, 2)\n",
    "        indices = tf.stack([repeated_range_tensor, pos_tensor], axis=-1)\n",
    "        \n",
    "        sketch = tf.tensor_scatter_nd_add(sketch, indices, deltas_tensor)\n",
    "        \n",
    "        return sketch\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector2(self, v):\n",
    "        \"\"\" Bad implementation for tensorflow. \"\"\"\n",
    "\n",
    "        sketch = tf.zeros(shape=(self.depth, self.width), dtype=tf.float32)\n",
    "\n",
    "        for i in tf.range(tf.shape(v)[0], dtype=tf.int32):\n",
    "            pos = self.hash31(i, self.F[0], self.F[1]) % self.width\n",
    "            delta = tf.cast(self.fourwise(i), dtype=tf.float32) * v[i]\n",
    "            indices_to_update = tf.stack([tf.range(self.depth, dtype=tf.int32), pos], axis=1)\n",
    "            sketch = tf.tensor_scatter_nd_add(sketch, indices_to_update, delta)\n",
    "\n",
    "        return sketch\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def estimate_euc_norm_squared(sketch):\n",
    "\n",
    "        @tf.function\n",
    "        def _median(v):\n",
    "            \"\"\" Median of tensor `v` with shape=(n,). Note: Suboptimal O(nlogn) but it's ok bcz n = `depth`\"\"\"\n",
    "            length = tf.shape(v)[0]\n",
    "            sorted_v = tf.sort(v)\n",
    "            middle = length // 2\n",
    "\n",
    "            return tf.cond(\n",
    "                tf.equal(length % 2, 0),\n",
    "                lambda: (sorted_v[middle - 1] + sorted_v[middle]) / 2.0,\n",
    "                lambda: sorted_v[middle]\n",
    "            )\n",
    "\n",
    "        return _median(tf.reduce_sum(tf.square(sketch), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe92e3b",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34a74388",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_sketch(last_sync_cnn, client_cnn, client_dataset, ams_sketch):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - last_sync_cnn.trainable_vars_as_vector()\n",
    "    \n",
    "    #||D(t)_i||^2 , shape = (1,) \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    # sketch approx\n",
    "    sketch = ams_sketch.sketch_for_vector(Delta_i)\n",
    "    \n",
    "    return Delta_i_euc_norm_squared, sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ba6cf",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6b0043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_sketch(S_1, S_2, epsilon):\n",
    "    \"\"\" `S_1` is mean || ||^2 as usual, S_2 is the `Ξ` as defined in the theoretical analysis above \"\"\"\n",
    "    return S_1 - (1. / (1. + epsilon)) * AmsSketch.estimate_euc_norm_squared(S_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9f520f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_sketch(server_cnn, client_cnns, federated_dataset, num_epochs, \n",
    "                                    theta, epoch_fda_steps, ams_sketch, epsilon):\n",
    "\n",
    "    print(\"retracing sketch\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    S_2 = tf.zeros(shape=(ams_sketch.depth, ams_sketch.width), dtype=tf.float32)\n",
    "    \n",
    "    \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "    estimated_var = tf.constant(0., shape=(), dtype=tf.float32)  # for time series data\n",
    "    actual_var = tf.constant(0., shape=(), dtype=tf.float32)  # for time series data\n",
    "    time_series_data = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)  # for time series data\n",
    "    \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_sketch(S_1, S_2, epsilon) <= theta:\n",
    "            euc_norm_squared_clients = []\n",
    "            sketch_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared, sketch = steps_sketch(\n",
    "                    server_cnn, client_cnn, client_dataset, ams_sketch\n",
    "                )\n",
    "                \n",
    "                euc_norm_squared_clients.append(Delta_i_euc_norm_squared)\n",
    "                sketch_clients.append(sketch)\n",
    "            \n",
    "            S_1 = tf.reduce_mean(euc_norm_squared_clients)\n",
    "            S_2 = tf.reduce_mean(sketch_clients, axis=0)  # shape=(`depth`, width`). See `Ξ` in theoretical analysis\n",
    "            \n",
    "            #del euc_norm_squared_clients, sketch_clients\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "        estimated_var = F_sketch(S_1, S_2, epsilon)\n",
    "        actual_var = variance(client_cnns, server_cnn)\n",
    "        \n",
    "        time_series_data = time_series_data.write(\n",
    "            total_rounds, \n",
    "            (tf.cast(total_fda_steps, dtype=tf.float32), estimated_var, actual_var)\n",
    "        )\n",
    "        \"\"\"------------------------------time-series metrics-------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "        S_2 = tf.zeros(shape=(ams_sketch.depth, ams_sketch.width), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps, time_series_data.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d998e29",
   "metadata": {},
   "source": [
    "# Simulation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615142b0",
   "metadata": {},
   "source": [
    "###  Basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94d94680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_test(server_cnn, client_cnns, previous_server_cnn, starting_trainable_variables, \n",
    "               NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, BATCH_SIZE, \n",
    "               THETA, EPSILON, ams_sketch, clients_federated_data, seed):\n",
    "    \n",
    "    \"\"\" One test for Naive,Linear,Sketch. Returns metrics \"\"\"\n",
    "    \n",
    "    num_epochs = tf.constant(NUM_EPOCHS, shape=(), dtype=tf.int32)\n",
    "    theta = tf.constant(THETA, shape=(), dtype=tf.float32)\n",
    "    \n",
    "    # for sketch\n",
    "    epsilon = tf.constant(EPSILON, shape=(), dtype=tf.float32) # new\n",
    "    \n",
    "    epoch_client_batches = (n_train / BATCH_SIZE) / NUM_CLIENTS\n",
    "    epoch_max_fda_steps = epoch_client_batches / NUM_STEPS_UNTIL_RTC_CHECK\n",
    "    epoch_max_fda_steps = tf.constant(round(epoch_max_fda_steps), shape=(), dtype=tf.int32)\n",
    "    \n",
    "    basic_test_metrics = []\n",
    "    basic_test_time_series_metrics = []\n",
    "    \n",
    "    \"\"\" --------------- Naive ----------------------------------\"\"\"\n",
    "    \n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "    \n",
    "    federated_dataset = prepare_federated_data_for_test(\n",
    "        federated_data=clients_federated_data, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Run \n",
    "\n",
    "    total_rounds, total_fda_steps, time_series_data = run_federated_simulation_naive(\n",
    "        server_cnn, \n",
    "        client_cnns, \n",
    "        federated_dataset, \n",
    "        num_epochs, \n",
    "        theta,\n",
    "        epoch_max_fda_steps\n",
    "    )\n",
    "    \n",
    "    # 3. compute metrics\n",
    "    \n",
    "    _, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"naive\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "    \n",
    "    time_series_metrics = create_time_series_metrics(\n",
    "        time_series_data=time_series_data.numpy(),\n",
    "        dataset_name=\"EMNIST\",\n",
    "        fda_name=\"naive\", \n",
    "        epochs=NUM_EPOCHS,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        theta=THETA, \n",
    "        num_weights=count_weights(server_cnn), \n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    basic_test_time_series_metrics.append(time_series_metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, time_series_data, time_series_metrics, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    \n",
    "    \"\"\" --------------- Linear ----------------------------------\"\"\"\n",
    "\n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "\n",
    "    federated_dataset = prepare_federated_data_for_test(\n",
    "        federated_data=clients_federated_data, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 3. Run \n",
    "\n",
    "    total_rounds, total_fda_steps, time_series_data = run_federated_simulation_linear(\n",
    "        previous_server_cnn,\n",
    "        server_cnn, \n",
    "        client_cnns, \n",
    "        federated_dataset, \n",
    "        num_epochs, \n",
    "        theta,\n",
    "        epoch_max_fda_steps\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 4. compute metrics\n",
    "    \n",
    "    loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"linear\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "    \n",
    "    time_series_metrics = create_time_series_metrics(\n",
    "        time_series_data=time_series_data.numpy(),\n",
    "        dataset_name=\"EMNIST\",\n",
    "        fda_name=\"linear\", \n",
    "        epochs=NUM_EPOCHS,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        theta=THETA, \n",
    "        num_weights=count_weights(server_cnn), \n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    basic_test_time_series_metrics.append(time_series_metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, time_series_data, time_series_metrics, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    previous_server_cnn.set_trainable_variables(starting_trainable_variables)  # +\n",
    "\n",
    "    \n",
    "    \"\"\" ------------------------ Sketch ----------------------\"\"\"\n",
    "\n",
    "    \n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "    \n",
    "    federated_dataset = prepare_federated_data_for_test(\n",
    "        federated_data=clients_federated_data, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Run \n",
    "\n",
    "    total_rounds, total_fda_steps, time_series_data = run_federated_simulation_sketch(\n",
    "        server_cnn=server_cnn, \n",
    "        client_cnns=client_cnns, \n",
    "        federated_dataset=federated_dataset,\n",
    "        num_epochs=num_epochs, \n",
    "        theta=theta, \n",
    "        epoch_fda_steps=epoch_max_fda_steps, \n",
    "        ams_sketch=ams_sketch, \n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3. compute metrics\n",
    "    \n",
    "    loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"sketch\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc, \n",
    "        sketch_width=ams_sketch.width, \n",
    "        sketch_depth=ams_sketch.depth\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "    \n",
    "    time_series_metrics = create_time_series_metrics(\n",
    "        time_series_data=time_series_data.numpy(),\n",
    "        dataset_name=\"EMNIST\",\n",
    "        fda_name=\"sketch\", \n",
    "        epochs=NUM_EPOCHS,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        theta=THETA, \n",
    "        num_weights=count_weights(server_cnn), \n",
    "        seed=seed, \n",
    "        sketch_width=ams_sketch.width, \n",
    "        sketch_depth=ams_sketch.depth\n",
    "    )\n",
    "    \n",
    "    basic_test_time_series_metrics.append(time_series_metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, time_series_data, time_series_metrics, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    return basic_test_metrics, basic_test_time_series_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f8f8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info_current_test(NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, THETA, BATCH_SIZE):\n",
    "    print()\n",
    "    print(f\"----------- Current Test --------------\")\n",
    "    print(f\"Num Clients : {NUM_CLIENTS}\")\n",
    "    print(f\"Num Epochs : {NUM_EPOCHS}\")\n",
    "    print(f\"Number of steps until we check RTC : {NUM_STEPS_UNTIL_RTC_CHECK}\")\n",
    "    print(f\"Batch size : {BATCH_SIZE}\")\n",
    "    print(f\"Theta : {THETA}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b2078c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt # new\n",
    "from copy import deepcopy\n",
    "\n",
    "def run_tests(NUM_CLIENTS_LIST, NUM_EPOCHS_LIST, NUM_STEPS_UNTIL_RTC_CHECK_LIST,\n",
    "              BATCH_SIZE_LIST, THETA_LIST, SKETCH_DEPTH, SKETCH_WIDTH, SEED=None):\n",
    "    \n",
    "    \"\"\" --------------- Fixed configurations -------------------\"\"\"\n",
    "\n",
    "    ams_sketch = AmsSketch(\n",
    "        depth=SKETCH_DEPTH,\n",
    "        width=SKETCH_WIDTH\n",
    "    )\n",
    "\n",
    "    EPSILON = 1. / sqrt(SKETCH_WIDTH)\n",
    "    \n",
    "    \n",
    "    \"\"\" --------------- Metrics list ----------------------\"\"\"\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    all_time_series_metrics = []\n",
    "        \n",
    "    \"\"\" --------------- Run tests -------------------\"\"\"\n",
    "    for NUM_CLIENTS in NUM_CLIENTS_LIST:\n",
    "\n",
    "        # 1. Dataset for the same number of `NUM_CLIENTS`\n",
    "\n",
    "        clients_federated_data = create_federated_data_for_clients(NUM_CLIENTS)  # new sliced dataset (diff NUM_CLIENTS)\n",
    "\n",
    "        # 2. CNNs for the same number of `NUM_CLIENTS` \n",
    "\n",
    "        # we will create the CNNs here to avoid graph retracing (we will keep the same starting variables)\n",
    "        server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "        client_cnns = [get_compiled_and_built_advanced_cnn() for _ in range(NUM_CLIENTS)]\n",
    "\n",
    "        previous_server_cnn = get_compiled_and_built_advanced_cnn()  # For linear\n",
    "\n",
    "        # synchronize\n",
    "        synchronize(server_cnn, client_cnns)\n",
    "\n",
    "        # keep the same starting variables in each test corresponding to the same `NUM_CLIENTS`\n",
    "        starting_trainable_variables = deepcopy(server_cnn.trainable_variables)\n",
    "\n",
    "        previous_server_cnn.set_trainable_variables(starting_trainable_variables)  # For linear\n",
    "\n",
    "        for NUM_EPOCHS in NUM_EPOCHS_LIST:\n",
    "\n",
    "            for NUM_STEPS_UNTIL_RTC_CHECK in NUM_STEPS_UNTIL_RTC_CHECK_LIST:\n",
    "\n",
    "                for BATCH_SIZE in BATCH_SIZE_LIST:\n",
    "\n",
    "                    for THETA in THETA_LIST:\n",
    "\n",
    "                        print_info_current_test(NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, THETA, BATCH_SIZE)\n",
    "\n",
    "                        basic_test_metrics, basic_test_time_series_metrics = basic_test(\n",
    "                            server_cnn=server_cnn,\n",
    "                            client_cnns=client_cnns,\n",
    "                            previous_server_cnn=previous_server_cnn,\n",
    "                            starting_trainable_variables=starting_trainable_variables,\n",
    "                            NUM_EPOCHS=NUM_EPOCHS, \n",
    "                            NUM_STEPS_UNTIL_RTC_CHECK=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "                            NUM_CLIENTS=NUM_CLIENTS,\n",
    "                            BATCH_SIZE=BATCH_SIZE, \n",
    "                            THETA=THETA, \n",
    "                            EPSILON=EPSILON,\n",
    "                            ams_sketch=ams_sketch,\n",
    "                            clients_federated_data=clients_federated_data,\n",
    "                            seed=SEED\n",
    "                        )\n",
    "\n",
    "                        all_metrics.extend(basic_test_metrics)\n",
    "                        all_time_series_metrics.extend(basic_test_time_series_metrics)\n",
    "\n",
    "        # Delete previous stuff because we will encounter a different `NUM_CLIENTS`\n",
    "        del clients_federated_data, server_cnn, client_cnns, previous_server_cnn, starting_trainable_variables\n",
    "\n",
    "    return all_metrics, all_time_series_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e61e5",
   "metadata": {},
   "source": [
    "# Run Simulation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a5b5b61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Current Test --------------\n",
      "Num Clients : 5\n",
      "Num Epochs : 1\n",
      "Number of steps until we check RTC : 1\n",
      "Batch size : 32\n",
      "Theta : 1.0\n",
      "----------------------------------------\n",
      "\n",
      "retracing naive\n",
      "retracing linear\n",
      "retracing sketch\n",
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "all_metrics, all_time_series_metrics = run_tests(\n",
    "    NUM_CLIENTS_LIST=[15],\n",
    "    NUM_EPOCHS_LIST=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    NUM_STEPS_UNTIL_RTC_CHECK_LIST=[1],\n",
    "    BATCH_SIZE_LIST=[32, 64, 128],\n",
    "    THETA_LIST=[0.05, 0.5, 1., 3., 5., 10.],\n",
    "    SKETCH_DEPTH=7,\n",
    "    SKETCH_WIDTH=500,\n",
    "    SEED=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4b2b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_df = pd.DataFrame(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da382e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_df.to_csv('test_results/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e3cd8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fda_name</th>\n",
       "      <th>theta</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>input_pixels</th>\n",
       "      <th>n_train</th>\n",
       "      <th>num_weights</th>\n",
       "      <th>seed</th>\n",
       "      <th>epochs</th>\n",
       "      <th>num_clients</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>...</th>\n",
       "      <th>model_bytes</th>\n",
       "      <th>local_state_bytes</th>\n",
       "      <th>final_accuracy</th>\n",
       "      <th>total_fda_steps</th>\n",
       "      <th>total_steps</th>\n",
       "      <th>total_rounds</th>\n",
       "      <th>model_bytes_exchanged</th>\n",
       "      <th>monitoring_bytes_exchanged</th>\n",
       "      <th>total_communication_bytes</th>\n",
       "      <th>trained_in_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EMNIST</td>\n",
       "      <td>784</td>\n",
       "      <td>60000</td>\n",
       "      <td>2592202</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>10368808</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>375</td>\n",
       "      <td>375</td>\n",
       "      <td>91</td>\n",
       "      <td>9435615280</td>\n",
       "      <td>7500</td>\n",
       "      <td>9435622780</td>\n",
       "      <td>188400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EMNIST</td>\n",
       "      <td>784</td>\n",
       "      <td>60000</td>\n",
       "      <td>2592202</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>10368808</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9493</td>\n",
       "      <td>375</td>\n",
       "      <td>375</td>\n",
       "      <td>86</td>\n",
       "      <td>8917174880</td>\n",
       "      <td>15000</td>\n",
       "      <td>8917189880</td>\n",
       "      <td>188400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sketch</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EMNIST</td>\n",
       "      <td>784</td>\n",
       "      <td>60000</td>\n",
       "      <td>2592202</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>10368808</td>\n",
       "      <td>14004</td>\n",
       "      <td>0.9616</td>\n",
       "      <td>375</td>\n",
       "      <td>375</td>\n",
       "      <td>95</td>\n",
       "      <td>9850367600</td>\n",
       "      <td>26257500</td>\n",
       "      <td>9876625100</td>\n",
       "      <td>188400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fda_name  theta dataset_name  input_pixels  n_train  num_weights  seed  \\\n",
       "0    naive    1.0       EMNIST           784    60000      2592202     7   \n",
       "1   linear    1.0       EMNIST           784    60000      2592202     7   \n",
       "2   sketch    1.0       EMNIST           784    60000      2592202     7   \n",
       "\n",
       "   epochs  num_clients  batch_size  ...  model_bytes  local_state_bytes  \\\n",
       "0       1            5          32  ...     10368808                  4   \n",
       "1       1            5          32  ...     10368808                  8   \n",
       "2       1            5          32  ...     10368808              14004   \n",
       "\n",
       "   final_accuracy  total_fda_steps  total_steps  total_rounds  \\\n",
       "0          0.9659              375          375            91   \n",
       "1          0.9493              375          375            86   \n",
       "2          0.9616              375          375            95   \n",
       "\n",
       "   model_bytes_exchanged  monitoring_bytes_exchanged  \\\n",
       "0             9435615280                        7500   \n",
       "1             8917174880                       15000   \n",
       "2             9850367600                    26257500   \n",
       "\n",
       "   total_communication_bytes  trained_in_bytes  \n",
       "0                 9435622780         188400000  \n",
       "1                 8917189880         188400000  \n",
       "2                 9876625100         188400000  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d0dc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data to create a DataFrame\n",
    "data = []\n",
    "index_tuples = []\n",
    "\n",
    "for time_series_dict in all_time_series_metrics:\n",
    "    for id_tuple, time_series_metrics in time_series_dict.items():\n",
    "        index_tuples.extend([id_tuple]*len(time_series_metrics))\n",
    "        data.extend(time_series_metrics)\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(index_tuples, names=['dataset_name', 'fda_name', 'epochs', 'num_clients', 'batch_size', 'steps_in_one_fda_step', 'theta', 'num_weights', 'seed', 'sketch_width', 'sketch_depth'])\n",
    "all_time_series_metrics_df = pd.DataFrame(data, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4482ff4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>total_fda_steps</th>\n",
       "      <th>total_steps</th>\n",
       "      <th>actual_var</th>\n",
       "      <th>estimated_var</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_name</th>\n",
       "      <th>fda_name</th>\n",
       "      <th>epochs</th>\n",
       "      <th>num_clients</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>steps_in_one_fda_step</th>\n",
       "      <th>theta</th>\n",
       "      <th>num_weights</th>\n",
       "      <th>seed</th>\n",
       "      <th>sketch_width</th>\n",
       "      <th>sketch_depth</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">EMNIST</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">naive</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">32</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2592202</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">7</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">-1</th>\n",
       "      <th>-1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.884019</td>\n",
       "      <td>1.098551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1.103115</td>\n",
       "      <td>1.369943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.944327</td>\n",
       "      <td>1.159325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0.945413</td>\n",
       "      <td>1.193771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.867770</td>\n",
       "      <td>1.120618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">sketch</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">32</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2592202</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">7</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">500</th>\n",
       "      <th>7</th>\n",
       "      <td>90</td>\n",
       "      <td>353</td>\n",
       "      <td>353</td>\n",
       "      <td>1.071295</td>\n",
       "      <td>1.084746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>91</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>1.200325</td>\n",
       "      <td>1.224319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92</td>\n",
       "      <td>366</td>\n",
       "      <td>366</td>\n",
       "      <td>1.314449</td>\n",
       "      <td>1.328762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93</td>\n",
       "      <td>372</td>\n",
       "      <td>372</td>\n",
       "      <td>1.069446</td>\n",
       "      <td>1.084648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>94</td>\n",
       "      <td>375</td>\n",
       "      <td>375</td>\n",
       "      <td>0.324661</td>\n",
       "      <td>0.327397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                            round  \\\n",
       "dataset_name fda_name epochs num_clients batch_size steps_in_one_fda_step theta num_weights seed sketch_width sketch_depth          \n",
       "EMNIST       naive    1      5           32         1                     1.0   2592202     7    -1           -1                0   \n",
       "                                                                                                              -1                1   \n",
       "                                                                                                              -1                2   \n",
       "                                                                                                              -1                3   \n",
       "                                                                                                              -1                4   \n",
       "...                                                                                                                           ...   \n",
       "             sketch   1      5           32         1                     1.0   2592202     7     500          7               90   \n",
       "                                                                                                               7               91   \n",
       "                                                                                                               7               92   \n",
       "                                                                                                               7               93   \n",
       "                                                                                                               7               94   \n",
       "\n",
       "                                                                                                                            total_fda_steps  \\\n",
       "dataset_name fda_name epochs num_clients batch_size steps_in_one_fda_step theta num_weights seed sketch_width sketch_depth                    \n",
       "EMNIST       naive    1      5           32         1                     1.0   2592202     7    -1           -1                          6   \n",
       "                                                                                                              -1                         12   \n",
       "                                                                                                              -1                         18   \n",
       "                                                                                                              -1                         24   \n",
       "                                                                                                              -1                         30   \n",
       "...                                                                                                                                     ...   \n",
       "             sketch   1      5           32         1                     1.0   2592202     7     500          7                        353   \n",
       "                                                                                                               7                        360   \n",
       "                                                                                                               7                        366   \n",
       "                                                                                                               7                        372   \n",
       "                                                                                                               7                        375   \n",
       "\n",
       "                                                                                                                            total_steps  \\\n",
       "dataset_name fda_name epochs num_clients batch_size steps_in_one_fda_step theta num_weights seed sketch_width sketch_depth                \n",
       "EMNIST       naive    1      5           32         1                     1.0   2592202     7    -1           -1                      6   \n",
       "                                                                                                              -1                     12   \n",
       "                                                                                                              -1                     18   \n",
       "                                                                                                              -1                     24   \n",
       "                                                                                                              -1                     30   \n",
       "...                                                                                                                                 ...   \n",
       "             sketch   1      5           32         1                     1.0   2592202     7     500          7                    353   \n",
       "                                                                                                               7                    360   \n",
       "                                                                                                               7                    366   \n",
       "                                                                                                               7                    372   \n",
       "                                                                                                               7                    375   \n",
       "\n",
       "                                                                                                                            actual_var  \\\n",
       "dataset_name fda_name epochs num_clients batch_size steps_in_one_fda_step theta num_weights seed sketch_width sketch_depth               \n",
       "EMNIST       naive    1      5           32         1                     1.0   2592202     7    -1           -1              0.884019   \n",
       "                                                                                                              -1              1.103115   \n",
       "                                                                                                              -1              0.944327   \n",
       "                                                                                                              -1              0.945413   \n",
       "                                                                                                              -1              0.867770   \n",
       "...                                                                                                                                ...   \n",
       "             sketch   1      5           32         1                     1.0   2592202     7     500          7              1.071295   \n",
       "                                                                                                               7              1.200325   \n",
       "                                                                                                               7              1.314449   \n",
       "                                                                                                               7              1.069446   \n",
       "                                                                                                               7              0.324661   \n",
       "\n",
       "                                                                                                                            estimated_var  \n",
       "dataset_name fda_name epochs num_clients batch_size steps_in_one_fda_step theta num_weights seed sketch_width sketch_depth                 \n",
       "EMNIST       naive    1      5           32         1                     1.0   2592202     7    -1           -1                 1.098551  \n",
       "                                                                                                              -1                 1.369943  \n",
       "                                                                                                              -1                 1.159325  \n",
       "                                                                                                              -1                 1.193771  \n",
       "                                                                                                              -1                 1.120618  \n",
       "...                                                                                                                                   ...  \n",
       "             sketch   1      5           32         1                     1.0   2592202     7     500          7                 1.084746  \n",
       "                                                                                                               7                 1.224319  \n",
       "                                                                                                               7                 1.328762  \n",
       "                                                                                                               7                 1.084648  \n",
       "                                                                                                               7                 0.327397  \n",
       "\n",
       "[272 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_time_series_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a70b45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series_metrics_df.to_csv('test_results/time_series_results.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c232b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_time_series_metrics_df = pd.read_csv('output_filename.csv', index_col=['dataset_name', 'fda_name', 'epochs', 'num_clients', 'batch_size', 'steps_in_one_fda_step', 'theta', 'num_weights', 'seed', 'sketch_width', 'sketch_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df9353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be7915fe",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    \n",
    "4. DONE: `get_compiled_and_built_...()` retraces for `server_cnn` every time (ofc for `client_cnns` aswell).\n",
    "   BUT: make sure once more that when we `reset` then afterwards `.evaluate` works correctly. Maybe weird shit with metrics. Check plz\n",
    "\n",
    "\n",
    "5. Approach on sketch should be `reduce_mean`, change it in PA-I.\n",
    "6. Approach on global tests `for` loop PA-I\n",
    "7. remove `one` as a `tf.constant(1)` PA-I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9cd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cde05750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_stuff(THETA):\n",
    "    \n",
    "    NUM_CLIENTS = 5\n",
    "    NUM_EPOCHS = 1\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_STEPS_UNTIL_RTC_CHECK = 1\n",
    "    seed = 7\n",
    "    \n",
    "    \"\"\" --------------- Fixed configurations -------------------\"\"\"\n",
    "    \n",
    "    SKETCH_DEPTH = 7\n",
    "    SKETCH_WIDTH = 500\n",
    "\n",
    "    ams_sketch = AmsSketch(\n",
    "        depth=SKETCH_DEPTH,\n",
    "        width=SKETCH_WIDTH\n",
    "    )\n",
    "\n",
    "    EPSILON = 1. / sqrt(SKETCH_WIDTH)\n",
    "    \n",
    "    \n",
    "    \"\"\" --------------- Metrics list ----------------------\"\"\"\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    clients_federated_data = create_federated_data_for_clients(NUM_CLIENTS)  # new sliced dataset (diff NUM_CLIENTS)\n",
    "            \n",
    "    # 2. CNNs for the same number of `NUM_CLIENTS` \n",
    "\n",
    "    # we will create the CNNs here to avoid graph retracing (we will keep the same starting variables)\n",
    "    server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "    client_cnns = [get_compiled_and_built_advanced_cnn() for _ in range(NUM_CLIENTS)]\n",
    "\n",
    "    previous_server_cnn = get_compiled_and_built_advanced_cnn()  # For linear\n",
    "\n",
    "    # synchronize\n",
    "    synchronize(server_cnn, client_cnns)\n",
    "\n",
    "    # keep the same starting variables in each test corresponding to the same `NUM_CLIENTS`\n",
    "    starting_trainable_variables = deepcopy(server_cnn.trainable_variables)\n",
    "\n",
    "    previous_server_cnn.set_trainable_variables(starting_trainable_variables)  # For linear\n",
    "    \n",
    "    \n",
    "    # 3. Dataset\n",
    "    \n",
    "    \n",
    "    \"\"\" One test for Naive,Linear,Sketch. Returns metrics \"\"\"\n",
    "    \n",
    "    num_epochs = tf.constant(NUM_EPOCHS, shape=(), dtype=tf.int32)\n",
    "    theta = tf.constant(THETA, shape=(), dtype=tf.float32)\n",
    "    \n",
    "    # for sketch\n",
    "    epsilon = tf.constant(EPSILON, shape=(), dtype=tf.float32) # new\n",
    "\n",
    "    epoch_max_fda_steps = tf.constant(20, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    basic_test_metrics = []\n",
    "    basic_test_time_series_metrics = []\n",
    "    \n",
    "    \"\"\" --------------- Naive ----------------------------------\"\"\"\n",
    "    \n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "    \n",
    "    federated_dataset = prepare_federated_data_for_test(\n",
    "        federated_data=clients_federated_data, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Run \n",
    "\n",
    "    total_rounds, total_fda_steps, time_series_data = run_federated_simulation_naive(\n",
    "        server_cnn, \n",
    "        client_cnns, \n",
    "        federated_dataset, \n",
    "        num_epochs, \n",
    "        theta,\n",
    "        epoch_max_fda_steps\n",
    "    )\n",
    "    \n",
    "    # 3. compute metrics\n",
    "    \n",
    "    _, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"naive\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "    \n",
    "    time_series_metrics = create_time_series_metrics(\n",
    "        time_series_data=time_series_data.numpy(),\n",
    "        dataset_name=\"EMNIST\",\n",
    "        fda_name=\"naive\", \n",
    "        epochs=NUM_EPOCHS,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        theta=THETA, \n",
    "        num_weights=count_weights(server_cnn), \n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    basic_test_time_series_metrics.append(time_series_metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, time_series_data, time_series_metrics, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    \n",
    "    \"\"\" --------------- Linear ----------------------------------\"\"\"\n",
    "\n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "\n",
    "    federated_dataset = prepare_federated_data_for_test(\n",
    "        federated_data=clients_federated_data, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 3. Run \n",
    "\n",
    "    total_rounds, total_fda_steps, time_series_data = run_federated_simulation_linear(\n",
    "        previous_server_cnn,\n",
    "        server_cnn, \n",
    "        client_cnns, \n",
    "        federated_dataset, \n",
    "        num_epochs, \n",
    "        theta,\n",
    "        epoch_max_fda_steps\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 4. compute metrics\n",
    "    \n",
    "    loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"linear\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "    \n",
    "    time_series_metrics = create_time_series_metrics(\n",
    "        time_series_data=time_series_data.numpy(),\n",
    "        dataset_name=\"EMNIST\",\n",
    "        fda_name=\"linear\", \n",
    "        epochs=NUM_EPOCHS,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        theta=THETA, \n",
    "        num_weights=count_weights(server_cnn), \n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    basic_test_time_series_metrics.append(time_series_metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, time_series_data, time_series_metrics, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    previous_server_cnn.set_trainable_variables(starting_trainable_variables)  # +\n",
    "\n",
    "    \n",
    "    \"\"\" ------------------------ Sketch ----------------------\"\"\"\n",
    "\n",
    "    \n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "    \n",
    "    federated_dataset = prepare_federated_data_for_test(\n",
    "        federated_data=clients_federated_data, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Run \n",
    "\n",
    "    total_rounds, total_fda_steps, time_series_data = run_federated_simulation_sketch(\n",
    "        server_cnn=server_cnn, \n",
    "        client_cnns=client_cnns, \n",
    "        federated_dataset=federated_dataset,\n",
    "        num_epochs=num_epochs, \n",
    "        theta=theta, \n",
    "        epoch_fda_steps=epoch_max_fda_steps, \n",
    "        ams_sketch=ams_sketch, \n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3. compute metrics\n",
    "    \n",
    "    loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"sketch\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc, \n",
    "        sketch_width=ams_sketch.width, \n",
    "        sketch_depth=ams_sketch.depth\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "    \n",
    "    time_series_metrics = create_time_series_metrics(\n",
    "        time_series_data=time_series_data.numpy(),\n",
    "        dataset_name=\"EMNIST\",\n",
    "        fda_name=\"sketch\", \n",
    "        epochs=NUM_EPOCHS,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        theta=THETA, \n",
    "        num_weights=count_weights(server_cnn), \n",
    "        seed=seed, \n",
    "        sketch_width=ams_sketch.width, \n",
    "        sketch_depth=ams_sketch.depth\n",
    "    )\n",
    "    \n",
    "    basic_test_time_series_metrics.append(time_series_metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, time_series_data, time_series_metrics, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    return basic_test_metrics, basic_test_time_series_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f04a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retracing naive\n",
      "retracing linear\n",
      "retracing sketch\n"
     ]
    }
   ],
   "source": [
    "basic_test_metrics, basic_test_time_series_metrics = testing_stuff(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b0f89-3892-40d7-bdd8-abfc7393bca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-212] *",
   "language": "python",
   "name": "conda-env-tf-212-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
