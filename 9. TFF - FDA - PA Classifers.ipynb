{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16945920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5666e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.50.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db373c",
   "metadata": {},
   "source": [
    "## Create Binary Classification data with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8936c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 100000\n",
    "d = 20\n",
    "noise_factor = 0.05\n",
    "test_size = 0.1 # % of n\n",
    "\n",
    "# Create (noisy) testing data for binary classification.\n",
    "X, y = make_classification(\n",
    "    n_samples=n, \n",
    "    n_features=d,\n",
    "    n_informative=d,\n",
    "    n_redundant=0, \n",
    "    n_classes=2,\n",
    "    class_sep=-1,\n",
    "    flip_y=noise_factor\n",
    ")\n",
    "\n",
    "# We will work with label values -1, +1 and not 0, +1 (convert)\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180164a",
   "metadata": {},
   "source": [
    "## Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d1b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the data to TensorFlow tensors\n",
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc001d5",
   "metadata": {},
   "source": [
    "## Prepare data for Tensorflow Federated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2921d7d",
   "metadata": {},
   "source": [
    "We have the training and testing Tensors holding our data. TFF expects for each client an `OrderedDict` containing `y` and `x` data. Hence, we preprocess our Tensors to follow this convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e2091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLIENTS = 8\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE_BUFFER = 96\n",
    "BATCHES_PER_STEP = 1 # How many batches until we check RTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec76f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches per client: 781\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of batches per client: {int(n / (NUM_CLIENTS*BATCH_SIZE))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5c09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "\n",
    "# Create a dictionary with the slices for each client\n",
    "client_slices_train = {}\n",
    "slices_test = {}\n",
    "\n",
    "n_test = int(n - n*test_size)\n",
    "\n",
    "for i in range(NUM_CLIENTS):\n",
    "    # Compute the indices for this client's slice\n",
    "    start_idx = int(i * n_test / NUM_CLIENTS)\n",
    "    end_idx = int((i + 1) * n_test / NUM_CLIENTS)\n",
    "\n",
    "    # Get the slice for this client\n",
    "    X_client_train = X_train_tensor[start_idx:end_idx]\n",
    "    y_client_train = y_train_tensor[start_idx:end_idx]\n",
    "    \n",
    "    client_data_train = collections.OrderedDict([('y', y_client_train), ('x', X_client_train)])\n",
    "    \n",
    "    # Combine the slices into a single dataset\n",
    "    client_slices_train[f'client_{i}'] = client_data_train\n",
    "\n",
    "slices_test = collections.OrderedDict([('y', y_test_tensor), ('x', X_test_tensor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c178a1f",
   "metadata": {},
   "source": [
    "For a sanity check let's see inside `client_slices_train` for the first x,y tuple of the 'first' client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f169ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([ 1.9104669 , -2.1262722 , -0.57036614,  0.55053246,  1.074606  ,\n",
       "       -1.7338465 , -1.2754428 , -2.7260678 ,  0.40247935,  1.2480997 ,\n",
       "       -0.05747155,  2.175786  , -1.0639805 ,  3.198423  , -1.7250559 ,\n",
       "       -0.5827713 ,  2.8746974 , -1.0082754 , -0.3457826 ,  1.5670983 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_slices_train['client_0']['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e19090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_slices_train['client_0']['y'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192acbc",
   "metadata": {},
   "source": [
    "Now, a client with `client_id` has it's single Tensor holding instances in`client_slices_train[client_id]['x']` and labels in `client_slices_train[client_id]['y']`. Let's take a step back from TFF. Having this data scheme, we can create a client's Tensorflow dataset using `from_tensor_slices` function passing the client's id as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c683cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tf_dataset_for_client(client_id):\n",
    "    return tf.data.Dataset.from_tensor_slices(client_slices_train[client_id]).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).take(BATCHES_PER_STEP)\n",
    "\n",
    "def create_tf_dataset_for_test():\n",
    "    return tf.data.Dataset.from_tensor_slices(slices_test).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef737537",
   "metadata": {},
   "source": [
    "For TFF we need to construct Federated data for clients, i.e., `tff.simulation.datasets.ClientData`. We can use the `from_clients_and_tf_fn` function that takes as argument the `client_ids` : a list of strings corresponding to client ids, and a `serializable_dataset_fn` : a function that takes a `client_id` from the above list, and returns a `tf.data.Dataset`. It's obvious how we proceed with the code (using the above function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b6c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_train_federated_dataset = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "    client_ids=list(client_slices_train.keys()),\n",
    "    serializable_dataset_fn=lambda client_id: create_tf_dataset_for_client(client_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1acb22b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client_0',\n",
       " 'client_1',\n",
       " 'client_2',\n",
       " 'client_3',\n",
       " 'client_4',\n",
       " 'client_5',\n",
       " 'client_6',\n",
       " 'client_7']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_federated_dataset.client_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a74b7",
   "metadata": {},
   "source": [
    "**Note**: Cross-device federated learning does not use client IDs or perform any tracking of clients. However in simulation experiments using centralized test data the experimenter may select specific clients to be processed per round. The concept of a client ID is only available at the preprocessing stage when preparing input data for the simulation and is not part of the TensorFlow Federated core APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde8820",
   "metadata": {},
   "source": [
    "Now, `preprocessed_train_federated_dataset` holds logic on how each client constructs its dataset. Note that so `client_slices_train` has already been materialized and lies in this context's memory.\n",
    "\n",
    "One way (the simplest) to feed federated data to TFF in a simulation is simply as a Python list, with each element of the list holds the data of an individual client, whether as a list or preferably as a `tf.data.Dataset`. Since we already created an interface that provides the latter we will use it. Here is a helper function that will construct a list of datasets from the set of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c68b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_federated_data():    \n",
    "    return [\n",
    "        preprocessed_train_federated_dataset.create_tf_dataset_for_client(client)\n",
    "        for client in preprocessed_train_federated_dataset.client_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f39ef",
   "metadata": {},
   "source": [
    "**Important Note**: Firstly, we used `sklearn` to create the binary classification data eagerly, i.e., we were forced to materialize it into memory. In simulation, in general it is more sound to push preprocessing logic into each client, i.e., each client constructs its own dataset (from the same underlying distribution) or reads from a file or something else and he, himself processes the data as needed. This is the best approach and uses the TFF distributed engine the best way. But in our case this was illogical to happen since we are forced to construct the dataset in memory anyway. For example, we could have stored each client's data inside some serialized file (`client_0.tfrecord` for the first client and so on) and push logic where each clients diserializes and processes its own data but this would be silly and slower when testing. For a small example that showcases this scenario see *TFF - Introduction - Federated Core API - Part 3(examples).ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2cafb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/60265798/tff-how-define-tff-simulation-clientdata-from-clients-and-fn-function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b3cc5",
   "metadata": {},
   "source": [
    "## TFF Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564093e5",
   "metadata": {},
   "source": [
    "Let's start with a simple float32 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c31c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT32_TYPE = tff.TensorType(dtype=tf.float32, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af4cde2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(FLOAT32_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dd832",
   "metadata": {},
   "source": [
    "1-dimensional tensor (vector) of length 1 with elements of type float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "315bfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT32_VECTOR_TYPE = tff.TensorType(dtype=tf.float32, shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ceb58",
   "metadata": {},
   "source": [
    "The local client state $ S_i(t) $ as defined in the unpublished paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a13d33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_STATE = tff.FederatedType(FLOAT32_VECTOR_TYPE, tff.CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "781e9ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{float32[1]}@CLIENTS'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CLIENT_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3c683",
   "metadata": {},
   "source": [
    "First, let's define the type of input as a TFF named tuple. Since the size of data batches may vary, we set the batch dimension to None to indicate that the size of this dimension is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47b88ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SPEC = collections.OrderedDict(\n",
    "    y=tf.TensorSpec(shape=[None], dtype=tf.float32),\n",
    "    x=tf.TensorSpec(shape=[None, d], dtype=tf.float32)\n",
    ")\n",
    "BATCH_TYPE = tff.to_type(BATCH_SPEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d69a37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<y=float32[?],x=float32[?,20]>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(BATCH_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a844eba",
   "metadata": {},
   "source": [
    "Every client holds a sequence of batches so the we define the client data type as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bd7c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOCAL_DATA_TYPE = tff.SequenceType(BATCH_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f9890c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<y=float32[?],x=float32[?,20]>*'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(LOCAL_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dedbfb",
   "metadata": {},
   "source": [
    "Let's now define the TFF type of the model which is simply a `tf.Variable` with shape (d, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1625384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_TYPE = tff.TensorType(dtype=tf.float32, shape=(d, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4db97c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32[20,1]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbc608",
   "metadata": {},
   "source": [
    "Since the server holds the 'global' model we need to create the Federated Type, defined as the tuple of a member: An instance of `tff.Type`, and a placement: The specification of placement of the member comonents (where this type is hosted at, for example, at `tff.SERVER` or `tff.CLIENTS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93453747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SERVER_MODEL_TYPE = tff.type_at_server(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "424c7703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32[20,1]@SERVER'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(SERVER_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd0413",
   "metadata": {},
   "source": [
    "Following, the same logic, we create the Federated Type of each client's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5a56be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLIENT_DATA_TYPE = tff.type_at_clients(LOCAL_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cc37383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{<y=float32[?],x=float32[?,20]>*}@CLIENTS'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CLIENT_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b43e2d",
   "metadata": {},
   "source": [
    "We will also need to define the client models at the CLIENTS (for FDA later, to be cont...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99246ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_MODEL_TYPE = tff.type_at_clients(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd991011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{float32[20,1]}@CLIENTS'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CLIENT_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af966b0",
   "metadata": {},
   "source": [
    "## Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2427e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def accuracy(model, dataset):\n",
    "    \n",
    "    @tf.function\n",
    "    def _batch_accuracy(model, batch):\n",
    "        x_batch, y_batch = batch['x'], tf.expand_dims(batch['y'], axis=1)\n",
    "\n",
    "        # dot(w, x) for the batch (each instance of x in x_batch) with with shape=(batchsize, 1)\n",
    "        weights_dot_x_batch = tf.matmul(x_batch, model)\n",
    "\n",
    "        # Prediction batch with shape=(batchsize, 1)\n",
    "        y_pred_batch = tf.sign(weights_dot_x_batch)\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_batch, y_batch), tf.float32))\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    # We take advantage of AutoGraph (convert Python code to TensorFlow-compatible graph code automatically)\n",
    "    acc, num_batches = 0., 0.\n",
    "    for batch in dataset:\n",
    "        acc += _batch_accuracy(model, batch)\n",
    "        num_batches += 1\n",
    "        \n",
    "    acc = acc / num_batches\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7db2f5a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@tff.tf_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
    "def accuracy_fn(model, dataset):\n",
    "    model = tf.Variable(initial_value=model)\n",
    "    return accuracy(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecea1204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<model=float32[20,1],dataset=<y=float32[?],x=float32[?,20]>*> -> float32)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(accuracy_fn.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bc43b",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3617a4",
   "metadata": {},
   "source": [
    "### Server Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030037c",
   "metadata": {},
   "source": [
    "The server update takes as input the *average* of the client's models and creates its model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97593144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.tf_computation(MODEL_TYPE)\n",
    "def server_update_fn(mean_client_model):\n",
    "    model = tf.Variable(initial_value=mean_client_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efe13c",
   "metadata": {},
   "source": [
    "**Note**: This abstraction for this simple jupyter (where the model is a `tf.Variable`) is not necessary. We create this abstraction since it is common practice generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7d7d6",
   "metadata": {},
   "source": [
    "### Client train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf37e7f",
   "metadata": {},
   "source": [
    "Each client trains on its own dataset (which is a sequence of batches). Hence, we create the training process, currently a PA-1 Classifier. The input of `client_train` is the client model materialized inside its client and its dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4758084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def client_train(model, dataset):\n",
    "    \n",
    "    @tf.function\n",
    "    def _train_on_batch(model, batch, C=0.01):\n",
    "\n",
    "        x_batch, y_batch = batch['x'], tf.expand_dims(batch['y'], axis=1)\n",
    "\n",
    "        # dot(w, x) for the batch (each instance of x in x_batch) with with shape=(batchsize, 1)\n",
    "        weights_dot_x_batch = tf.matmul(x_batch, model)\n",
    "\n",
    "        # Prediction batch with shape=(batchsize, 1)\n",
    "        y_pred_batch = tf.sign(weights_dot_x_batch)\n",
    "\n",
    "        # Suffer loss for each prediction (of instance) in the batch with shape=(batchsize,1)\n",
    "        loss_batch = tf.maximum(0., 1. - tf.multiply(y_batch, weights_dot_x_batch))\n",
    "\n",
    "        # shape=(batchsize,1) where each instance is ||x||^2, x in x_batch\n",
    "        norm_batch = tf.expand_dims(tf.reduce_sum(tf.square(x_batch), axis=1), axis=1)\n",
    "\n",
    "        # PA-1 : Learning rate t for each instance x, with shape=(batchsize,1)\n",
    "        t_batch = tf.maximum(C, tf.divide(loss_batch, norm_batch))\n",
    "\n",
    "        # each instance is y*t*x, where y,t scalars and x in x_batch. shape=(batchsize,d)\n",
    "        t_y_x_batch = tf.multiply(t_batch, tf.multiply(y_batch, x_batch))\n",
    "\n",
    "        # !!!! Update with mean t*y*x\n",
    "        t_y_x_update = tf.expand_dims(tf.reduce_mean(t_y_x_batch, axis=0) ,axis=1)\n",
    "\n",
    "        # Update\n",
    "        model.assign_add(t_y_x_update)\n",
    "    \n",
    "    for batch in dataset:\n",
    "        _train_on_batch(model, batch)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32448c2f",
   "metadata": {},
   "source": [
    "# Functional Dynamic Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5498899",
   "metadata": {},
   "source": [
    "We follow the Functional Dynamic Averaging (FDA) scheme. Let the mean model be\n",
    "\n",
    "$$ \\overline{w_t} = \\frac{1}{k} \\sum_{i=1}^{k} w_t^{(i)} $$\n",
    "\n",
    "where $ w_t^{(i)} $ is the model at time $ t $ in some round in the $i$-th learner.\n",
    "\n",
    "Local models are trained independently and cooperatively and we want to monitor the Round Terminating Conditon (**RTC**):\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2  \\leq \\Theta $$\n",
    "\n",
    "where the left-hand side is the **model variance**, and threshold $\\Theta$ is a hyperparameter of the FDA, defined at the beginning of the round; it may change at each round. When the monitoring logic cannot guarantee the validity of RTC, the round terminates. All local models are pulled into `tff.SERVER`, and $\\bar{w_t}$ is set to their average. Then, another round begins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0629d",
   "metadata": {},
   "source": [
    "### Monitoring the RTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35706d3f",
   "metadata": {},
   "source": [
    "FDA monitors the RTC by applying techniques from Functionary [Functional Geometric Averaging](http://users.softnet.tuc.gr/~minos/Papers/edbt19.pdf). We first restate the problem of monitoring RTC into the standard distributed stream monitoring formulation. Let\n",
    "\n",
    "$$ S(t) =  \\frac{1}{k} \\sum_{i=1}^{k} S_i(t) $$\n",
    "\n",
    "where $ S(t) \\in \\mathbb{R}^n $ be the \"global state\" of the system and $ S_i(t) \\in \\mathbb{R}^n $ the \"local states\". The goal is to monitor the threshold condition on the global state in the form $ F(S(t)) \\leq \\Theta $ where $ F : \\mathbb{R}^n \\to \\mathbb{R} $ a non-linear function. Let\n",
    "\n",
    "$$ \\Delta_t^{(i)} = w_t^{(i)} - w_{t_0}^{(i)} $$\n",
    "\n",
    "be the update at the $ i $-th learner, that is, the change to the local model at time $t$ since the beginning of the current round at time $t_0$. Let the average update be\n",
    "\n",
    "$$ \\overline{\\Delta_t} = \\frac{1}{k} \\sum_{i=1}^{k} \\Delta_t^{(i)} $$\n",
    "\n",
    "it follows that the variance can be written as\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2 = \\Big( \\frac{1}{k} \\sum_{i=1}^{k} \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\Big) - \\lVert \\overline{\\Delta_t} \\rVert_2^2 $$\n",
    "\n",
    "So, conceptually, if we define\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\bf{x}\n",
    "         \\end{bmatrix}) = v - \\lVert \\bf{x} \\rVert_2^2 $$\n",
    "\n",
    "The RTC is equivalent to condition $$ F(S(t)) \\leq \\Theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2389e",
   "metadata": {},
   "source": [
    "## 1️⃣ Naive FDA\n",
    "\n",
    "In the naive approach, we eliminate the update vector from the local state (i.e. recuce the dimension to 0). Define local state as\n",
    "\n",
    "$$ S_i(t) = \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\in \\mathbb{R}$$ \n",
    "\n",
    "and the identity function\n",
    "\n",
    "$$ F(v) = v $$\n",
    "\n",
    "It is trivial that $ F(S(t)) \\leq \\Theta $ implies the RTC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4743f",
   "metadata": {},
   "source": [
    "Using the functions decorated with `tf.function` (context inside Tensorflow) we create the `client_train_fn` with context inside TFF. `client_train_fn` takes as input the `initial_model` which is the model broadcasted from the server to each client and the client dataset. Notice that each client first creates it's own model using the server model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "888be5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: last synchronized model. Not initial model\n",
    "@tff.tf_computation(MODEL_TYPE, MODEL_TYPE, LOCAL_DATA_TYPE)\n",
    "def client_train_fn(last_sync_model, initial_model, dataset):\n",
    "    \n",
    "    model = client_train(\n",
    "        tf.Variable(initial_value=initial_model), \n",
    "        dataset\n",
    "    )\n",
    "    \n",
    "    Delta_i = model - last_sync_model # AutoGraph\n",
    "    S_i = tf.reduce_sum(tf.square(Delta_i), axis=0) # ||D(t)_i||^2\n",
    "\n",
    "    return model, S_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "349b9ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<last_sync_model=float32[20,1],initial_model=float32[20,1],dataset=<y=float32[?],x=float32[?,20]>*> -> <float32[20,1],float32[1]>)'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(client_train_fn.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d317f2",
   "metadata": {},
   "source": [
    "### Training Round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a40a92",
   "metadata": {},
   "source": [
    "Remember the 4 elements of an FL round:\n",
    "\n",
    "1. A server-to-client broadcast of the weights.\n",
    "2. A local client training 'step' on its own data.\n",
    "3. A client-to-server upload step (returning the trained weights).\n",
    "4. A server update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07ef3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation(CLIENT_MODEL_TYPE)\n",
    "def server_update(client_models):\n",
    "    # 4. Compute the mean of the client weights\n",
    "    mean_client_model = tff.federated_mean(client_models)\n",
    "    \n",
    "    # 4. Update the server model\n",
    "    server_model = tff.federated_map(server_update_fn, mean_client_model)\n",
    "    \n",
    "    return server_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c71d14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'({float32[20,1]}@CLIENTS -> float32[20,1]@SERVER)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(server_update.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d41b3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation(CLIENT_MODEL_TYPE, CLIENT_MODEL_TYPE, CLIENT_DATA_TYPE)\n",
    "def step(last_sync_client_models, client_models, federated_dataset):\n",
    "    # 2. 3. Train the client models on their respective datasets\n",
    "    client_models, client_S_i = tff.federated_map(\n",
    "        client_train_fn, \n",
    "        (last_sync_client_models, client_models, federated_dataset)\n",
    "    )\n",
    "    \n",
    "    return client_models, client_S_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fddd7dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<last_sync_client_models={float32[20,1]}@CLIENTS,client_models={float32[20,1]}@CLIENTS,federated_dataset={<y=float32[?],x=float32[?,20]>*}@CLIENTS> -> <{float32[20,1]}@CLIENTS,{float32[1]}@CLIENTS>)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(step.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6303a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation(SERVER_MODEL_TYPE, CLIENT_DATA_TYPE)\n",
    "def synchronize_and_step(server_model, federated_dataset):\n",
    "    # 1. Broadcast the current server model to the clients\n",
    "    server_model_at_client = tff.federated_broadcast(server_model)\n",
    "    \n",
    "    # 2. 3. Train the client models on their respective datasets\n",
    "    client_models, client_S_i = tff.federated_map(\n",
    "        client_train_fn, \n",
    "        (server_model_at_client, server_model_at_client, federated_dataset)\n",
    "    )\n",
    "    \n",
    "    return client_models, client_S_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "335a4c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<server_model=float32[20,1]@SERVER,federated_dataset={<y=float32[?],x=float32[?,20]>*}@CLIENTS> -> <{float32[20,1]}@CLIENTS,{float32[1]}@CLIENTS>)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(synchronize_and_step.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba2f3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.federated_computation(CLIENT_STATE)\n",
    "def server_global_state(client_S_i):\n",
    "    \n",
    "    server_S = tff.federated_mean(client_S_i)\n",
    "    \n",
    "    return server_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2eefc579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'({float32[1]}@CLIENTS -> float32[1]@SERVER)'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(server_global_state.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1472e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.tf_computation(FLOAT32_VECTOR_TYPE, FLOAT32_TYPE)\n",
    "def RTC_holds(S_t, THETA):\n",
    "    \"\"\" Returns True if RTC holds (has not been defied). False otherwise (sync must happen)\"\"\"\n",
    "    \n",
    "    @tf.function\n",
    "    def _F(S_t, THETA):\n",
    "        \"\"\" Naive FDA \"\"\"\n",
    "        return S_t <= THETA\n",
    "    \n",
    "    return _F(S_t, THETA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91ba3d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<S_t=float32[1],THETA=float32> -> bool[1])'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(RTC_holds.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05037e",
   "metadata": {},
   "source": [
    "## Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f975e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRINT_INFO_BEFORE_SYNC(num_rounds, num_steps, global_state, accuracy):\n",
    "    print(f\"---------------------------------- Round {num_rounds} ----------------------------------------------\")\n",
    "    print(f\"Steps: {num_steps} , Server Model Accuracy: {accuracy} ,  Global State (S_t): {S_t}\")\n",
    "    \n",
    "    \n",
    "def PRINT_INFO_AFTER_SYNC(global_state):    \n",
    "    print(f\"Global State after synchronization and one step: {global_state}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b89426b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model of zeros (in Python context, to be passed to server)\n",
    "model = tf.Variable(tf.zeros(shape=(d, 1)), trainable=True, name='weights', dtype=tf.float32)\n",
    "\n",
    "client_models = [model]*NUM_CLIENTS\n",
    "last_sync_client_models = [model]*NUM_CLIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "742222ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_federated_data = create_federated_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b50f09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = create_tf_dataset_for_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b930ceea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------- Round 0 ----------------------------------------------\n",
      "Steps: 0 , Server Model Accuracy: 0.0 ,  Global State (S_t): [inf]\n",
      "Global State after synchronization: [0.00190214]\n",
      "\n",
      "---------------------------------- Round 1 ----------------------------------------------\n",
      "Steps: 131 , Server Model Accuracy: 0.8648999929428101 ,  Global State (S_t): [10.032373]\n",
      "Global State after synchronization: [0.00338402]\n",
      "\n",
      "---------------------------------- Round 2 ----------------------------------------------\n",
      "Steps: 274 , Server Model Accuracy: 0.8583999872207642 ,  Global State (S_t): [10.082127]\n",
      "Global State after synchronization: [0.00420818]\n",
      "\n",
      "---------------------------------- Round 3 ----------------------------------------------\n",
      "Steps: 411 , Server Model Accuracy: 0.85589998960495 ,  Global State (S_t): [10.002724]\n",
      "Global State after synchronization: [0.0194706]\n",
      "\n",
      "---------------------------------- Round 4 ----------------------------------------------\n",
      "Steps: 541 , Server Model Accuracy: 0.8550000190734863 ,  Global State (S_t): [10.01195]\n",
      "Global State after synchronization: [0.00580706]\n",
      "\n",
      "---------------------------------- Round 5 ----------------------------------------------\n",
      "Steps: 660 , Server Model Accuracy: 0.8543000221252441 ,  Global State (S_t): [10.0637665]\n",
      "Global State after synchronization: [0.01565047]\n",
      "\n",
      "---------------------------------- Round 6 ----------------------------------------------\n",
      "Steps: 767 , Server Model Accuracy: 0.8539999723434448 ,  Global State (S_t): [10.028286]\n",
      "Global State after synchronization: [0.02172038]\n",
      "\n",
      "---------------------------------- Round 7 ----------------------------------------------\n",
      "Steps: 871 , Server Model Accuracy: 0.8540999889373779 ,  Global State (S_t): [10.041206]\n",
      "Global State after synchronization: [0.01002947]\n",
      "\n",
      "---------------------------------- Round 8 ----------------------------------------------\n",
      "Steps: 967 , Server Model Accuracy: 0.8536999821662903 ,  Global State (S_t): [10.025046]\n",
      "Global State after synchronization: [0.01632462]\n",
      "\n",
      "---------------------------------- Round 9 ----------------------------------------------\n",
      "Steps: 1072 , Server Model Accuracy: 0.8540999889373779 ,  Global State (S_t): [10.30848]\n",
      "Global State after synchronization: [0.0064036]\n",
      "\n",
      "\n",
      "Total number of steps: 1073\n"
     ]
    }
   ],
   "source": [
    "S_t = tf.constant([float('inf')], dtype=tf.float32) # Force synchronization at the start\n",
    "THETA = 10\n",
    "\n",
    "num_rounds = 0 \n",
    "num_steps = 0 # Each step invoke is a step\n",
    "        \n",
    "while num_rounds < 10:\n",
    "    \n",
    "    if RTC_holds(S_t, THETA): # RTC holds, no sync needed\n",
    "        \n",
    "        # Perform a training step with the current client_models (no sync yet)\n",
    "        client_models, client_S_i = step(last_sync_client_models, client_models, train_federated_data)\n",
    "        \n",
    "        # Compute 'global state' as defined in the manuscript\n",
    "        S_t = server_global_state(client_S_i)\n",
    "    \n",
    "    else: # RTC defied, sync needed\n",
    "        \n",
    "        # Update the server model from the client models.\n",
    "        model = server_update(client_models)\n",
    "        \n",
    "        PRINT_INFO_BEFORE_SYNC(num_rounds, num_steps, S_t, accuracy_fn(model, test_dataset))\n",
    "        \n",
    "        # Synchronize client models with server model, and perform a train step\n",
    "        client_models, client_S_i = synchronize_and_step(model, train_federated_data)\n",
    "        \n",
    "        last_sync_client_models = [model]*NUM_CLIENTS\n",
    "        \n",
    "        # Compute 'global state' as defined in the manuscript\n",
    "        S_t = server_global_state(client_S_i)\n",
    "        \n",
    "        PRINT_INFO_AFTER_SYNC(S_t)\n",
    "        \n",
    "        num_rounds += 1\n",
    "    \n",
    "    num_steps += 1\n",
    "\n",
    "print()\n",
    "print(f\"Total number of steps: {num_steps}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62052b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51841d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7b8ca38",
   "metadata": {},
   "source": [
    "1. Check correctness Delta_i etc.\n",
    "1. Fix C = 0.01\n",
    "2. Comments + Check approach (maybe pass string \"Naive FDA\" or deduplicate functions)\n",
    "3. comments\n",
    "4. THink about Whilte True.\n",
    "5. Wrap in tff.tf_computation or federated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4b444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tff-py39] *",
   "language": "python",
   "name": "conda-env-tff-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
