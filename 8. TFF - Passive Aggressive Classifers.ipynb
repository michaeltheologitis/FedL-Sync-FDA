{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16945920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5666e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.50.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db373c",
   "metadata": {},
   "source": [
    "## Create Binary Classification data with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8936c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 100000\n",
    "d = 50\n",
    "noise_factor = 0.05\n",
    "test_size = 0.1 # % of n\n",
    "\n",
    "# Create (noisy) testing data for binary classification.\n",
    "X, y = make_classification(\n",
    "    n_samples=n, \n",
    "    n_features=d,\n",
    "    n_informative=d,\n",
    "    n_redundant=0, \n",
    "    n_classes=2,\n",
    "    class_sep=-1,\n",
    "    flip_y=noise_factor\n",
    ")\n",
    "\n",
    "# We will work with label values -1, +1 and not 0, +1 (convert)\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180164a",
   "metadata": {},
   "source": [
    "## Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d1b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the data to TensorFlow tensors\n",
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc001d5",
   "metadata": {},
   "source": [
    "## Prepare data for Tensorflow Federated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2921d7d",
   "metadata": {},
   "source": [
    "We have the training and testing Tensors holding our data. TFF expects for each client an `OrderedDict` containing `y` and `x` data. Hence, we preprocess our Tensors to follow this convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e2091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLIENTS = 8\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9358fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "\n",
    "# Create a dictionary with the slices for each client\n",
    "client_slices_train = {}\n",
    "slices_test = {}\n",
    "\n",
    "n_test = int(n - n*test_size)\n",
    "\n",
    "for i in range(NUM_CLIENTS):\n",
    "    # Compute the indices for this client's slice\n",
    "    start_idx = int(i * n_test / NUM_CLIENTS)\n",
    "    end_idx = int((i + 1) * n_test / NUM_CLIENTS)\n",
    "\n",
    "    # Get the slice for this client\n",
    "    X_client_train = X_train_tensor[start_idx:end_idx]\n",
    "    y_client_train = y_train_tensor[start_idx:end_idx]\n",
    "    \n",
    "    client_data_train = collections.OrderedDict([('y', y_client_train), ('x', X_client_train)])\n",
    "    \n",
    "    # Combine the slices into a single dataset\n",
    "    client_slices_train[f'client_{i}'] = client_data_train\n",
    "\n",
    "slices_test = collections.OrderedDict([('y', y_test_tensor), ('x', X_test_tensor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142d80d",
   "metadata": {},
   "source": [
    "For a sanity check let's see inside `client_slices_train` for the first x,y tuple of the 'first' client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1512cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=float32, numpy=\n",
       "array([-1.7185166 , -5.0518475 ,  0.02975657,  4.507718  , -2.9012241 ,\n",
       "       -1.3904775 , -0.6722793 , -5.201269  ,  0.4005102 ,  5.250195  ,\n",
       "       -5.3100247 ,  0.3369487 ,  4.662774  ,  5.5472927 ,  3.9250505 ,\n",
       "        2.3702579 ,  4.892519  ,  1.3388315 ,  2.2262855 , -1.2632746 ,\n",
       "       -1.6392467 ,  3.9341595 ,  0.9980433 , -0.2969859 , -1.6189998 ,\n",
       "        2.2584572 , -1.639607  , -2.4005718 ,  2.3321502 , -4.670214  ,\n",
       "        3.8024392 ,  5.2460275 , -1.852771  ,  0.7435165 ,  0.54761195,\n",
       "       -5.3764625 , -0.12647538, -3.522802  ,  8.834553  ,  8.7223625 ,\n",
       "       -3.9466345 ,  4.4238544 ,  3.8179338 , -0.90905523, -0.07614926,\n",
       "       -4.1437864 ,  2.8217857 ,  0.72625434, -0.56228954, -0.7264684 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_slices_train['client_0']['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5091300e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_slices_train['client_0']['y'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b2536",
   "metadata": {},
   "source": [
    "Now, a client with `client_id` has it's single Tensor holding instances in`client_slices_train[client_id]['x']` and labels in `client_slices_train[client_id]['y']`. Let's take a step back from TFF. Having this data scheme, we can create a client's Tensorflow dataset using `from_tensor_slices` function passing the client's id as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a45a007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tf_dataset_for_client(client_id):\n",
    "    return tf.data.Dataset.from_tensor_slices(client_slices_train[client_id]).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "\n",
    "def create_tf_dataset_for_test():\n",
    "    return tf.data.Dataset.from_tensor_slices(slices_test).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef737537",
   "metadata": {},
   "source": [
    "For TFF we need to construct Federated data for clients, i.e., `tff.simulation.datasets.ClientData`. We can use the `from_clients_and_tf_fn` function that takes as argument the `client_ids` : a list of strings corresponding to client ids, and a `serializable_dataset_fn` : a function that takes a `client_id` from the above list, and returns a `tf.data.Dataset`. It's obvious how we proceed with the code (using the above function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df86699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_train_federated_dataset = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "    client_ids=list(client_slices_train.keys()),\n",
    "    serializable_dataset_fn=lambda client_id: create_tf_dataset_for_client(client_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1acb22b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client_0',\n",
       " 'client_1',\n",
       " 'client_2',\n",
       " 'client_3',\n",
       " 'client_4',\n",
       " 'client_5',\n",
       " 'client_6',\n",
       " 'client_7']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_federated_dataset.client_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07ff87",
   "metadata": {},
   "source": [
    "**Note**: Cross-device federated learning does not use client IDs or perform any tracking of clients. However in simulation experiments using centralized test data the experimenter may select specific clients to be processed per round. The concept of a client ID is only available at the preprocessing stage when preparing input data for the simulation and is not part of the TensorFlow Federated core APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49288eb0",
   "metadata": {},
   "source": [
    "Now, `preprocessed_train_federated_dataset` holds logic on how each client constructs its dataset. Note that so `client_slices_train` has already been materialized and lies in this context's memory.\n",
    "\n",
    "One way (the simplest) to feed federated data to TFF in a simulation is simply as a Python list, with each element of the list holds the data of an individual client, whether as a list or preferably as a `tf.data.Dataset`. Since we already created an interface that provides the latter we will use it. Here is a helper function that will construct a list of datasets from the set of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364392af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_federated_data():    \n",
    "    return [\n",
    "        preprocessed_train_federated_dataset.create_tf_dataset_for_client(client)\n",
    "        for client in preprocessed_train_federated_dataset.client_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a82d1",
   "metadata": {},
   "source": [
    "**Important Note**: Firstly, we used `sklearn` to create the binary classification data eagerly, i.e., we were forced to materialize it into memory. In simulation, in general it is more sound to push preprocessing logic into each client, i.e., each client constructs its own dataset (from the same underlying distribution) or reads from a file or something else and he, himself processes the data as needed. This is the best approach and uses the TFF distributed engine the best way. But in our case this was illogical to happen since we are forced to construct the dataset in memory anyway. For example, we could have stored each client's data inside some serialized file (`client_0.tfrecord` for the first client and so on) and push logic where each clients diserializes and processes its own data but this would be silly and slower when testing. For a small example that showcases this scenario see *TFF - Introduction - Federated Core API - Part 3(examples).ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0ba792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/60265798/tff-how-define-tff-simulation-clientdata-from-clients-and-fn-function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94880a",
   "metadata": {},
   "source": [
    "## TFF Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3c683",
   "metadata": {},
   "source": [
    "First, let's define the type of input as a TFF named tuple. Since the size of data batches may vary, we set the batch dimension to None to indicate that the size of this dimension is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47b88ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SPEC = collections.OrderedDict(\n",
    "    y=tf.TensorSpec(shape=[None], dtype=tf.float32),\n",
    "    x=tf.TensorSpec(shape=[None, d], dtype=tf.float32)\n",
    ")\n",
    "BATCH_TYPE = tff.to_type(BATCH_SPEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d69a37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<y=float32[?],x=float32[?,50]>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(BATCH_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0d942",
   "metadata": {},
   "source": [
    "Every client holds a sequence of batches so the we define the client data type as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44198932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOCAL_DATA_TYPE = tff.SequenceType(BATCH_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7af4c4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<y=float32[?],x=float32[?,50]>*'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(LOCAL_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dedbfb",
   "metadata": {},
   "source": [
    "Let's now define the TFF type of the model which is simply a `tf.Variable` with shape (d, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1625384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_TYPE = tff.TensorType(dtype=tf.float32, shape=(d, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4db97c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32[50,1]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefc47e",
   "metadata": {},
   "source": [
    "Since the server holds the 'global' model we need to create the Federated Type, defined as the tuple of a member: An instance of `tff.Type`, and a placement: The specification of placement of the member comonents (where this type is hosted at, for example, at `tff.SERVER` or `tff.CLIENTS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6871d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SERVER_MODEL_TYPE = tff.type_at_server(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ba2dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32[50,1]@SERVER'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(SERVER_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d50f3d",
   "metadata": {},
   "source": [
    "Following, the same logic, we create the Federated Type of each client's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8911394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLIENT_DATA_TYPE = tff.type_at_clients(LOCAL_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "287eb78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{<y=float32[?],x=float32[?,50]>*}@CLIENTS'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CLIENT_DATA_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054b762",
   "metadata": {},
   "source": [
    "## Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2427e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def accuracy(model, dataset):\n",
    "    \n",
    "    @tf.function\n",
    "    def _batch_accuracy(model, batch):\n",
    "        x_batch, y_batch = batch['x'], tf.expand_dims(batch['y'], axis=1)\n",
    "\n",
    "        # dot(w, x) for the batch (each instance of x in x_batch) with with shape=(batchsize, 1)\n",
    "        weights_dot_x_batch = tf.matmul(x_batch, model)\n",
    "\n",
    "        # Prediction batch with shape=(batchsize, 1)\n",
    "        y_pred_batch = tf.sign(weights_dot_x_batch)\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_batch, y_batch), tf.float32))\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    # We take advantage of AutoGraph (convert Python code to TensorFlow-compatible graph code automatically)\n",
    "    acc, num_batches = 0., 0.\n",
    "    for batch in dataset:\n",
    "        acc += _batch_accuracy(model, batch)\n",
    "        num_batches += 1\n",
    "        \n",
    "    acc = acc / num_batches\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7db2f5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@tff.tf_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
    "def accuracy_fn(model, dataset):\n",
    "    model = tf.Variable(initial_value=model)\n",
    "    return accuracy(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecea1204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<model=float32[50,1],dataset=<y=float32[?],x=float32[?,50]>*> -> float32)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(accuracy_fn.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c90ca",
   "metadata": {},
   "source": [
    "## Federated Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc23d34",
   "metadata": {},
   "source": [
    "### Server Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e93071",
   "metadata": {},
   "source": [
    "The server update takes as input the *average* of the client's models and creates its model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2207ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.tf_computation(MODEL_TYPE)\n",
    "def server_update_fn(mean_client_model):\n",
    "    model = tf.Variable(initial_value=mean_client_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef9772",
   "metadata": {},
   "source": [
    "**Note**: This abstraction for this simple jupyter (where the model is a `tf.Variable`) is not necessary. We create this abstraction since it is common practice generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf93983",
   "metadata": {},
   "source": [
    "### Client train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd219b9",
   "metadata": {},
   "source": [
    "Each client trains on its own dataset (which is a sequence of batches). Hence, we create the training process, currently a PA-1 Classifier. The input of `client_train` is the client model materialized inside its client and its dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc8b1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def client_train(model, dataset):\n",
    "    \n",
    "    @tf.function\n",
    "    def _train_on_batch(model, batch, C=0.01):\n",
    "\n",
    "        x_batch, y_batch = batch['x'], tf.expand_dims(batch['y'], axis=1)\n",
    "\n",
    "        # dot(w, x) for the batch (each instance of x in x_batch) with with shape=(batchsize, 1)\n",
    "        weights_dot_x_batch = tf.matmul(x_batch, model)\n",
    "\n",
    "        # Prediction batch with shape=(batchsize, 1)\n",
    "        y_pred_batch = tf.sign(weights_dot_x_batch)\n",
    "\n",
    "        # Suffer loss for each prediction (of instance) in the batch with shape=(batchsize,1)\n",
    "        loss_batch = tf.maximum(0., 1. - tf.multiply(y_batch, weights_dot_x_batch))\n",
    "\n",
    "        # shape=(batchsize,1) where each instance is ||x||^2, x in x_batch\n",
    "        norm_batch = tf.expand_dims(tf.reduce_sum(tf.square(x_batch), axis=1), axis=1)\n",
    "\n",
    "        # PA-1 : Learning rate t for each instance x, with shape=(batchsize,1)\n",
    "        t_batch = tf.maximum(C, tf.divide(loss_batch, norm_batch))\n",
    "\n",
    "        # each instance is y*t*x, where y,t scalars and x in x_batch. shape=(batchsize,d)\n",
    "        t_y_x_batch = tf.multiply(t_batch, tf.multiply(y_batch, x_batch))\n",
    "\n",
    "        # !!!! Update with mean t*y*x\n",
    "        t_y_x_update = tf.expand_dims(tf.reduce_mean(t_y_x_batch, axis=0) ,axis=1)\n",
    "\n",
    "        # Update\n",
    "        model.assign_add(t_y_x_update)\n",
    "    \n",
    "    for batch in dataset:\n",
    "        _train_on_batch(model, batch)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590da30d",
   "metadata": {},
   "source": [
    "Using the functions decorated with `tf.function` (context inside Tensorflow) we create the `client_train_fn` with context inside TFF. `client_train_fn` takes as input the `initial_model` which is the model broadcasted from the server to each client and the client dataset. Notice that each client first creates it's own model using the server model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "888be5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.tf_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
    "def client_train_fn(initial_model, dataset):\n",
    "    model = tf.Variable(initial_value=initial_model)\n",
    "    return client_train(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "349b9ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<initial_model=float32[50,1],dataset=<y=float32[?],x=float32[?,50]>*> -> float32[50,1])'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(client_train_fn.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d832b",
   "metadata": {},
   "source": [
    "### Training Round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5525812",
   "metadata": {},
   "source": [
    "Remember the 4 elements of an FL round:\n",
    "\n",
    "1. A server-to-client broadcast of the weights.\n",
    "2. A local client training 'step' on its own data.\n",
    "3. A client-to-server upload step (returning the trained weights).\n",
    "4. A server update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7633c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tff.federated_computation(SERVER_MODEL_TYPE, CLIENT_DATA_TYPE)\n",
    "def run_one_round(server_model, federated_dataset):\n",
    "    \n",
    "    # 1. Broadcast the current server model to the clients\n",
    "    server_model_at_client = tff.federated_broadcast(server_model)\n",
    "    \n",
    "    # 2. 3. Train the client models on their respective datasets\n",
    "    client_models = tff.federated_map(client_train_fn, (server_model_at_client, federated_dataset))\n",
    "    \n",
    "    # 4. Compute the mean of the client weights\n",
    "    mean_client_model = tff.federated_mean(client_models)\n",
    "    \n",
    "    # 4. Update the server model\n",
    "    server_model = tff.federated_map(server_update_fn, mean_client_model)\n",
    "    \n",
    "    return server_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f71a12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<server_model=float32[50,1]@SERVER,federated_dataset={<y=float32[?],x=float32[?,50]>*}@CLIENTS> -> float32[50,1]@SERVER)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(run_one_round.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35984795",
   "metadata": {},
   "source": [
    "## Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9150356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_ROUNDS = 20\n",
    "\n",
    "# Initial model of zeros (in Python context, to be passed to server)\n",
    "model = tf.Variable(tf.zeros(shape=(d, 1)), trainable=True, name='weights', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b21961b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_federated_data = create_federated_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa8bd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = create_tf_dataset_for_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73f9d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Server Model Accuracy: 0.8102036714553833\n",
      "Round: 1  Server Model Accuracy: 0.8213858008384705\n",
      "Round: 2  Server Model Accuracy: 0.8266773223876953\n",
      "Round: 3  Server Model Accuracy: 0.8282747864723206\n",
      "Round: 4  Server Model Accuracy: 0.8309704661369324\n",
      "Round: 5  Server Model Accuracy: 0.8332667946815491\n",
      "Round: 6  Server Model Accuracy: 0.8334664702415466\n",
      "Round: 7  Server Model Accuracy: 0.8340654969215393\n",
      "Round: 8  Server Model Accuracy: 0.8343650102615356\n",
      "Round: 9  Server Model Accuracy: 0.834664523601532\n",
      "Round: 10  Server Model Accuracy: 0.8343650102615356\n",
      "Round: 11  Server Model Accuracy: 0.8344648480415344\n",
      "Round: 12  Server Model Accuracy: 0.8343650102615356\n",
      "Round: 13  Server Model Accuracy: 0.8344648480415344\n",
      "Round: 14  Server Model Accuracy: 0.8347643613815308\n",
      "Round: 15  Server Model Accuracy: 0.8345646858215332\n",
      "Round: 16  Server Model Accuracy: 0.8342651724815369\n",
      "Round: 17  Server Model Accuracy: 0.8342651724815369\n",
      "Round: 18  Server Model Accuracy: 0.8340654969215393\n",
      "Round: 19  Server Model Accuracy: 0.8341653347015381\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for r in range(NUM_ROUNDS):\n",
    "    model = run_one_round(model, train_federated_data)\n",
    "    print(f\"Round: {r}  Server Model Accuracy: {accuracy_fn(model, test_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tff-py39] *",
   "language": "python",
   "name": "conda-env-tff-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
