{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0733e01",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8002636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a90400",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Keras provides default training and evaluation loops, `fit()` and `evaluate()`.\n",
    "Their usage is covered in the guide\n",
    "[Training & evaluation with the built-in methods](https://www.tensorflow.org/guide/keras/train_and_evaluate/).\n",
    "\n",
    "If you want to customize the learning algorithm of your model while still leveraging\n",
    "the convenience of `fit()`\n",
    "(for instance, to train a GAN using `fit()`), you can subclass the `Model` class and\n",
    "implement your own `train_step()` method, which\n",
    "is called repeatedly during `fit()`. This is covered in the guide\n",
    "[Customizing what happens in `fit()`](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/).\n",
    "\n",
    "Now, if you want very low-level control over training & evaluation, you should write\n",
    "your own training & evaluation loops from scratch. This is what this guide is about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0754708",
   "metadata": {},
   "source": [
    "## Using the `GradientTape`: end-to-end example\n",
    "\n",
    "Calling a model inside a `GradientTape` scope enables us to retrieve the gradients of\n",
    "the trainable weights of the layer with respect to a loss value. Using an optimizer\n",
    "instance, we can use these gradients to update these variables (which we can\n",
    "retrieve using `model.trainable_weights`).\n",
    "\n",
    "Let's consider a simple MNIST model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be4bdf",
   "metadata": {},
   "source": [
    "### Functional Model (more flexible API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b457cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02526b5b",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bbc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(784,), name=\"digits\"))\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, name=\"predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8a8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(784,), name=\"digits\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(10, name=\"predictions\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f51fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9201ef",
   "metadata": {},
   "source": [
    "Let's train it using mini-batch gradient with a custom training loop.\n",
    "\n",
    "First, we're going to need an optimizer, a loss function, and a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad66b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "test_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the testing dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b80b4b",
   "metadata": {},
   "source": [
    "Here's our training loop:\n",
    "\n",
    "- We open a `for` loop that iterates over epochs\n",
    "- For each epoch, we open a `for` loop that iterates over the dataset, in batches\n",
    "- For each batch, we open a `GradientTape()` scope\n",
    "- Inside this scope, we call the model (forward pass) and compute the loss\n",
    "- Outside the scope, we retrieve the gradients of the weights\n",
    "of the model with regard to the loss\n",
    "- Finally, we use the optimizer to update the weights of the model based on the\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df43e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Validation acc: 0.8172\n",
      "\n",
      "Start of epoch 1\n",
      "Validation acc: 0.8436\n",
      "\n",
      "Start of epoch 2\n",
      "Validation acc: 0.8760\n",
      "\n",
      "Start of epoch 3\n",
      "Validation acc: 0.8985\n",
      "\n",
      "Start of epoch 4\n",
      "Validation acc: 0.8983\n",
      "51.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffbf2c",
   "metadata": {},
   "source": [
    "## Speeding-up your training step with `tf.function`\n",
    "\n",
    "The default runtime in TensorFlow 2 is\n",
    "[eager execution](https://www.tensorflow.org/guide/eager).\n",
    "As such, our training loop above executes eagerly.\n",
    "\n",
    "This is great for debugging, but graph compilation has a definite performance\n",
    "advantage. Describing your computation as a static graph enables the framework\n",
    "to apply global performance optimizations. This is impossible when\n",
    "the framework is constrained to greedly execute one operation after another,\n",
    "with no knowledge of what comes next.\n",
    "\n",
    "You can compile into a static graph any function that takes tensors as input.\n",
    "Just add a `@tf.function` decorator on it, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1fa0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Logits for this minibatch, forward pass\n",
    "        logits = model(x, training=True)\n",
    "        \n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    \n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    \n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09cc63",
   "metadata": {},
   "source": [
    "Let's do the same with the evaluation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779bec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b96d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    test_logits = model(x, training=False)\n",
    "    test_acc_metric.update_state(y, test_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ae0bd",
   "metadata": {},
   "source": [
    "Now, let's re-run our training loop with this compiled training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d433607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Validation acc: 0.9145\n",
      "\n",
      "Start of epoch 1\n",
      "Validation acc: 0.9183\n",
      "\n",
      "Start of epoch 2\n",
      "Validation acc: 0.9220\n",
      "\n",
      "Start of epoch 3\n",
      "Validation acc: 0.9225\n",
      "\n",
      "Start of epoch 4\n",
      "Validation acc: 0.9252\n",
      "7.17 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_step(x_batch_val, y_batch_val)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e455ca",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a255ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing acc: 0.9210\n"
     ]
    }
   ],
   "source": [
    "for x_batch_val, y_batch_val in test_dataset:\n",
    "    test_step(x_batch_val, y_batch_val)\n",
    "test_acc = test_acc_metric.result()\n",
    "test_acc_metric.reset_states()\n",
    "print(\"Testing acc: %.4f\" % (float(test_acc),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db3d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
