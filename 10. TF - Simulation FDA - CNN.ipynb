{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2759993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74531322",
   "metadata": {},
   "source": [
    "## Import EMNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a550108",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc37c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c8e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_BATCH_INPUT = (None, 28, 28) # EMNIST dataset (None is used for batch size, as it varies)\n",
    "CNN_INPUT_RESHAPE = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a83a61",
   "metadata": {},
   "source": [
    "## Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb02a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "del X_train\n",
    "\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.int32)\n",
    "del y_train\n",
    "\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "del X_test\n",
    "\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.int32)\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ab8a7",
   "metadata": {},
   "source": [
    "## Prepare data for Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e112b",
   "metadata": {},
   "source": [
    "### Create centralized testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fad165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_test = (X_test_tensor, y_test_tensor)\n",
    "\n",
    "def create_tf_dataset_for_testing(batch_size):\n",
    "    return tf.data.Dataset.from_tensor_slices(slices_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84aa24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = create_tf_dataset_for_testing(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24a95c",
   "metadata": {},
   "source": [
    "### Slice the Tensors for each Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43734f7",
   "metadata": {},
   "source": [
    "We will cut the training data, i.e., (`X_train_tensor`, `y_train_tensor`) to equal parts, each part corresponding to one Client. We want to give the result back as a dictionary with key `client_id` and value the training tensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b27a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_clients(num_clients):\n",
    "    \n",
    "    client_slices_train = {}\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        # Compute the indices for this client's slice\n",
    "        start_idx = int(i * n_train / num_clients)\n",
    "        end_idx = int((i + 1) * n_train / num_clients)\n",
    "\n",
    "        # Get the slice for this client\n",
    "        X_client_train = X_train_tensor[start_idx:end_idx]\n",
    "        y_client_train = y_train_tensor[start_idx:end_idx]\n",
    "        \n",
    "        # Combine the slices into a single dataset\n",
    "        client_slices_train[f'client_{i}'] = (X_client_train, y_client_train)\n",
    "    \n",
    "    return client_slices_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88734523",
   "metadata": {},
   "source": [
    "### Create TF friendly data for each Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c58dbae",
   "metadata": {},
   "source": [
    "Given a Tensor slice (i.e. value of `client_slices_train[\"client_id\"]` we convert it to highly optimized `tf.data.Dataset` to prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794535c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset_for_client(client_tensor_slices, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed):\n",
    "    \n",
    "        return tf.data.Dataset.from_tensor_slices(client_tensor_slices) \\\n",
    "            .shuffle(buffer_size=shuffle_buffer_size, seed=seed).batch(batch_size) \\\n",
    "            .prefetch(tf.data.AUTOTUNE).take(num_steps_until_rtc_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794b1a0",
   "metadata": {},
   "source": [
    "### Create Federated Learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d770c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_federated_data(client_slices_train, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed=None):\n",
    "    \n",
    "    federated_dataset = [ \n",
    "        create_tf_dataset_for_client(client_tensor_slices, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed)\n",
    "        for client, client_tensor_slices in client_slices_train.items()\n",
    "    ]\n",
    "    \n",
    "    return federated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a0455",
   "metadata": {},
   "source": [
    "# Miscallenious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d696f",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6aa1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def variance(cnn_list, cnn_sync):\n",
    "    \n",
    "    squared_distances = [\n",
    "        tf.reduce_sum(tf.square(cnn.trainable_vars_as_vector() - cnn_sync.trainable_vars_as_vector())) \n",
    "        for cnn in cnn_list\n",
    "    ]\n",
    "    \n",
    "    var = tf.reduce_mean(squared_distances)\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88347123",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_dict(fda_name, n_train, dataset_name, input_pixels, seed, epochs, num_clients, \n",
    "                        batch_size, steps_in_one_fda_step, theta, total_fda_steps, num_weights,\n",
    "                        total_rounds, final_accuracy, sketch_width=None, sketch_depth=None):\n",
    "    metrics = {\n",
    "            \"fda_name\" : fda_name,\n",
    "            \"theta\" : theta,\n",
    "            \"dataset_name\" : dataset_name, # new\n",
    "            \"input_pixels\" : input_pixels, # new\n",
    "            \"n_train\" : n_train, # new\n",
    "            \"num_weights\" : num_weights, # new\n",
    "            \"seed\" : seed,\n",
    "            \"epochs\" : epochs,\n",
    "            \"num_clients\" : num_clients,\n",
    "            \"batch_size\" : batch_size,\n",
    "            \"steps_in_one_fda_step\" : steps_in_one_fda_step,\n",
    "            \"sketch_width\" : sketch_width,\n",
    "            \"sketch_depth\" : sketch_depth\n",
    "        }\n",
    "    \n",
    "    # one batch bytes\n",
    "    metrics[\"one_sample_bytes\"] = 4 * (metrics[\"input_pixels\"] + 1)  # 4 bytes float32\n",
    "    \n",
    "    # training dataset size\n",
    "    metrics[\"training_dataset_bytes\"] = metrics[\"one_sample_bytes\"] * metrics[\"n_train\"]\n",
    "    \n",
    "    # model bytes\n",
    "    metrics[\"model_bytes\"] = metrics[\"num_weights\"] * 4\n",
    "    \n",
    "    \n",
    "    # local state bytes (i.e. S_i), for one client\n",
    "    if fda_name == \"naive\":\n",
    "        metrics[\"local_state_bytes\"] = 4\n",
    "    elif fda_name == \"linear\":\n",
    "        metrics[\"local_state_bytes\"] = 8\n",
    "    else:\n",
    "        metrics[\"local_state_bytes\"] = sketch_width * sketch_depth * 4 + 4\n",
    "        \n",
    "    # accuracy (already computed in parameter)\n",
    "    metrics[\"final_accuracy\"] = final_accuracy\n",
    "    \n",
    "    # total fda steps from algo\n",
    "    metrics[\"total_fda_steps\"] = total_fda_steps\n",
    "    \n",
    "    # total steps (a single fda step might have many normal SGD steps, batch steps)\n",
    "    metrics[\"total_steps\"] = metrics[\"total_fda_steps\"] * metrics[\"steps_in_one_fda_step\"]\n",
    "    \n",
    "    # total rounds in algo. Reason why we differentiate from the hardcoded NUM_ROUNDS\n",
    "    # is because we might run less rounds in the future (i.e. stop on 10^7 samples idk)\n",
    "    metrics[\"total_rounds\"] = total_rounds\n",
    "    \n",
    "    # bytes exchanged for synchronizing weights (x2 because server sends back)\n",
    "    metrics[\"model_bytes_exchanged\"] = metrics[\"total_rounds\"] * metrics[\"model_bytes\"] \\\n",
    "        * metrics[\"num_clients\"] * 2\n",
    "    \n",
    "    # bytes exchanged for monitoring the variance (communication)\n",
    "    metrics[\"monitoring_bytes_exchanged\"] = metrics[\"local_state_bytes\"] * metrics[\"total_fda_steps\"] \\\n",
    "        * metrics[\"num_clients\"]\n",
    "    \n",
    "    # total communication bytes (for both monitoring and model synchronization)\n",
    "    metrics[\"total_communication_bytes\"] = metrics[\"model_bytes_exchanged\"] + metrics[\"monitoring_bytes_exchanged\"]\n",
    "    \n",
    "    # total seen dataset bytes (across all learning, i.e., all clients)\n",
    "    metrics[\"trained_in_bytes\"] = metrics[\"batch_size\"] * metrics[\"one_sample_bytes\"] \\\n",
    "        * metrics[\"total_steps\"] * metrics[\"num_clients\"]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e079822",
   "metadata": {},
   "source": [
    "## Neural Net weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9b44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weights(model):\n",
    "    total_params = 0\n",
    "    for layer in model.layers:\n",
    "        total_params += np.sum([np.prod(weight.shape) for weight in layer.trainable_weights])\n",
    "    return int(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee0c14",
   "metadata": {},
   "source": [
    "## Reseting NN weights for Server-Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8abe642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables):\n",
    "    \n",
    "    server_cnn.set_trainable_variables(starting_trainable_variables)\n",
    "    \n",
    "    for client_cnn in client_cnns:\n",
    "        client_cnn.set_trainable_variables(starting_trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbfbd3",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Net (CNN) - Medium Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3a252",
   "metadata": {},
   "source": [
    "A simple Convolutional Neural Network with a single convolutional layer, followed by a max-pooling layer, and two dense layers for classification. Designed for 28x28 grayscale images. It has 692,352 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac88dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.reshape = layers.Reshape(CNN_INPUT_RESHAPE)\n",
    "        self.conv1 = layers.Conv2D(32, 3, activation='relu')\n",
    "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dense2 = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "        \n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.reshape(inputs)  # Add a channel dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, batch):\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            y_batch_pred = self(x_batch, training=True)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(\n",
    "                y_true=y_batch,\n",
    "                y_pred=y_batch_pred,\n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients to the model's trainable variables (update weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        #self.compiled_metrics.update_state(y_batch, y_batch_pred)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, dataset):\n",
    "\n",
    "        for batch in dataset:\n",
    "            self.step(batch)\n",
    "            \n",
    "    \n",
    "    def set_trainable_variables(self, trainable_vars):\n",
    "        \"\"\" Given `trainable_vars` set our `self.trainable_vars` \"\"\"\n",
    "        for model_var, var in zip(self.trainable_variables, trainable_vars):\n",
    "            model_var.assign(var)\n",
    "\n",
    "            \n",
    "    def trainable_vars_as_vector(self):\n",
    "        return tf.concat([tf.reshape(var, [-1]) for var in self.trainable_variables], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead5a6c",
   "metadata": {},
   "source": [
    "### Helper function to compile and return the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ddd2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_and_built_cnn():\n",
    "    cnn = CNN()\n",
    "    \n",
    "    cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # we have softmax\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')]\n",
    "    )\n",
    "    \n",
    "    cnn.build(CNN_BATCH_INPUT)  # EMNIST dataset (None is used for batch size, as it varies)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f99e0",
   "metadata": {},
   "source": [
    "# Advanced Convolutional Neural Net (CNN) - Large Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259d325",
   "metadata": {},
   "source": [
    "A more complex Convolutional Neural Network with three sets of two convolutional layers, each followed by a max-pooling layer, and two dense layers with dropout for classification. Designed for 28x28 grayscale images. It has 2,592,202 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee5705b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AdvancedCNN, self).__init__()\n",
    "        \n",
    "        self.reshape = layers.Reshape(CNN_INPUT_RESHAPE)\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv4 = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.conv5 = layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv6 = layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(512, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.dense2 = layers.Dense(512, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "        self.dense3 = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.reshape(inputs)  # Add a channel dimension\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.max_pool2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.max_pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, batch):\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            y_batch_pred = self(x_batch, training=True)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(\n",
    "                y_true=y_batch,\n",
    "                y_pred=y_batch_pred,\n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients to the model's trainable variables (update weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        #self.compiled_metrics.update_state(y_batch, y_batch_pred)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, dataset):\n",
    "\n",
    "        for batch in dataset:\n",
    "            self.step(batch)\n",
    "            \n",
    "    \n",
    "    def set_trainable_variables(self, trainable_vars):\n",
    "        \"\"\" Given `trainable_vars` set our `self.trainable_vars` \"\"\"\n",
    "        for model_var, var in zip(self.trainable_variables, trainable_vars):\n",
    "            model_var.assign(var)\n",
    "\n",
    "            \n",
    "    def trainable_vars_as_vector(self):\n",
    "        return tf.concat([tf.reshape(var, [-1]) for var in self.trainable_variables], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1289f0",
   "metadata": {},
   "source": [
    "### Helper function to compile and return the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a705fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_and_built_advanced_cnn():\n",
    "    advanced_cnn = AdvancedCNN()\n",
    "    \n",
    "    advanced_cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # we have softmax\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')]\n",
    "    )\n",
    "    \n",
    "    advanced_cnn.build(CNN_BATCH_INPUT)  # EMNIST dataset (None is used for batch size, as it varies)\n",
    "    \n",
    "    return advanced_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c454f52",
   "metadata": {},
   "source": [
    "### Average NN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7cb196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_client_weights(client_models):\n",
    "    # client_weights[0] the trainable variables of Client 0 (a list of tf.Variable)\n",
    "    client_weights = [model.trainable_variables for model in client_models]\n",
    "\n",
    "    # concise solution. per layer. `layer_weight_tensors` corresponds to a list of tensors of a layer\n",
    "    avg_weights = [\n",
    "        tf.reduce_mean(layer_weight_tensors, axis=0)\n",
    "        for layer_weight_tensors in zip(*client_weights)\n",
    "    ]\n",
    "\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e3dab",
   "metadata": {},
   "source": [
    "### Server - Clients synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "666ac14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def synchronize(server_cnn, client_cnns):\n",
    "    # server average\n",
    "    server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "    \n",
    "    # synchronize clients\n",
    "    for client_cnn in client_cnns:\n",
    "        client_cnn.set_trainable_variables(server_cnn.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a13e6",
   "metadata": {},
   "source": [
    "# Functional Dynamic Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2a1b5",
   "metadata": {},
   "source": [
    "We follow the Functional Dynamic Averaging (FDA) scheme. Let the mean model be\n",
    "\n",
    "$$ \\overline{w_t} = \\frac{1}{k} \\sum_{i=1}^{k} w_t^{(i)} $$\n",
    "\n",
    "where $ w_t^{(i)} $ is the model at time $ t $ in some round in the $i$-th learner.\n",
    "\n",
    "Local models are trained independently and cooperatively and we want to monitor the Round Terminating Conditon (**RTC**):\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2  \\leq \\Theta $$\n",
    "\n",
    "where the left-hand side is the **model variance**, and threshold $\\Theta$ is a hyperparameter of the FDA, defined at the beginning of the round; it may change at each round. When the monitoring logic cannot guarantee the validity of RTC, the round terminates. All local models are pulled into `tff.SERVER`, and $\\bar{w_t}$ is set to their average. Then, another round begins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0ccbb",
   "metadata": {},
   "source": [
    "### Monitoring the RTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6e34b",
   "metadata": {},
   "source": [
    "FDA monitors the RTC by applying techniques from Functionary [Functional Geometric Averaging](http://users.softnet.tuc.gr/~minos/Papers/edbt19.pdf). We first restate the problem of monitoring RTC into the standard distributed stream monitoring formulation. Let\n",
    "\n",
    "$$ S(t) =  \\frac{1}{k} \\sum_{i=1}^{k} S_i(t) $$\n",
    "\n",
    "where $ S(t) \\in \\mathbb{R}^n $ be the \"global state\" of the system and $ S_i(t) \\in \\mathbb{R}^n $ the \"local states\". The goal is to monitor the threshold condition on the global state in the form $ F(S(t)) \\leq \\Theta $ where $ F : \\mathbb{R}^n \\to \\mathbb{R} $ a non-linear function. Let\n",
    "\n",
    "$$ \\Delta_t^{(i)} = w_t^{(i)} - w_{t_0}^{(i)} $$\n",
    "\n",
    "be the update at the $ i $-th learner, that is, the change to the local model at time $t$ since the beginning of the current round at time $t_0$. Let the average update be\n",
    "\n",
    "$$ \\overline{\\Delta_t} = \\frac{1}{k} \\sum_{i=1}^{k} \\Delta_t^{(i)} $$\n",
    "\n",
    "it follows that the variance can be written as\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2 = \\Big( \\frac{1}{k} \\sum_{i=1}^{k} \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\Big) - \\lVert \\overline{\\Delta_t} \\rVert_2^2 $$\n",
    "\n",
    "So, conceptually, if we define\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\bf{x}\n",
    "         \\end{bmatrix}) = v - \\lVert \\bf{x} \\rVert_2^2 $$\n",
    "\n",
    "The RTC is equivalent to condition $$ F(S(t)) \\leq \\Theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a0df4",
   "metadata": {},
   "source": [
    "## 1️⃣ Naive FDA\n",
    "\n",
    "In the naive approach, we eliminate the update vector from the local state (i.e. recuce the dimension to 0). Define local state as\n",
    "\n",
    "$$ S_i(t) = \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\in \\mathbb{R}$$ \n",
    "\n",
    "and the identity function\n",
    "\n",
    "$$ F(v) = v $$\n",
    "\n",
    "It is trivial that $ F(S(t)) \\leq \\Theta $ implies the RTC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6875d47",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0cf704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_naive(last_sync_cnn, client_cnn, client_dataset):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - last_sync_cnn.trainable_vars_as_vector()\n",
    "    \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    return Delta_i_euc_norm_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2013ea",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4780782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_naive(S):\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2c6b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_naive(server_cnn, client_cnns, federated_dataset,\n",
    "                                   num_epochs, theta, epoch_fda_steps):\n",
    "    \n",
    "    print(\"retracing naive\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_naive(S) <= theta:\n",
    "            S_i_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared = steps_naive(server_cnn, client_cnn, client_dataset)\n",
    "                S_i_clients.append(Delta_i_euc_norm_squared)\n",
    "                \n",
    "            S = tf.reduce_mean(S_i_clients)\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        Delta_i_clients = [\n",
    "            tf.subtract(client_cnn.trainable_vars_as_vector(), server_cnn.trainable_vars_as_vector()) \n",
    "            for client_cnn in client_cnns\n",
    "        ] #test\n",
    "        actual_S_2 = tf.reduce_sum(tf.square(tf.reduce_mean(Delta_i_clients, axis=0))) #test\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        tf.print(\"Naive Epoch count: \", epoch_count, \" Total fda steps: \", total_fda_steps, output_stream=sys.stdout)\n",
    "        tf.print(\"Est var: \", S, \" Actual S_2 (Assumed 0): \", actual_S_2, \" Actual var: \", variance(client_cnns, server_cnn), output_stream=sys.stdout)\n",
    "        tf.print(\"\\n\", output_stream=sys.stdout)\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6d3f1",
   "metadata": {},
   "source": [
    "## 2️⃣ Linear FDA\n",
    "\n",
    "In the linear case, we reduce the update vector to a scalar, $ \\xi \\Delta_t^{(i)} \\in \\mathbb{R}$, where $ \\xi $ is any unit vector.\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\xi \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^2 $$\n",
    "\n",
    "Also, define \n",
    "\n",
    "$$ F(v, x) = v - x^2 $$\n",
    "\n",
    "The RTC is equivalent to condition \n",
    "\n",
    "$$ F(S(t)) \\leq \\Theta $$\n",
    "\n",
    "A random choice of $ \\xi $ is likely to perform poorly (terminate round prematurely), as it wil likely be close to orthogonal to $ \\overline{\\Delta_t} $. A good choice would be a vector $ \\xi $ correlated to $ \\overline{\\Delta_t} $. A heuristic choice is to take $ \\overline{\\Delta_{t_0}} $ (after scaling it to norm 1), i.e., the update vector right before the current round started. All nodes can estimate this without communication, as $ \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} $, the difference of the last two models pushed by the Server. Hence, \n",
    "\n",
    "$$ \\xi = \\frac{\\overline{w_{t_0}} - \\overline{w_{t_{-1}}}}{\\lVert \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} \\rVert_2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbfb12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ksi_unit_fn(w_t0, w_tminus1):\n",
    "    \n",
    "    if tf.reduce_all(tf.equal(w_t0, w_tminus1)):\n",
    "        # if equal then ksi becomes a random vector (will only happen in round 1)\n",
    "        ksi = tf.random.normal(shape=w_t0.shape)\n",
    "    else:\n",
    "        ksi = w_t0 - w_tminus1\n",
    "\n",
    "    # Normalize and return\n",
    "    return tf.divide(ksi, tf.norm(ksi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b66f11",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0ad6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_linear(cnn_tminus, cnn_t0, client_cnn, client_dataset):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - cnn_t0.trainable_vars_as_vector()\n",
    "    \n",
    "    #||D(t)_i||^2 , shape = (1,) \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    # heuristic unit vector ksi\n",
    "    ksi = ksi_unit_fn(cnn_t0.trainable_vars_as_vector(), cnn_tminus.trainable_vars_as_vector())\n",
    "    \n",
    "    # ksi * Delta_i (* is dot) , shape = ()\n",
    "    ksi_Delta_i = tf.reduce_sum(tf.multiply(ksi, Delta_i))\n",
    "    \n",
    "    return Delta_i_euc_norm_squared, ksi_Delta_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c734b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d8dd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_linear(S_1, S_2):\n",
    "    return S_1 - S_2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7e87dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_linear(previous_server_cnn, server_cnn, client_cnns, federated_dataset,\n",
    "                                   num_epochs, theta, epoch_fda_steps):\n",
    "    \n",
    "    print(\"retracing linear\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    S_2 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_linear(S_1, S_2) <= theta:\n",
    "            euc_norm_squared_clients = []\n",
    "            ksi_delta_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared, ksi_Delta_i = steps_linear(\n",
    "                    previous_server_cnn, server_cnn, client_cnn, client_dataset\n",
    "                )\n",
    "                \n",
    "                euc_norm_squared_clients.append(Delta_i_euc_norm_squared)\n",
    "                ksi_delta_clients.append(ksi_Delta_i)\n",
    "            \n",
    "            S_1 = tf.reduce_mean(euc_norm_squared_clients)\n",
    "            S_2 = tf.reduce_mean(ksi_delta_clients)\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        Delta_i_clients = [\n",
    "            tf.subtract(client_cnn.trainable_vars_as_vector(), server_cnn.trainable_vars_as_vector()) \n",
    "            for client_cnn in client_cnns\n",
    "        ] #test\n",
    "        actual_S_2 = tf.reduce_sum(tf.square(tf.reduce_mean(Delta_i_clients, axis=0))) #test\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # last server model (previous sync)\n",
    "        previous_server_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        tf.print(\"Linear Epoch count: \", epoch_count, \" Total fda steps: \", total_fda_steps, output_stream=sys.stdout)\n",
    "        tf.print(\"Est var: \", F_linear(S_1, S_2), \" Actual S_2: \", actual_S_2, \" Approx S_2: \", S_2**2, \" Actual var: \", variance(client_cnns, server_cnn), output_stream=sys.stdout)\n",
    "        tf.print(\"\\n\", output_stream=sys.stdout)\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "        S_2 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b28f1a",
   "metadata": {},
   "source": [
    "## 3️⃣ Sketch FDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a6434",
   "metadata": {},
   "source": [
    "An optimal estimator for $ \\lVert \\overline{\\Delta_t} \\rVert_2^2  $ can be obtained by employing AMS sketches. An AMS sketch of a vector $ v \\in \\mathbb{R}^M $ is a $ d \\times m $ real matrix\n",
    "\n",
    "$$ \\Xi = \\text{sk}(v) = \\begin{bmatrix}\n",
    "           \\Xi_1 \\\\\n",
    "           \\Xi_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           \\Xi_d \n",
    "         \\end{bmatrix} $$\n",
    "         \n",
    "where $ d \\cdot m \\ll M$. Operator sk($ \\cdot $) is linear, i.e., let $a, b \\in \\mathbb{R}$ and $v_1, v_2 \\in \\mathbb{R}^N$ then \n",
    "\n",
    "$$ \\text{sk}(a v_1 + b v_2) = a \\; \\text{sk}(v_1) + b \\; \\text{sk}(v_2)  $$\n",
    "\n",
    "Also, sk($ v $) can be computed in $ \\mathcal{O}(dN) $ steps.\n",
    "\n",
    "The interesting property of AMS sketches is that the function \n",
    "\n",
    "$$ M(sk(\\textbf{v})) = \\underset{i=1,...,d}{\\text{median}} \\; \\lVert \\boldsymbol{\\Xi}_i \\rVert_2^2  $$ \n",
    "\n",
    "is an excellent estimator of the Euclidean norm of **v** (within relative $\\epsilon$-error):\n",
    "\n",
    "$$ M(sk(\\textbf{v})) \\; \\in (1 \\pm \\epsilon) \\lVert \\textbf{v} \\rVert_2^2 \\; \\; \\text{with probability at least} \\; (1-\\delta) $$\n",
    "\n",
    "where $m = \\mathcal{O}(\\frac{1}{\\epsilon^2})$ and $d = \\mathcal{O}(\\log \\frac{1}{\\delta})$\n",
    "            \n",
    "Moreover, let $\\boldsymbol{\\Xi} \\in \\mathbb{R}^{d \\times m}$ and $ k \\in \\mathbb{R}$. It can be proven that\n",
    "\n",
    "$$ M( \\frac{1}{k} \\boldsymbol{\\Xi}) = \\frac{1}{k^2} M(\\boldsymbol{\\Xi}) $$\n",
    "\n",
    "Let's investigate a little further on how this helps us. The $i$-th client computes $ sk(\\Delta_t^{(i)}) $ and sends it to the server. Notice\n",
    "\n",
    "$$ M\\big(sk(\\Delta_t^{(1)}) + sk(\\Delta_t^{(2)}) + ... + sk(\\Delta_t^{(k)}) \\big) = M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big)$$\n",
    "\n",
    "Remember that\n",
    "\n",
    "$$ \\overline{\\boldsymbol{\\Delta}}_t = \\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} $$\n",
    "\n",
    "Then\n",
    "            \n",
    "$$ M\\Big( \\text{sk}\\big( \\overline{\\boldsymbol{\\Delta}}_t \\big) \\Big) = M\\Big( \\text{sk}\\big( \\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) = \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) $$\n",
    "\n",
    "\n",
    "Which means that \n",
    "\n",
    "$$ \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) \\in (1 \\pm \\epsilon) \\lVert \\overline{\\boldsymbol{\\Delta}}_t \\rVert_2^2 \\; \\; \\text{w.p. at least} \\; (1-\\delta) $$\n",
    "\n",
    "In the monitoring process it is essential that we do not overestimate $ \\lVert \\overline{\\Delta_t} \\rVert_2^2 $ because we would then underestimate the variance which would potentially result in actual varience exceeding $ \\Theta$ without us noticing it. With this in mind,\n",
    "\n",
    "$$ \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big) \\leq (1+\\epsilon) \\lVert \\overline{\\Delta_t} \\rVert_2^2 \\quad \\text{with probability at least} \\; (1-\\delta)$$\n",
    "\n",
    "Which means\n",
    "\n",
    "$$ \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big) \\leq \\lVert \\overline{\\Delta_t} \\rVert_2^2 \\quad \\text{with probability at least} \\; (1-\\delta)$$\n",
    "\n",
    "Hence, the Server's estimation of $ \\lVert \\overline{\\Delta_t} \\rVert_2^2 $ is\n",
    "\n",
    "$$ \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M\\Big( sk(\\Delta_t^{(1)}) + sk(\\Delta_t^{(2)}) + ... + sk(\\Delta_t^{(k)}) \\big) \\Big) $$\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           sk(\\Delta_t^{(i)})\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^{1+d \\times m} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\Xi\n",
    "         \\end{bmatrix}) = v - \\frac{1}{(1+\\epsilon)}  M(\\Xi) \\quad \\text{where} \\quad \\Xi = \\frac{1}{k} \\sum_{i=1}^{k} sk(\\Delta_t^{(i)}) $$\n",
    "\n",
    "It follows that $ F(S(t)) \\leq \\Theta $ implies that the variance is less or equal to $ \\Theta $ with probability at least $ 1-\\delta $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db738526",
   "metadata": {},
   "source": [
    "## AMS sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e3bfa",
   "metadata": {},
   "source": [
    "We use `ExtensionType` which is the way to go in order to avoid unecessary graph retracing when passing around `AmsSketch` type 'objects'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7f6b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.experimental import ExtensionType\n",
    "\n",
    "class AmsSketch(ExtensionType):\n",
    "    depth: int\n",
    "    width: int\n",
    "    F: tf.Tensor\n",
    "        \n",
    "        \n",
    "    def __init__(self, depth=7, width=1500):\n",
    "        self.depth = depth\n",
    "        self.width = width\n",
    "        self.F = tf.random.uniform(shape=(6, depth), minval=0, maxval=(1 << 31) - 1, dtype=tf.int32)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def hash31(self, x, a, b):\n",
    "\n",
    "        r = a * x + b\n",
    "        fold = tf.bitwise.bitwise_xor(tf.bitwise.right_shift(r, 31), r)\n",
    "        return tf.bitwise.bitwise_and(fold, 2147483647)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def tensor_hash31(self, x, a, b): # GOOD\n",
    "        \"\"\" Assumed that x is tensor shaped (d,) , i.e., a vector (for example, indices, i.e., tf.range(d)) \"\"\"\n",
    "\n",
    "        # Reshape x to have an extra dimension, resulting in a shape of (k, 1)\n",
    "        x_reshaped = tf.expand_dims(x, axis=-1)\n",
    "\n",
    "        # shape=(`v_dim`, 7)\n",
    "        r = tf.multiply(a, x_reshaped) + b\n",
    "\n",
    "        fold = tf.bitwise.bitwise_xor(tf.bitwise.right_shift(r, 31), r)\n",
    "        \n",
    "        return tf.bitwise.bitwise_and(fold, 2147483647)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def tensor_fourwise(self, x):\n",
    "        \"\"\" Assumed that x is tensor shaped (d,) , i.e., a vector (for example, indices, i.e., tf.range(d)) \"\"\"\n",
    "        # 1st use the tensor hash31\n",
    "        in1 = self.tensor_hash31(x, self.F[2], self.F[3])  # (`x_dim`, 7)\n",
    "        \n",
    "        # 2nd (notice we swap the first two params, no change really)\n",
    "        in2 = self.tensor_hash31(x, in1, self.F[4])  # (`x_dim`, 7)\n",
    "        \n",
    "        in3 = self.tensor_hash31(x, in2, self.F[5])  # (`x_dim`, 7)\n",
    "        \n",
    "        in4 = tf.bitwise.bitwise_and(in3, 32768)  # (`x_dim`, 7)\n",
    "        \n",
    "        return 2 * (tf.bitwise.right_shift(in4, 15)) - 1  # (`x_dim`, 7)\n",
    "        \n",
    "        \n",
    "    @tf.function\n",
    "    def fourwise(self, x):\n",
    "\n",
    "        result = 2 * (tf.bitwise.right_shift(tf.bitwise.bitwise_and(self.hash31(self.hash31(self.hash31(x, self.F[2], self.F[3]), x, self.F[4]), x, self.F[5]), 32768), 15)) - 1\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector(self, v):\n",
    "        \"\"\" Extremely efficient computation of sketch with only using tensors. \"\"\"\n",
    "        \n",
    "        sketch = tf.zeros(shape=(self.depth, self.width), dtype=tf.float32)\n",
    "        \n",
    "        len_v = v.shape[0]\n",
    "        \n",
    "        pos_tensor = self.tensor_hash31(tf.range(len_v), self.F[0], self.F[1]) % self.width\n",
    "        \n",
    "        v_expand = tf.expand_dims(v, axis=-1)\n",
    "        \n",
    "        deltas_tensor = tf.multiply(tf.cast(self.tensor_fourwise(tf.range(len_v)), dtype=tf.float32), v_expand)\n",
    "        \n",
    "        range_tensor = tf.range(self.depth)\n",
    "        \n",
    "        # Expand dimensions to create a 2D tensor with shape (1, depth)\n",
    "        range_tensor_expanded = tf.expand_dims(range_tensor, 0)\n",
    "\n",
    "        # Use tf.tile to repeat the range `len_v` times\n",
    "        repeated_range_tensor = tf.tile(range_tensor_expanded, [len_v, 1])\n",
    "        \n",
    "        # shape=(`len_v`, 7, 2)\n",
    "        indices = tf.stack([repeated_range_tensor, pos_tensor], axis=-1)\n",
    "        \n",
    "        sketch = tf.tensor_scatter_nd_add(sketch, indices, deltas_tensor)\n",
    "        \n",
    "        return sketch\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector2(self, v):\n",
    "        \"\"\" Bad implementation for tensorflow. \"\"\"\n",
    "\n",
    "        sketch = tf.zeros(shape=(self.depth, self.width), dtype=tf.float32)\n",
    "\n",
    "        for i in tf.range(tf.shape(v)[0], dtype=tf.int32):\n",
    "            pos = self.hash31(i, self.F[0], self.F[1]) % self.width\n",
    "            delta = tf.cast(self.fourwise(i), dtype=tf.float32) * v[i]\n",
    "            indices_to_update = tf.stack([tf.range(self.depth, dtype=tf.int32), pos], axis=1)\n",
    "            sketch = tf.tensor_scatter_nd_add(sketch, indices_to_update, delta)\n",
    "\n",
    "        return sketch\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def estimate_euc_norm_squared(sketch):\n",
    "\n",
    "        @tf.function\n",
    "        def _median(v):\n",
    "            \"\"\" Median of tensor `v` with shape=(n,). Note: Suboptimal O(nlogn) but it's ok bcz n = `depth`\"\"\"\n",
    "            length = tf.shape(v)[0]\n",
    "            sorted_v = tf.sort(v)\n",
    "            middle = length // 2\n",
    "\n",
    "            return tf.cond(\n",
    "                tf.equal(length % 2, 0),\n",
    "                lambda: (sorted_v[middle - 1] + sorted_v[middle]) / 2.0,\n",
    "                lambda: sorted_v[middle]\n",
    "            )\n",
    "\n",
    "        return _median(tf.reduce_sum(tf.square(sketch), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe92e3b",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a74388",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_sketch(last_sync_cnn, client_cnn, client_dataset, ams_sketch):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - last_sync_cnn.trainable_vars_as_vector()\n",
    "    \n",
    "    #||D(t)_i||^2 , shape = (1,) \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    # sketch approx\n",
    "    sketch = ams_sketch.sketch_for_vector(Delta_i)\n",
    "    \n",
    "    return Delta_i_euc_norm_squared, sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ba6cf",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6b0043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_sketch(S_1, S_2, epsilon):\n",
    "    \"\"\" `S_1` is mean || ||^2 as usual, S_2 is the `Ξ` as defined in the theoretical analysis above \"\"\"\n",
    "    \n",
    "    return S_1 - (1. / (1. + epsilon)) * AmsSketch.estimate_euc_norm_squared(S_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd3e3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(S_2, epsilon):\n",
    "    \n",
    "    return (1. / (1. + epsilon)) * AmsSketch.estimate_euc_norm_squared(S_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9f520f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_sketch(server_cnn, client_cnns, federated_dataset, num_epochs, \n",
    "                                    theta, epoch_fda_steps, ams_sketch, epsilon):\n",
    "    \n",
    "    print(\"retracing sketch\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    S_2 = tf.zeros(shape=(ams_sketch.depth, ams_sketch.width), dtype=tf.float32)\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_sketch(S_1, S_2, epsilon) <= theta:\n",
    "            euc_norm_squared_clients = []\n",
    "            sketch_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared, sketch = steps_sketch(\n",
    "                    server_cnn, client_cnn, client_dataset, ams_sketch\n",
    "                )\n",
    "                \n",
    "                euc_norm_squared_clients.append(Delta_i_euc_norm_squared)\n",
    "                sketch_clients.append(sketch)\n",
    "            \n",
    "            S_1 = tf.reduce_mean(euc_norm_squared_clients)\n",
    "            S_2 = tf.reduce_mean(sketch_clients, axis=0)  # shape=(`depth`, width`). See `Ξ` in theoretical analysis\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        Delta_i_clients = [\n",
    "            tf.subtract(client_cnn.trainable_vars_as_vector(), server_cnn.trainable_vars_as_vector()) \n",
    "            for client_cnn in client_cnns\n",
    "        ] #test\n",
    "        actual_S_2 = tf.reduce_sum(tf.square(tf.reduce_mean(Delta_i_clients, axis=0))) #test\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        tf.print(\"Sketch Epoch count: \", epoch_count, \" Total fda steps: \", total_fda_steps, output_stream=sys.stdout)\n",
    "        #tf.print(\"Naive Epoch count: \", epoch_count, output_stream=sys.stdout)\n",
    "        tf.print(\"Est var: \", F_sketch(S_1, S_2, epsilon), \" Actual S_2: \", actual_S_2, \" Apprxo S_2\", t(S_2, epsilon),  \" Actual var: \", variance(client_cnns, server_cnn), output_stream=sys.stdout)\n",
    "        tf.print(\"\\n\", output_stream=sys.stdout)\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "        S_2 = tf.zeros(shape=(ams_sketch.depth, ams_sketch.width), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d998e29",
   "metadata": {},
   "source": [
    "# Simulation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615142b0",
   "metadata": {},
   "source": [
    "###  Basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94d94680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_test(server_cnn, client_cnns, previous_server_cnn, starting_trainable_variables, \n",
    "               NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, BATCH_SIZE, \n",
    "               THETA, EPSILON, ams_sketch, client_slices_train, seed):\n",
    "    \n",
    "    \"\"\" One test for Naive,Linear,Sketch. Returns metrics \"\"\"\n",
    "    \n",
    "    num_epochs = tf.constant(NUM_EPOCHS, shape=(), dtype=tf.int32)\n",
    "    theta = tf.constant(THETA, shape=(), dtype=tf.float32)\n",
    "    \n",
    "    # for sketch\n",
    "    epsilon = tf.constant(EPSILON, shape=(), dtype=tf.float32) # new\n",
    "    \n",
    "    \n",
    "    epoch_client_batches = (n_train / BATCH_SIZE) / NUM_CLIENTS\n",
    "    epoch_max_fda_steps = epoch_client_batches / NUM_STEPS_UNTIL_RTC_CHECK\n",
    "    epoch_max_fda_steps = tf.constant(int(epoch_max_fda_steps), shape=(), dtype=tf.int32)\n",
    "    \n",
    "    basic_test_metrics = []\n",
    "    \n",
    "    \"\"\" --------------- Naive ----------------------------------\"\"\"\n",
    "    \n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "\n",
    "    federated_dataset = create_federated_data(\n",
    "        client_slices_train=client_slices_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_buffer_size=int(n_train/NUM_CLIENTS),\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Run \n",
    "\n",
    "    total_rounds, total_fda_steps = run_federated_simulation_naive(\n",
    "        server_cnn, \n",
    "        client_cnns, \n",
    "        federated_dataset, \n",
    "        num_epochs, \n",
    "        theta,\n",
    "        epoch_max_fda_steps\n",
    "    )\n",
    "    \n",
    "    # 3. compute metrics\n",
    "    \n",
    "    _, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"naive\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc, \n",
    "        sketch_width=None, \n",
    "        sketch_depth=None\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    \n",
    "    \"\"\" --------------- Linear ----------------------------------\"\"\"\n",
    "\n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "\n",
    "    federated_dataset = create_federated_data(\n",
    "        client_slices_train=client_slices_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_buffer_size=int(n_train/NUM_CLIENTS),\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 3. Run \n",
    "\n",
    "    total_rounds, total_fda_steps = run_federated_simulation_linear(\n",
    "        previous_server_cnn,\n",
    "        server_cnn, \n",
    "        client_cnns, \n",
    "        federated_dataset, \n",
    "        num_epochs, \n",
    "        theta,\n",
    "        epoch_max_fda_steps\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 4. compute metrics\n",
    "    \n",
    "    loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"linear\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc, \n",
    "        sketch_width=None, \n",
    "        sketch_depth=None\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    previous_server_cnn.set_trainable_variables(starting_trainable_variables)  # +\n",
    "\n",
    "    \n",
    "    \"\"\" ------------------------ Sketch ----------------------\"\"\"\n",
    "\n",
    "    \n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "    \n",
    "    federated_dataset = create_federated_data(\n",
    "        client_slices_train=client_slices_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_buffer_size=int(n_train/NUM_CLIENTS),\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Run \n",
    "\n",
    "    total_rounds, total_fda_steps = run_federated_simulation_sketch(\n",
    "        server_cnn=server_cnn, \n",
    "        client_cnns=client_cnns, \n",
    "        federated_dataset=federated_dataset,\n",
    "        num_epochs=num_epochs, \n",
    "        theta=theta, \n",
    "        epoch_fda_steps=epoch_max_fda_steps, \n",
    "        ams_sketch=ams_sketch, \n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 3. compute metrics\n",
    "    \n",
    "    loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"sketch\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc, \n",
    "        sketch_width=ams_sketch.width, \n",
    "        sketch_depth=ams_sketch.depth\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "\n",
    "    del federated_dataset, total_rounds, total_fda_steps, acc\n",
    "    \n",
    "    # 4. IMPORTAND: Reset to the starting state all models\n",
    "    reset_trainable_variables(server_cnn, client_cnns, starting_trainable_variables)\n",
    "    \n",
    "    return basic_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f8f8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info_current_test(NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, THETA, BATCH_SIZE):\n",
    "    print()\n",
    "    print(f\"----------- Current Test --------------\")\n",
    "    print(f\"Num Clients : {NUM_CLIENTS}\")\n",
    "    print(f\"Num Epochs : {NUM_EPOCHS}\")\n",
    "    print(f\"Number of steps until we check RTC : {NUM_STEPS_UNTIL_RTC_CHECK}\")\n",
    "    print(f\"Batch size : {BATCH_SIZE}\")\n",
    "    print(f\"Theta : {THETA}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b2078c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt # new\n",
    "from copy import deepcopy\n",
    "\n",
    "def run_tests(NUM_CLIENTS_LIST, NUM_EPOCHS_LIST, NUM_STEPS_UNTIL_RTC_CHECK_LIST,\n",
    "              BATCH_SIZE_LIST, THETA_LIST, SKETCH_DEPTH, SKETCH_WIDTH, SEED=None):\n",
    "    \n",
    "    \"\"\" --------------- Fixed configurations -------------------\"\"\"\n",
    "\n",
    "    ams_sketch = AmsSketch(\n",
    "        depth=SKETCH_DEPTH,\n",
    "        width=SKETCH_WIDTH\n",
    "    )\n",
    "\n",
    "    EPSILON = 1. / sqrt(SKETCH_WIDTH)\n",
    "    \n",
    "    \n",
    "    \"\"\" --------------- Metrics list ----------------------\"\"\"\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        \"\"\" --------------- Run tests -------------------\"\"\"\n",
    "        for NUM_CLIENTS in NUM_CLIENTS_LIST:\n",
    "            \n",
    "            client_slices_train = create_data_for_clients(NUM_CLIENTS)  # new sliced dataset (diff NUM_CLIENTS)\n",
    "            \n",
    "            # we will create the CNNs here to avoid graph retracing (we will keep the same starting variables)\n",
    "            server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "            client_cnns = [get_compiled_and_built_advanced_cnn() for _ in range(NUM_CLIENTS)]\n",
    "            \n",
    "            previous_server_cnn = get_compiled_and_built_advanced_cnn()  # For linear\n",
    "            \n",
    "            # synchronize\n",
    "            synchronize(server_cnn, client_cnns)\n",
    "            \n",
    "            # keep the same starting variables in each test corresponding to the same `NUM_CLIENTS`\n",
    "            starting_trainable_variables = deepcopy(server_cnn.trainable_variables)\n",
    "            \n",
    "            previous_server_cnn.set_trainable_variables(starting_trainable_variables)  # For linear\n",
    "            \n",
    "            for NUM_EPOCHS in NUM_EPOCHS_LIST:\n",
    "                \n",
    "                for NUM_STEPS_UNTIL_RTC_CHECK in NUM_STEPS_UNTIL_RTC_CHECK_LIST:\n",
    "                    \n",
    "                    for BATCH_SIZE in BATCH_SIZE_LIST:\n",
    "                        \n",
    "                        for THETA in THETA_LIST:\n",
    "                            \n",
    "                            print_info_current_test(NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, THETA, BATCH_SIZE)\n",
    "                            \n",
    "                            basic_test_metrics = basic_test(\n",
    "                                server_cnn=server_cnn,\n",
    "                                client_cnns=client_cnns,\n",
    "                                previous_server_cnn=previous_server_cnn,\n",
    "                                starting_trainable_variables=starting_trainable_variables,\n",
    "                                NUM_EPOCHS=NUM_EPOCHS, \n",
    "                                NUM_STEPS_UNTIL_RTC_CHECK=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "                                NUM_CLIENTS=NUM_CLIENTS,\n",
    "                                BATCH_SIZE=BATCH_SIZE, \n",
    "                                THETA=THETA, \n",
    "                                EPSILON=EPSILON,\n",
    "                                ams_sketch=ams_sketch,\n",
    "                                client_slices_train=client_slices_train,\n",
    "                                seed=SEED\n",
    "                            )\n",
    "                            \n",
    "                            all_metrics.extend(basic_test_metrics)\n",
    "            \n",
    "            # Delete previous stuff because we will encounter a different `NUM_CLIENTS`\n",
    "            del client_slices_train, server_cnn, client_cnns, previous_server_cnn, starting_trainable_variables\n",
    "            \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"shit\")\n",
    "    \n",
    "    finally:\n",
    "        return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e61e5",
   "metadata": {},
   "source": [
    "# Run Simulation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a5b5b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Current Test --------------\n",
      "Num Clients : 5\n",
      "Num Epochs : 1\n",
      "Number of steps until we check RTC : 1\n",
      "Batch size : 32\n",
      "Theta : 1.0\n",
      "----------------------------------------\n",
      "\n",
      "retracing naive\n",
      "Naive Epoch count:  0  Total fda steps:  6\n",
      "Est var:  1.00654268  Actual S_2 (Assumed 0):  0.211811244  Actual var:  0.794731498\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  13\n",
      "Est var:  1.19458485  Actual S_2 (Assumed 0):  0.254070729  Actual var:  0.940514088\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  22\n",
      "Est var:  1.16193247  Actual S_2 (Assumed 0):  0.260227323  Actual var:  0.901705146\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  29\n",
      "Est var:  1.08550227  Actual S_2 (Assumed 0):  0.2277904  Actual var:  0.857711792\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  35\n",
      "Est var:  1.17221296  Actual S_2 (Assumed 0):  0.26786539  Actual var:  0.904347599\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  42\n",
      "Est var:  1.13227963  Actual S_2 (Assumed 0):  0.326543421  Actual var:  0.805736363\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  50\n",
      "Est var:  1.09558165  Actual S_2 (Assumed 0):  0.276186138  Actual var:  0.819395423\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  58\n",
      "Est var:  1.19389379  Actual S_2 (Assumed 0):  0.280888557  Actual var:  0.913005233\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  65\n",
      "Est var:  1.24619508  Actual S_2 (Assumed 0):  0.41591233  Actual var:  0.830282807\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  70\n",
      "Est var:  1.27364206  Actual S_2 (Assumed 0):  0.462740093  Actual var:  0.81090194\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  74\n",
      "Est var:  1.36403048  Actual S_2 (Assumed 0):  0.66467011  Actual var:  0.69936043\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  77\n",
      "Est var:  1.33839536  Actual S_2 (Assumed 0):  0.656295121  Actual var:  0.682100296\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  80\n",
      "Est var:  1.37194908  Actual S_2 (Assumed 0):  0.562173247  Actual var:  0.809775829\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  83\n",
      "Est var:  1.448789  Actual S_2 (Assumed 0):  0.503282309  Actual var:  0.945506573\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  86\n",
      "Est var:  1.50345433  Actual S_2 (Assumed 0):  0.443293273  Actual var:  1.06016099\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  89\n",
      "Est var:  1.30462265  Actual S_2 (Assumed 0):  0.453884125  Actual var:  0.850738406\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  92\n",
      "Est var:  1.16106033  Actual S_2 (Assumed 0):  0.454000473  Actual var:  0.70705986\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  95\n",
      "Est var:  1.20214522  Actual S_2 (Assumed 0):  0.57026279  Actual var:  0.63188237\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  98\n",
      "Est var:  1.30018842  Actual S_2 (Assumed 0):  0.747353554  Actual var:  0.552835\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  101\n",
      "Est var:  1.28356767  Actual S_2 (Assumed 0):  0.731146753  Actual var:  0.552420616\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  104\n",
      "Est var:  1.31660104  Actual S_2 (Assumed 0):  0.775053  Actual var:  0.541548\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  107\n",
      "Est var:  1.175686  Actual S_2 (Assumed 0):  0.65498656  Actual var:  0.520699382\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  110\n",
      "Est var:  1.16862297  Actual S_2 (Assumed 0):  0.682739735  Actual var:  0.485883236\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  113\n",
      "Est var:  1.17841041  Actual S_2 (Assumed 0):  0.569032371  Actual var:  0.609378159\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  116\n",
      "Est var:  1.13815856  Actual S_2 (Assumed 0):  0.507479072  Actual var:  0.630679548\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  119\n",
      "Est var:  1.1513412  Actual S_2 (Assumed 0):  0.498003393  Actual var:  0.653337836\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  122\n",
      "Est var:  1.15072441  Actual S_2 (Assumed 0):  0.47573778  Actual var:  0.674986541\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  126\n",
      "Est var:  1.64963412  Actual S_2 (Assumed 0):  0.696468234  Actual var:  0.953165889\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  130\n",
      "Est var:  1.45709205  Actual S_2 (Assumed 0):  0.628048122  Actual var:  0.829044\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  134\n",
      "Est var:  1.42441881  Actual S_2 (Assumed 0):  0.559024096  Actual var:  0.865394592\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  138\n",
      "Est var:  1.32511282  Actual S_2 (Assumed 0):  0.51837182  Actual var:  0.80674088\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  142\n",
      "Est var:  1.25324309  Actual S_2 (Assumed 0):  0.469316691  Actual var:  0.783926308\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  146\n",
      "Est var:  1.26556015  Actual S_2 (Assumed 0):  0.489607185  Actual var:  0.775953054\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  150\n",
      "Est var:  1.22692072  Actual S_2 (Assumed 0):  0.472262383  Actual var:  0.754658341\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  154\n",
      "Est var:  1.32558799  Actual S_2 (Assumed 0):  0.469131231  Actual var:  0.856456935\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  158\n",
      "Est var:  1.38316989  Actual S_2 (Assumed 0):  0.558483  Actual var:  0.824686885\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  162\n",
      "Est var:  1.41663456  Actual S_2 (Assumed 0):  0.513645709  Actual var:  0.902988791\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  166\n",
      "Est var:  1.34000921  Actual S_2 (Assumed 0):  0.457696855  Actual var:  0.882312298\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  170\n",
      "Est var:  1.21220374  Actual S_2 (Assumed 0):  0.422210842  Actual var:  0.789992929\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  174\n",
      "Est var:  1.06164062  Actual S_2 (Assumed 0):  0.350101352  Actual var:  0.711539268\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  179\n",
      "Est var:  1.40573359  Actual S_2 (Assumed 0):  0.454451352  Actual var:  0.951282203\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  183\n",
      "Est var:  1.03340566  Actual S_2 (Assumed 0):  0.314786017  Actual var:  0.718619645\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  188\n",
      "Est var:  1.21986  Actual S_2 (Assumed 0):  0.403010666  Actual var:  0.816849351\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  193\n",
      "Est var:  1.26925159  Actual S_2 (Assumed 0):  0.438195527  Actual var:  0.831056\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  198\n",
      "Est var:  1.28381312  Actual S_2 (Assumed 0):  0.417058915  Actual var:  0.866754174\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  203\n",
      "Est var:  1.42846942  Actual S_2 (Assumed 0):  0.419596374  Actual var:  1.00887299\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  207\n",
      "Est var:  1.0852288  Actual S_2 (Assumed 0):  0.335108101  Actual var:  0.750120759\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  211\n",
      "Est var:  1.04278541  Actual S_2 (Assumed 0):  0.348532587  Actual var:  0.694252849\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  215\n",
      "Est var:  1.03031194  Actual S_2 (Assumed 0):  0.367990732  Actual var:  0.66232127\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  220\n",
      "Est var:  1.38671732  Actual S_2 (Assumed 0):  0.447901398  Actual var:  0.938815892\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  224\n",
      "Est var:  1.02548301  Actual S_2 (Assumed 0):  0.352189  Actual var:  0.673293948\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  229\n",
      "Est var:  1.06859767  Actual S_2 (Assumed 0):  0.34777528  Actual var:  0.720822334\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  235\n",
      "Est var:  1.2937901  Actual S_2 (Assumed 0):  0.379045814  Actual var:  0.914744198\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  241\n",
      "Est var:  1.36191523  Actual S_2 (Assumed 0):  0.388096064  Actual var:  0.973819137\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  246\n",
      "Est var:  1.13376832  Actual S_2 (Assumed 0):  0.313758761  Actual var:  0.820009589\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  251\n",
      "Est var:  1.0970037  Actual S_2 (Assumed 0):  0.303392589  Actual var:  0.793611169\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  256\n",
      "Est var:  1.11161828  Actual S_2 (Assumed 0):  0.27627939  Actual var:  0.835338771\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  262\n",
      "Est var:  1.2462306  Actual S_2 (Assumed 0):  0.324266493  Actual var:  0.921964049\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  268\n",
      "Est var:  1.29097331  Actual S_2 (Assumed 0):  0.354965478  Actual var:  0.936007798\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  274\n",
      "Est var:  1.35838735  Actual S_2 (Assumed 0):  0.372641  Actual var:  0.985746384\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  279\n",
      "Est var:  1.14293122  Actual S_2 (Assumed 0):  0.346362531  Actual var:  0.796568692\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est var:  1.15916312  Actual S_2 (Assumed 0):  0.314011514  Actual var:  0.845151603\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  289\n",
      "Est var:  1.1322639  Actual S_2 (Assumed 0):  0.35259971  Actual var:  0.779664218\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  294\n",
      "Est var:  1.14527106  Actual S_2 (Assumed 0):  0.293053776  Actual var:  0.852217197\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  299\n",
      "Est var:  1.19535041  Actual S_2 (Assumed 0):  0.34828195  Actual var:  0.847068608\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  304\n",
      "Est var:  1.03404903  Actual S_2 (Assumed 0):  0.27295208  Actual var:  0.761097\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  309\n",
      "Est var:  1.06748939  Actual S_2 (Assumed 0):  0.299064487  Actual var:  0.768424928\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  314\n",
      "Est var:  1.09277046  Actual S_2 (Assumed 0):  0.31620127  Actual var:  0.776569128\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  319\n",
      "Est var:  1.21098506  Actual S_2 (Assumed 0):  0.374061048  Actual var:  0.836923957\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  324\n",
      "Est var:  1.0861342  Actual S_2 (Assumed 0):  0.279231668  Actual var:  0.806902528\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  331\n",
      "Est var:  1.24031043  Actual S_2 (Assumed 0):  0.356670707  Actual var:  0.883639693\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  337\n",
      "Est var:  1.05457187  Actual S_2 (Assumed 0):  0.261867583  Actual var:  0.792704225\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  343\n",
      "Est var:  1.14398897  Actual S_2 (Assumed 0):  0.286074191  Actual var:  0.857914805\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  348\n",
      "Est var:  1.11389315  Actual S_2 (Assumed 0):  0.294901848  Actual var:  0.818991184\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  354\n",
      "Est var:  1.33366036  Actual S_2 (Assumed 0):  0.387018055  Actual var:  0.94664222\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  359\n",
      "Est var:  1.20949113  Actual S_2 (Assumed 0):  0.339756  Actual var:  0.869735122\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  364\n",
      "Est var:  1.23688304  Actual S_2 (Assumed 0):  0.316561162  Actual var:  0.920321822\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  369\n",
      "Est var:  1.06786835  Actual S_2 (Assumed 0):  0.266267806  Actual var:  0.801600575\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  374\n",
      "Est var:  1.04178071  Actual S_2 (Assumed 0):  0.25675261  Actual var:  0.7850281\n",
      "\n",
      "\n",
      "Naive Epoch count:  1  Total fda steps:  375\n",
      "Est var:  0.0537952855  Actual S_2 (Assumed 0):  0.0112011544  Actual var:  0.0425941311\n",
      "\n",
      "\n",
      "retracing linear\n",
      "Linear Epoch count:  0  Total fda steps:  4\n",
      "Est var:  1.57393754  Actual S_2:  0.328801692  Approx S_2:  5.50158035e-08  Actual var:  1.24513578\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  7\n",
      "Est var:  1.63277733  Actual S_2:  0.380260438  Approx S_2:  0.199012592  Actual var:  1.4515295\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  10\n",
      "Est var:  1.66582274  Actual S_2:  0.394472718  Approx S_2:  0.215947688  Actual var:  1.48729777\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  13\n",
      "Est var:  1.49991989  Actual S_2:  0.379593968  Approx S_2:  0.237553984  Actual var:  1.35788012\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  16\n",
      "Est var:  1.21116352  Actual S_2:  0.307847142  Approx S_2:  0.216463074  Actual var:  1.11977959\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  19\n",
      "Est var:  1.22238064  Actual S_2:  0.351262301  Approx S_2:  0.203953549  Actual var:  1.07507169\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  22\n",
      "Est var:  1.56620336  Actual S_2:  0.61240685  Approx S_2:  0.279922217  Actual var:  1.23371887\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  24\n",
      "Est var:  1.07652688  Actual S_2:  0.441408634  Approx S_2:  0.293405056  Actual var:  0.928523421\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  26\n",
      "Est var:  1.43384278  Actual S_2:  0.505033851  Approx S_2:  0.108223818  Actual var:  1.03703272\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  28\n",
      "Est var:  1.39842689  Actual S_2:  0.469398141  Approx S_2:  0.379351735  Actual var:  1.30838048\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  30\n",
      "Est var:  1.2837199  Actual S_2:  0.474868774  Approx S_2:  0.314404339  Actual var:  1.12325549\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  32\n",
      "Est var:  1.28403676  Actual S_2:  0.570711553  Approx S_2:  0.376686186  Actual var:  1.09001148\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  34\n",
      "Est var:  1.18453276  Actual S_2:  1.09841561  Approx S_2:  0.795031548  Actual var:  0.881148696\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  36\n",
      "Est var:  1.81351471  Actual S_2:  1.12148225  Approx S_2:  0.181580782  Actual var:  0.87361306\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  38\n",
      "Est var:  1.77212501  Actual S_2:  0.999603212  Approx S_2:  0.676426828  Actual var:  1.44894862\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  40\n",
      "Est var:  2.48260617  Actual S_2:  1.24882305  Approx S_2:  0.686592042  Actual var:  1.92037547\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  42\n",
      "Est var:  1.90136266  Actual S_2:  0.988156915  Approx S_2:  0.476976395  Actual var:  1.3901819\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  44\n",
      "Est var:  2.17811489  Actual S_2:  0.975829601  Approx S_2:  0.601101398  Actual var:  1.80338693\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  46\n",
      "Est var:  1.77815151  Actual S_2:  0.93877089  Approx S_2:  0.615287244  Actual var:  1.45466793\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  48\n",
      "Est var:  1.56878757  Actual S_2:  0.982626915  Approx S_2:  0.669661939  Actual var:  1.25582242\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  50\n",
      "Est var:  1.06425691  Actual S_2:  0.900707841  Approx S_2:  0.793706119  Actual var:  0.957255185\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  52\n",
      "Est var:  1.03341293  Actual S_2:  0.675180435  Approx S_2:  0.392640293  Actual var:  0.750872791\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  55\n",
      "Est var:  1.72464025  Actual S_2:  1.49092031  Approx S_2:  1.1617254  Actual var:  1.39544511\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  58\n",
      "Est var:  1.9643445  Actual S_2:  1.47719586  Approx S_2:  0.766747952  Actual var:  1.25389636\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  61\n",
      "Est var:  1.70570016  Actual S_2:  1.56605732  Approx S_2:  1.1957233  Actual var:  1.33536601\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  63\n",
      "Est var:  1.08933818  Actual S_2:  0.64872396  Approx S_2:  0.431507349  Actual var:  0.872121632\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  65\n",
      "Est var:  1.21751392  Actual S_2:  0.616989613  Approx S_2:  0.426560491  Actual var:  1.02708483\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  67\n",
      "Est var:  1.13684285  Actual S_2:  0.576350808  Approx S_2:  0.414555907  Actual var:  0.975047886\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  69\n",
      "Est var:  1.09168077  Actual S_2:  0.523694277  Approx S_2:  0.359002799  Actual var:  0.926989377\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  71\n",
      "Est var:  1.05247045  Actual S_2:  0.524452209  Approx S_2:  0.369805843  Actual var:  0.897824109\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  74\n",
      "Est var:  2.127285  Actual S_2:  1.06813633  Approx S_2:  0.759376  Actual var:  1.8185246\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  76\n",
      "Est var:  1.03446054  Actual S_2:  0.486614496  Approx S_2:  0.343052626  Actual var:  0.890898824\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  79\n",
      "Est var:  1.90920663  Actual S_2:  1.03643775  Approx S_2:  0.769212067  Actual var:  1.64198077\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  82\n",
      "Est var:  1.91557336  Actual S_2:  0.980882883  Approx S_2:  0.524809122  Actual var:  1.45949936\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  85\n",
      "Est var:  1.92030263  Actual S_2:  1.07995713  Approx S_2:  0.606189787  Actual var:  1.44653535\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  87\n",
      "Est var:  1.10156298  Actual S_2:  0.56017983  Approx S_2:  0.192198083  Actual var:  0.733581066\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  90\n",
      "Est var:  2.1568644  Actual S_2:  1.04573739  Approx S_2:  0.544563353  Actual var:  1.65569019\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  93\n",
      "Est var:  1.87372613  Actual S_2:  1.19555593  Approx S_2:  0.847352  Actual var:  1.52552235\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  96\n",
      "Est var:  1.88402653  Actual S_2:  1.17973924  Approx S_2:  0.732168376  Actual var:  1.43645549\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  99\n",
      "Est var:  1.8298583  Actual S_2:  1.11373258  Approx S_2:  0.673781455  Actual var:  1.38990724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  102\n",
      "Est var:  1.94687223  Actual S_2:  1.09023583  Approx S_2:  0.71665436  Actual var:  1.57329059\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  104\n",
      "Est var:  1.01601601  Actual S_2:  0.544462204  Approx S_2:  0.299692839  Actual var:  0.771246612\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  107\n",
      "Est var:  2.02239799  Actual S_2:  1.11133301  Approx S_2:  0.580629408  Actual var:  1.49169421\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  110\n",
      "Est var:  1.82016909  Actual S_2:  1.02833724  Approx S_2:  0.601660132  Actual var:  1.39349186\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  113\n",
      "Est var:  1.78124058  Actual S_2:  0.941491187  Approx S_2:  0.584117293  Actual var:  1.42386651\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  116\n",
      "Est var:  1.69707644  Actual S_2:  0.838538527  Approx S_2:  0.504163623  Actual var:  1.36270165\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  119\n",
      "Est var:  1.84507406  Actual S_2:  0.808854699  Approx S_2:  0.480940729  Actual var:  1.51716018\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  122\n",
      "Est var:  1.80693769  Actual S_2:  0.8466326  Approx S_2:  0.522631705  Actual var:  1.48293662\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  125\n",
      "Est var:  1.61990011  Actual S_2:  0.704161584  Approx S_2:  0.370702386  Actual var:  1.28644109\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  128\n",
      "Est var:  1.370978  Actual S_2:  0.644078672  Approx S_2:  0.39980188  Actual var:  1.126701\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  131\n",
      "Est var:  1.46841025  Actual S_2:  0.713662326  Approx S_2:  0.468368381  Actual var:  1.2231164\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  134\n",
      "Est var:  1.44124401  Actual S_2:  0.547195077  Approx S_2:  0.275234342  Actual var:  1.16928315\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  137\n",
      "Est var:  1.46984649  Actual S_2:  0.572290897  Approx S_2:  0.268786907  Actual var:  1.16634238\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  140\n",
      "Est var:  1.28808975  Actual S_2:  0.534046054  Approx S_2:  0.233691514  Actual var:  0.987735271\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  143\n",
      "Est var:  1.27951932  Actual S_2:  0.454389513  Approx S_2:  0.118548132  Actual var:  0.943678\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  147\n",
      "Est var:  1.55900335  Actual S_2:  0.651389539  Approx S_2:  0.415905505  Actual var:  1.32351935\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  151\n",
      "Est var:  1.36447179  Actual S_2:  0.527289748  Approx S_2:  0.306271523  Actual var:  1.1434536\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  155\n",
      "Est var:  1.65355933  Actual S_2:  0.598912537  Approx S_2:  0.311435729  Actual var:  1.36608243\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  158\n",
      "Est var:  1.03394091  Actual S_2:  0.337296426  Approx S_2:  0.179208145  Actual var:  0.875852466\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  161\n",
      "Est var:  1.03106058  Actual S_2:  0.36465764  Approx S_2:  0.229619816  Actual var:  0.896022677\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  165\n",
      "Est var:  1.62704921  Actual S_2:  0.62450844  Approx S_2:  0.280213177  Actual var:  1.28275406\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  169\n",
      "Est var:  1.56374431  Actual S_2:  0.529842  Approx S_2:  0.196117073  Actual var:  1.23001933\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  173\n",
      "Est var:  1.26471114  Actual S_2:  0.435525388  Approx S_2:  0.250929862  Actual var:  1.08011568\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  177\n",
      "Est var:  1.44585633  Actual S_2:  0.452703118  Approx S_2:  0.193529621  Actual var:  1.18668294\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  181\n",
      "Est var:  1.58383393  Actual S_2:  0.535740614  Approx S_2:  0.214545622  Actual var:  1.26263881\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  185\n",
      "Est var:  1.54670703  Actual S_2:  0.564267  Approx S_2:  0.16378352  Actual var:  1.14622378\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  189\n",
      "Est var:  1.24724436  Actual S_2:  0.408629566  Approx S_2:  0.201424778  Actual var:  1.04003966\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  193\n",
      "Est var:  1.12678397  Actual S_2:  0.413253069  Approx S_2:  0.206737176  Actual var:  0.920268059\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  197\n",
      "Est var:  1.22062349  Actual S_2:  0.397618741  Approx S_2:  0.183882982  Actual var:  1.00688767\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  201\n",
      "Est var:  1.25612795  Actual S_2:  0.360443085  Approx S_2:  0.169393852  Actual var:  1.06507897\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  205\n",
      "Est var:  1.25349021  Actual S_2:  0.374873817  Approx S_2:  0.205249324  Actual var:  1.08386588\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  209\n",
      "Est var:  1.15342581  Actual S_2:  0.347913146  Approx S_2:  0.163930312  Actual var:  0.969442725\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  213\n",
      "Est var:  1.13675261  Actual S_2:  0.339880317  Approx S_2:  0.144064382  Actual var:  0.940936744\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  218\n",
      "Est var:  1.46851218  Actual S_2:  0.416397274  Approx S_2:  0.187539414  Actual var:  1.23965454\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  223\n",
      "Est var:  1.48651636  Actual S_2:  0.435357749  Approx S_2:  0.203148857  Actual var:  1.25430751\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  227\n",
      "Est var:  1.11045563  Actual S_2:  0.290985137  Approx S_2:  0.0969604179  Actual var:  0.916430831\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  231\n",
      "Est var:  1.13499165  Actual S_2:  0.337882787  Approx S_2:  0.102063417  Actual var:  0.899172187\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  235\n",
      "Est var:  1.06089914  Actual S_2:  0.27311042  Approx S_2:  0.0974454  Actual var:  0.885234177\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  240\n",
      "Est var:  1.33234668  Actual S_2:  0.381789953  Approx S_2:  0.208909988  Actual var:  1.15946674\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  245\n",
      "Est var:  1.14653289  Actual S_2:  0.331138074  Approx S_2:  0.163292646  Actual var:  0.978687406\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  250\n",
      "Est var:  1.11632204  Actual S_2:  0.304870903  Approx S_2:  0.131919459  Actual var:  0.94337064\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  255\n",
      "Est var:  1.40665042  Actual S_2:  0.35310632  Approx S_2:  0.0821409598  Actual var:  1.13568509\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  259\n",
      "Est var:  1.1154958  Actual S_2:  0.314255506  Approx S_2:  0.132747456  Actual var:  0.933987737\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  264\n",
      "Est var:  1.18137789  Actual S_2:  0.319871515  Approx S_2:  0.159675375  Actual var:  1.0211817\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  269\n",
      "Est var:  1.18847954  Actual S_2:  0.296641618  Approx S_2:  0.120304473  Actual var:  1.01214242\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  274\n",
      "Est var:  1.12777519  Actual S_2:  0.29190436  Approx S_2:  0.123700336  Actual var:  0.959571242\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  279\n",
      "Est var:  1.13168108  Actual S_2:  0.283970743  Approx S_2:  0.112758271  Actual var:  0.96046865\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  284\n",
      "Est var:  1.26022041  Actual S_2:  0.326748  Approx S_2:  0.136462897  Actual var:  1.0699352\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  289\n",
      "Est var:  1.24494219  Actual S_2:  0.325669736  Approx S_2:  0.121985652  Actual var:  1.04125822\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  294\n",
      "Est var:  1.16471815  Actual S_2:  0.299263  Approx S_2:  0.133107811  Actual var:  0.998562813\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  299\n",
      "Est var:  1.17452681  Actual S_2:  0.287583768  Approx S_2:  0.0988299  Actual var:  0.985772908\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  304\n",
      "Est var:  1.48288679  Actual S_2:  0.370598137  Approx S_2:  0.12450204  Actual var:  1.23679078\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  309\n",
      "Est var:  1.26466119  Actual S_2:  0.377117336  Approx S_2:  0.16991742  Actual var:  1.0574615\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  314\n",
      "Est var:  1.05639279  Actual S_2:  0.26409179  Approx S_2:  0.123206072  Actual var:  0.915506959\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  319\n",
      "Est var:  1.12490416  Actual S_2:  0.274415  Approx S_2:  0.116949163  Actual var:  0.967438221\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  325\n",
      "Est var:  1.23307097  Actual S_2:  0.283093035  Approx S_2:  0.117762163  Actual var:  1.06774\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est var:  1.13301432  Actual S_2:  0.284650147  Approx S_2:  0.116843864  Actual var:  0.965207934\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  336\n",
      "Est var:  1.02094877  Actual S_2:  0.244424477  Approx S_2:  0.102091387  Actual var:  0.878615737\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  341\n",
      "Est var:  1.21733034  Actual S_2:  0.273468763  Approx S_2:  0.0864011  Actual var:  1.03026271\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  346\n",
      "Est var:  1.26684439  Actual S_2:  0.321352035  Approx S_2:  0.131026298  Actual var:  1.07651842\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  351\n",
      "Est var:  1.14376378  Actual S_2:  0.272951  Approx S_2:  0.121256828  Actual var:  0.992069602\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  357\n",
      "Est var:  1.3763783  Actual S_2:  0.36931026  Approx S_2:  0.116683476  Actual var:  1.12375152\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  362\n",
      "Est var:  1.06656194  Actual S_2:  0.253412843  Approx S_2:  0.104409926  Actual var:  0.917559\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  367\n",
      "Est var:  1.00662947  Actual S_2:  0.242345393  Approx S_2:  0.0588748492  Actual var:  0.82315886\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  372\n",
      "Est var:  1.00154245  Actual S_2:  0.247366726  Approx S_2:  0.117180511  Actual var:  0.871356189\n",
      "\n",
      "\n",
      "Linear Epoch count:  1  Total fda steps:  375\n",
      "Est var:  0.360076696  Actual S_2:  0.0865951926  Approx S_2:  0.0479313247  Actual var:  0.321412832\n",
      "\n",
      "\n",
      "retracing sketch\n",
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Sketch Epoch count:  0  Total fda steps:  5\n",
      "Est var:  1.41179848  Actual S_2:  0.364079505  Apprxo S_2 0.35141176  Actual var:  1.3991307\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  8\n",
      "Est var:  1.19383192  Actual S_2:  0.289275169  Apprxo S_2 0.279979885  Actual var:  1.18453658\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  11\n",
      "Est var:  1.23700142  Actual S_2:  0.313181043  Apprxo S_2 0.298936218  Actual var:  1.22275662\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  14\n",
      "Est var:  1.30070817  Actual S_2:  0.347848386  Apprxo S_2 0.332534641  Actual var:  1.28539467\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  17\n",
      "Est var:  1.27580333  Actual S_2:  0.409081668  Apprxo S_2 0.395261079  Actual var:  1.26198268\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  21\n",
      "Est var:  1.67845106  Actual S_2:  0.659007  Apprxo S_2 0.639810264  Actual var:  1.65925431\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  24\n",
      "Est var:  1.18779135  Actual S_2:  0.509493232  Apprxo S_2 0.501623273  Actual var:  1.17992151\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  27\n",
      "Est var:  1.80880141  Actual S_2:  0.549745202  Apprxo S_2 0.553019941  Actual var:  1.81207621\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  30\n",
      "Est var:  1.6787467  Actual S_2:  0.918382883  Apprxo S_2 0.937580109  Actual var:  1.69794428\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  32\n",
      "Est var:  1.02336144  Actual S_2:  0.641569555  Apprxo S_2 0.664577842  Actual var:  1.04636979\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  34\n",
      "Est var:  1.03240442  Actual S_2:  0.965170205  Apprxo S_2 0.959153414  Actual var:  1.02638769\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  36\n",
      "Est var:  1.05265582  Actual S_2:  0.9485147  Apprxo S_2 0.911106467  Actual var:  1.0152477\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  38\n",
      "Est var:  1.54644835  Actual S_2:  1.82223129  Apprxo S_2 1.77606332  Actual var:  1.50028086\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  40\n",
      "Est var:  1.447878  Actual S_2:  1.33141077  Apprxo S_2 1.30066717  Actual var:  1.41713428\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  42\n",
      "Est var:  1.41632223  Actual S_2:  1.00043809  Apprxo S_2 0.974032342  Actual var:  1.3899163\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  44\n",
      "Est var:  1.351578  Actual S_2:  0.774957657  Apprxo S_2 0.76252  Actual var:  1.33914018\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  46\n",
      "Est var:  1.25427866  Actual S_2:  0.62145108  Apprxo S_2 0.605833232  Actual var:  1.23866081\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  48\n",
      "Est var:  1.07379735  Actual S_2:  0.499423832  Apprxo S_2 0.48574838  Actual var:  1.06012177\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  51\n",
      "Est var:  1.80858898  Actual S_2:  0.988747656  Apprxo S_2 0.979742825  Actual var:  1.79958403\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  54\n",
      "Est var:  1.30899644  Actual S_2:  0.711322904  Apprxo S_2 0.705071688  Actual var:  1.30274522\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  58\n",
      "Est var:  1.52180696  Actual S_2:  0.730173886  Apprxo S_2 0.707754672  Actual var:  1.49938774\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  62\n",
      "Est var:  1.05617523  Actual S_2:  0.460119486  Apprxo S_2 0.440782905  Actual var:  1.03683853\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  67\n",
      "Est var:  1.28903687  Actual S_2:  0.612400055  Apprxo S_2 0.603712559  Actual var:  1.28034949\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  71\n",
      "Est var:  1.14855576  Actual S_2:  0.65891391  Apprxo S_2 0.670481324  Actual var:  1.16012311\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  75\n",
      "Est var:  1.50890541  Actual S_2:  0.96746707  Apprxo S_2 0.961850405  Actual var:  1.50328863\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  79\n",
      "Est var:  1.32399821  Actual S_2:  1.04759014  Apprxo S_2 1.04040456  Actual var:  1.31681263\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  83\n",
      "Est var:  1.34856272  Actual S_2:  1.07104051  Apprxo S_2 1.02565  Actual var:  1.30317187\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  86\n",
      "Est var:  1.46377671  Actual S_2:  0.916512609  Apprxo S_2 0.872364879  Actual var:  1.4196291\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  89\n",
      "Est var:  1.14289594  Actual S_2:  0.72140646  Apprxo S_2 0.715677261  Actual var:  1.13716662\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  93\n",
      "Est var:  1.52628922  Actual S_2:  1.25169742  Apprxo S_2 1.21888041  Actual var:  1.49347234\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  96\n",
      "Est var:  1.04361558  Actual S_2:  1.07302558  Apprxo S_2 1.04384732  Actual var:  1.0144372\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  99\n",
      "Est var:  1.54494619  Actual S_2:  1.12631607  Apprxo S_2 1.10355568  Actual var:  1.52218568\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  102\n",
      "Est var:  1.72653937  Actual S_2:  1.25124049  Apprxo S_2 1.23067832  Actual var:  1.70597708\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  105\n",
      "Est var:  1.42280006  Actual S_2:  1.00174451  Apprxo S_2 0.990096629  Actual var:  1.41115212\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  108\n",
      "Est var:  1.12603474  Actual S_2:  0.82018584  Apprxo S_2 0.82252866  Actual var:  1.12837756\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  111\n",
      "Est var:  1.08003271  Actual S_2:  0.674446821  Apprxo S_2 0.677643061  Actual var:  1.08322871\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  114\n",
      "Est var:  1.05414414  Actual S_2:  0.597858667  Apprxo S_2 0.595347464  Actual var:  1.05163276\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  118\n",
      "Est var:  1.39962411  Actual S_2:  0.925798297  Apprxo S_2 0.94022578  Actual var:  1.41405165\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  122\n",
      "Est var:  1.37866116  Actual S_2:  0.852334619  Apprxo S_2 0.827310085  Actual var:  1.35363674\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  126\n",
      "Est var:  1.25665  Actual S_2:  0.825639  Apprxo S_2 0.796519697  Actual var:  1.22753072\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  130\n",
      "Est var:  1.17122436  Actual S_2:  0.739832044  Apprxo S_2 0.704326212  Actual var:  1.13571858\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  134\n",
      "Est var:  1.18383598  Actual S_2:  0.670663953  Apprxo S_2 0.63653475  Actual var:  1.14970672\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  138\n",
      "Est var:  1.13771176  Actual S_2:  0.616063654  Apprxo S_2 0.58344537  Actual var:  1.10509348\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  142\n",
      "Est var:  1.07735777  Actual S_2:  0.571674764  Apprxo S_2 0.572937548  Actual var:  1.07862055\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est var:  1.09927654  Actual S_2:  0.601066589  Apprxo S_2 0.602844477  Actual var:  1.10105431\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  151\n",
      "Est var:  1.37855768  Actual S_2:  0.821070254  Apprxo S_2 0.815485656  Actual var:  1.3729732\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  156\n",
      "Est var:  1.38467741  Actual S_2:  0.771135092  Apprxo S_2 0.76621604  Actual var:  1.37975824\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  161\n",
      "Est var:  1.36109269  Actual S_2:  0.642576456  Apprxo S_2 0.625255466  Actual var:  1.3437717\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  166\n",
      "Est var:  1.33712935  Actual S_2:  0.599927604  Apprxo S_2 0.587046802  Actual var:  1.32424855\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  171\n",
      "Est var:  1.24446917  Actual S_2:  0.582102418  Apprxo S_2 0.562763393  Actual var:  1.22513032\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  176\n",
      "Est var:  1.34563  Actual S_2:  0.524715483  Apprxo S_2 0.516708612  Actual var:  1.33762324\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  181\n",
      "Est var:  1.22568715  Actual S_2:  0.474903703  Apprxo S_2 0.469814807  Actual var:  1.22059834\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  186\n",
      "Est var:  1.22477126  Actual S_2:  0.501878679  Apprxo S_2 0.483431637  Actual var:  1.20632434\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  191\n",
      "Est var:  1.12668872  Actual S_2:  0.44294557  Apprxo S_2 0.436092079  Actual var:  1.11983502\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  196\n",
      "Est var:  1.17139482  Actual S_2:  0.447189212  Apprxo S_2 0.433558583  Actual var:  1.1577642\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  201\n",
      "Est var:  1.06976986  Actual S_2:  0.402717888  Apprxo S_2 0.391882718  Actual var:  1.05893481\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  207\n",
      "Est var:  1.35064256  Actual S_2:  0.471434653  Apprxo S_2 0.469346404  Actual var:  1.34855437\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  213\n",
      "Est var:  1.23209274  Actual S_2:  0.48518002  Apprxo S_2 0.482581466  Actual var:  1.22949421\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  219\n",
      "Est var:  1.28768933  Actual S_2:  0.496739507  Apprxo S_2 0.472113222  Actual var:  1.26306319\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  225\n",
      "Est var:  1.27701747  Actual S_2:  0.495217532  Apprxo S_2 0.489668936  Actual var:  1.27146888\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  231\n",
      "Est var:  1.14018536  Actual S_2:  0.462124705  Apprxo S_2 0.462727278  Actual var:  1.14078796\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  237\n",
      "Est var:  1.05031466  Actual S_2:  0.417970806  Apprxo S_2 0.412741482  Actual var:  1.04508519\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  243\n",
      "Est var:  1.05211091  Actual S_2:  0.3991763  Apprxo S_2 0.397856832  Actual var:  1.05079138\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  249\n",
      "Est var:  1.07497108  Actual S_2:  0.475729495  Apprxo S_2 0.458652586  Actual var:  1.05789423\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  256\n",
      "Est var:  1.28326964  Actual S_2:  0.49328661  Apprxo S_2 0.482174546  Actual var:  1.27215779\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  262\n",
      "Est var:  1.17444789  Actual S_2:  0.408864886  Apprxo S_2 0.396231383  Actual var:  1.16181433\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  268\n",
      "Est var:  1.23055017  Actual S_2:  0.477723479  Apprxo S_2 0.452018648  Actual var:  1.20484543\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  273\n",
      "Est var:  1.02133119  Actual S_2:  0.356116235  Apprxo S_2 0.349528432  Actual var:  1.01474357\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  279\n",
      "Est var:  1.31278253  Actual S_2:  0.437774569  Apprxo S_2 0.446435124  Actual var:  1.32144308\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  285\n",
      "Est var:  1.15379572  Actual S_2:  0.397525489  Apprxo S_2 0.392558783  Actual var:  1.14882886\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  291\n",
      "Est var:  1.02758789  Actual S_2:  0.347700417  Apprxo S_2 0.340092  Actual var:  1.01997948\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  298\n",
      "Est var:  1.27736926  Actual S_2:  0.475426883  Apprxo S_2 0.47121352  Actual var:  1.27315593\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  304\n",
      "Est var:  1.15440893  Actual S_2:  0.373040527  Apprxo S_2 0.360585392  Actual var:  1.14195371\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  310\n",
      "Est var:  1.05842757  Actual S_2:  0.41135928  Apprxo S_2 0.394100249  Actual var:  1.04116857\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  317\n",
      "Est var:  1.26334882  Actual S_2:  0.441313922  Apprxo S_2 0.426223874  Actual var:  1.24825883\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  323\n",
      "Est var:  1.05952096  Actual S_2:  0.337820649  Apprxo S_2 0.337848812  Actual var:  1.05954909\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  330\n",
      "Est var:  1.20331168  Actual S_2:  0.418317854  Apprxo S_2 0.403936803  Actual var:  1.18893075\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  337\n",
      "Est var:  1.16571128  Actual S_2:  0.360002458  Apprxo S_2 0.355944276  Actual var:  1.16165316\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  343\n",
      "Est var:  1.08748817  Actual S_2:  0.315716207  Apprxo S_2 0.317369729  Actual var:  1.08914161\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  349\n",
      "Est var:  1.00755453  Actual S_2:  0.334217459  Apprxo S_2 0.324397296  Actual var:  0.997734427\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  356\n",
      "Est var:  1.08952856  Actual S_2:  0.359058022  Apprxo S_2 0.354096144  Actual var:  1.08456659\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  363\n",
      "Est var:  1.14253449  Actual S_2:  0.349496156  Apprxo S_2 0.333526969  Actual var:  1.12656534\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  370\n",
      "Est var:  1.19226623  Actual S_2:  0.367509484  Apprxo S_2 0.348633587  Actual var:  1.17339039\n",
      "\n",
      "\n",
      "Sketch Epoch count:  1  Total fda steps:  375\n",
      "Est var:  0.65257  Actual S_2:  0.186266541  Apprxo S_2 0.174513683  Actual var:  0.640817106\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_metrics = run_tests(\n",
    "    NUM_CLIENTS_LIST=[10],\n",
    "    NUM_EPOCHS_LIST=[1],\n",
    "    NUM_STEPS_UNTIL_RTC_CHECK_LIST=[1],\n",
    "    BATCH_SIZE_LIST=[128],\n",
    "    THETA_LIST=[1.],\n",
    "    SKETCH_DEPTH=7,\n",
    "    SKETCH_WIDTH=1000,\n",
    "    SEED=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93aa398",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    \n",
    "4. DONE: `get_compiled_and_built_...()` retraces for `server_cnn` every time (ofc for `client_cnns` aswell).\n",
    "   BUT: make sure once more that when we `reset` then afterwards `.evaluate` works correctly. Maybe weird shit with metrics. Check plz\n",
    "\n",
    "\n",
    "5. Approach on sketch should be `reduce_mean`, change it in PA-I.\n",
    "6. Approach on global tests `for` loop PA-I\n",
    "7. remove `one` as a `tf.constant(1)` PA-I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4b2b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da382e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_results/advanced_cnn_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e3cd8a9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fda_name': 'naive',\n",
       "  'theta': 1.0,\n",
       "  'dataset_name': 'EMNIST',\n",
       "  'input_pixels': 784,\n",
       "  'n_train': 60000,\n",
       "  'num_weights': 2592202,\n",
       "  'seed': 7,\n",
       "  'epochs': 1,\n",
       "  'num_clients': 5,\n",
       "  'batch_size': 32,\n",
       "  'steps_in_one_fda_step': 1,\n",
       "  'sketch_width': None,\n",
       "  'sketch_depth': None,\n",
       "  'one_sample_bytes': 3140,\n",
       "  'training_dataset_bytes': 188400000,\n",
       "  'model_bytes': 10368808,\n",
       "  'local_state_bytes': 4,\n",
       "  'final_accuracy': 0.9589999914169312,\n",
       "  'total_fda_steps': 375,\n",
       "  'total_steps': 375,\n",
       "  'total_rounds': 80,\n",
       "  'model_bytes_exchanged': 8295046400,\n",
       "  'monitoring_bytes_exchanged': 7500,\n",
       "  'total_communication_bytes': 8295053900,\n",
       "  'trained_in_bytes': 188400000},\n",
       " {'fda_name': 'linear',\n",
       "  'theta': 1.0,\n",
       "  'dataset_name': 'EMNIST',\n",
       "  'input_pixels': 784,\n",
       "  'n_train': 60000,\n",
       "  'num_weights': 2592202,\n",
       "  'seed': 7,\n",
       "  'epochs': 1,\n",
       "  'num_clients': 5,\n",
       "  'batch_size': 32,\n",
       "  'steps_in_one_fda_step': 1,\n",
       "  'sketch_width': None,\n",
       "  'sketch_depth': None,\n",
       "  'one_sample_bytes': 3140,\n",
       "  'training_dataset_bytes': 188400000,\n",
       "  'model_bytes': 10368808,\n",
       "  'local_state_bytes': 8,\n",
       "  'final_accuracy': 0.9790999889373779,\n",
       "  'total_fda_steps': 375,\n",
       "  'total_steps': 375,\n",
       "  'total_rounds': 106,\n",
       "  'model_bytes_exchanged': 10990936480,\n",
       "  'monitoring_bytes_exchanged': 15000,\n",
       "  'total_communication_bytes': 10990951480,\n",
       "  'trained_in_bytes': 188400000},\n",
       " {'fda_name': 'sketch',\n",
       "  'theta': 1.0,\n",
       "  'dataset_name': 'EMNIST',\n",
       "  'input_pixels': 784,\n",
       "  'n_train': 60000,\n",
       "  'num_weights': 2592202,\n",
       "  'seed': 7,\n",
       "  'epochs': 1,\n",
       "  'num_clients': 5,\n",
       "  'batch_size': 32,\n",
       "  'steps_in_one_fda_step': 1,\n",
       "  'sketch_width': 1700,\n",
       "  'sketch_depth': 7,\n",
       "  'one_sample_bytes': 3140,\n",
       "  'training_dataset_bytes': 188400000,\n",
       "  'model_bytes': 10368808,\n",
       "  'local_state_bytes': 47604,\n",
       "  'final_accuracy': 0.9682999849319458,\n",
       "  'total_fda_steps': 375,\n",
       "  'total_steps': 375,\n",
       "  'total_rounds': 84,\n",
       "  'model_bytes_exchanged': 8709798720,\n",
       "  'monitoring_bytes_exchanged': 89257500,\n",
       "  'total_communication_bytes': 8799056220,\n",
       "  'trained_in_bytes': 188400000}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952a821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d44204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tff-py39] *",
   "language": "python",
   "name": "conda-env-tff-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
