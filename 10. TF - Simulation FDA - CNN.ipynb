{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2759993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74531322",
   "metadata": {},
   "source": [
    "## Import EMNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a550108",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65570a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a83a61",
   "metadata": {},
   "source": [
    "## Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb02a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "del X_train\n",
    "\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.int32)\n",
    "del y_train\n",
    "\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "del X_test\n",
    "\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.int32)\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ab8a7",
   "metadata": {},
   "source": [
    "## Prepare data for Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e112b",
   "metadata": {},
   "source": [
    "### Create centralized testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fad165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_test = (X_test_tensor, y_test_tensor)\n",
    "\n",
    "def create_tf_dataset_for_testing(batch_size):\n",
    "    return tf.data.Dataset.from_tensor_slices(slices_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84aa24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = create_tf_dataset_for_testing(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24a95c",
   "metadata": {},
   "source": [
    "### Slice the Tensors for each Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43734f7",
   "metadata": {},
   "source": [
    "We will cut the training data, i.e., (`X_train_tensor`, `y_train_tensor`) to equal parts, each part corresponding to one Client. We want to give the result back as a dictionary with key `client_id` and value the training tensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b27a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_clients(num_clients):\n",
    "    \n",
    "    client_slices_train = {}\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        # Compute the indices for this client's slice\n",
    "        start_idx = int(i * n_train / num_clients)\n",
    "        end_idx = int((i + 1) * n_train / num_clients)\n",
    "\n",
    "        # Get the slice for this client\n",
    "        X_client_train = X_train_tensor[start_idx:end_idx]\n",
    "        y_client_train = y_train_tensor[start_idx:end_idx]\n",
    "        \n",
    "        # Combine the slices into a single dataset\n",
    "        client_slices_train[f'client_{i}'] = (X_client_train, y_client_train)\n",
    "    \n",
    "    return client_slices_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88734523",
   "metadata": {},
   "source": [
    "### Create TF friendly data for each Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c58dbae",
   "metadata": {},
   "source": [
    "Given a Tensor slice (i.e. value of `client_slices_train[\"client_id\"]` we convert it to highly optimized `tf.data.Dataset` to prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794535c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset_for_client(client_tensor_slices, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed):\n",
    "    \n",
    "        return tf.data.Dataset.from_tensor_slices(client_tensor_slices) \\\n",
    "            .shuffle(buffer_size=shuffle_buffer_size, seed=seed).batch(batch_size) \\\n",
    "            .prefetch(tf.data.AUTOTUNE).take(num_steps_until_rtc_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794b1a0",
   "metadata": {},
   "source": [
    "### Create Federated Learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d770c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_federated_data(client_slices_train, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed=None):\n",
    "    \n",
    "    federated_dataset = [ \n",
    "        create_tf_dataset_for_client(client_tensor_slices, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed)\n",
    "        for client, client_tensor_slices in client_slices_train.items()\n",
    "    ]\n",
    "    \n",
    "    return federated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a0455",
   "metadata": {},
   "source": [
    "# Miscallenious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d696f",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6aa1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def variance(cnn_list, cnn_sync):\n",
    "    \n",
    "    squared_distances = [\n",
    "        tf.reduce_sum(tf.square(cnn.trainable_vars_as_vector() - cnn_sync.trainable_vars_as_vector())) \n",
    "        for cnn in cnn_list\n",
    "    ]\n",
    "    \n",
    "    var = tf.reduce_mean(squared_distances)\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbfbd3",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Net (CNN) - Medium Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3a252",
   "metadata": {},
   "source": [
    "A simple Convolutional Neural Network with a single convolutional layer, followed by a max-pooling layer, and two dense layers for classification. Designed for 28x28 grayscale images. It has 692,352 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac88dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.reshape = layers.Reshape((28, 28, 1))\n",
    "        self.conv1 = layers.Conv2D(32, 3, activation='relu')\n",
    "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dense2 = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "        \n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.reshape(inputs)  # Add a channel dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, batch):\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            y_batch_pred = self(x_batch, training=True)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(\n",
    "                y_true=y_batch,\n",
    "                y_pred=y_batch_pred,\n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients to the model's trainable variables (update weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        #self.compiled_metrics.update_state(y_batch, y_batch_pred)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, dataset):\n",
    "\n",
    "        for batch in dataset:\n",
    "            self.step(batch)\n",
    "            \n",
    "    \n",
    "    def set_trainable_variables(self, trainable_vars):\n",
    "        \"\"\" Given `trainable_vars` set our `self.trainable_vars` \"\"\"\n",
    "        for model_var, var in zip(self.trainable_variables, trainable_vars):\n",
    "            model_var.assign(var)\n",
    "\n",
    "            \n",
    "    def trainable_vars_as_vector(self):\n",
    "        return tf.concat([tf.reshape(var, [-1]) for var in self.trainable_variables], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead5a6c",
   "metadata": {},
   "source": [
    "### Helper function to compile and return the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ddd2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_and_built_cnn():\n",
    "    cnn = CNN()\n",
    "    \n",
    "    cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # we have softmax\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')]\n",
    "    )\n",
    "    \n",
    "    cnn.build((None, 28, 28))  # EMNIST dataset (None is used for batch size, as it varies)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f99e0",
   "metadata": {},
   "source": [
    "# Advanced Convolutional Neural Net (CNN) - Large Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259d325",
   "metadata": {},
   "source": [
    "A more complex Convolutional Neural Network with three sets of two convolutional layers, each followed by a max-pooling layer, and two dense layers with dropout for classification. Designed for 28x28 grayscale images. It has 2,592,202 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5705b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AdvancedCNN, self).__init__()\n",
    "        \n",
    "        self.reshape = layers.Reshape((28, 28, 1))\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv4 = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.conv5 = layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv6 = layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(512, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.dense2 = layers.Dense(512, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "        self.dense3 = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.reshape(inputs)  # Add a channel dimension\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.max_pool2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.max_pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, batch):\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            y_batch_pred = self(x_batch, training=True)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(\n",
    "                y_true=y_batch,\n",
    "                y_pred=y_batch_pred,\n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients to the model's trainable variables (update weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        #self.compiled_metrics.update_state(y_batch, y_batch_pred)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, dataset):\n",
    "\n",
    "        for batch in dataset:\n",
    "            self.step(batch)\n",
    "            \n",
    "    \n",
    "    def set_trainable_variables(self, trainable_vars):\n",
    "        \"\"\" Given `trainable_vars` set our `self.trainable_vars` \"\"\"\n",
    "        for model_var, var in zip(self.trainable_variables, trainable_vars):\n",
    "            model_var.assign(var)\n",
    "\n",
    "            \n",
    "    def trainable_vars_as_vector(self):\n",
    "        return tf.concat([tf.reshape(var, [-1]) for var in self.trainable_variables], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1289f0",
   "metadata": {},
   "source": [
    "### Helper function to compile and return the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a705fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_and_built_advanced_cnn():\n",
    "    advanced_cnn = AdvancedCNN()\n",
    "    \n",
    "    advanced_cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # we have softmax\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')]\n",
    "    )\n",
    "    \n",
    "    advanced_cnn.build((None, 28, 28))  # EMNIST dataset (None is used for batch size, as it varies)\n",
    "    \n",
    "    return advanced_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c454f52",
   "metadata": {},
   "source": [
    "### Average NN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7cb196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_client_weights(client_models):\n",
    "    # client_weights[0] the trainable variables of Client 0 (a list of tf.Variable)\n",
    "    client_weights = [model.trainable_variables for model in client_models]\n",
    "\n",
    "    # concise solution. per layer. `layer_weight_tensors` corresponds to a list of tensors of a layer\n",
    "    avg_weights = [\n",
    "        tf.reduce_mean(layer_weight_tensors, axis=0)\n",
    "        for layer_weight_tensors in zip(*client_weights)\n",
    "    ]\n",
    "\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def022c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cc30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b5348d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a0df4",
   "metadata": {},
   "source": [
    "## 1️⃣ Naive FDA\n",
    "\n",
    "In the naive approach, we eliminate the update vector from the local state (i.e. recuce the dimension to 0). Define local state as\n",
    "\n",
    "$$ S_i(t) = \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\in \\mathbb{R}$$ \n",
    "\n",
    "and the identity function\n",
    "\n",
    "$$ F(v) = v $$\n",
    "\n",
    "It is trivial that $ F(S(t)) \\leq \\Theta $ implies the RTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cf704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_naive(last_sync_cnn, client_cnn, client_dataset):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - last_sync_cnn.trainable_vars_as_vector()\n",
    "    \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    return Delta_i_euc_norm_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2013ea",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4780782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_naive(S):\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c6b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_naive(server_cnn, client_cnns, federated_dataset,\n",
    "                                   num_epochs, theta, epoch_fda_steps):\n",
    "    \n",
    "    print(\"retracing naive\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_naive(S) <= theta:\n",
    "            S_i_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared = steps_naive(server_cnn, client_cnn, client_dataset)\n",
    "                S_i_clients.append(Delta_i_euc_norm_squared)\n",
    "                \n",
    "            S = tf.reduce_mean(S_i_clients)\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        #tf.print(\"\\n sync : \", output_stream=sys.stdout)\n",
    "        Delta_i_clients = [\n",
    "            tf.subtract(client_cnn.trainable_vars_as_vector(), server_cnn.trainable_vars_as_vector()) \n",
    "            for client_cnn in client_cnns\n",
    "        ] #test\n",
    "        testing_approx_0 = tf.reduce_sum(tf.square(tf.reduce_mean(Delta_i_clients, axis=0))) #test\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        #loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "        #tf.print(\"acc (after) : \", acc, output_stream=sys.stdout)\n",
    "        tf.print(\"Naive Epoch count: \", epoch_count, \" Total fda steps: \", total_fda_steps, output_stream=sys.stdout)\n",
    "        #tf.print(\"Naive Epoch count: \", epoch_count, output_stream=sys.stdout)\n",
    "        tf.print(\"Est var: \", S, \" Assumed 0: \", testing_approx_0, \" Actual var: \", variance(client_cnns, server_cnn), output_stream=sys.stdout)\n",
    "        tf.print(\"\\n\", output_stream=sys.stdout)\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6d3f1",
   "metadata": {},
   "source": [
    "## 2️⃣ Linear FDA\n",
    "\n",
    "In the linear case, we reduce the update vector to a scalar, $ \\xi \\Delta_t^{(i)} \\in \\mathbb{R}$, where $ \\xi $ is any unit vector.\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\xi \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^2 $$\n",
    "\n",
    "Also, define \n",
    "\n",
    "$$ F(v, x) = v - x^2 $$\n",
    "\n",
    "The RTC is equivalent to condition \n",
    "\n",
    "$$ F(S(t)) \\leq \\Theta $$\n",
    "\n",
    "A random choice of $ \\xi $ is likely to perform poorly (terminate round prematurely), as it wil likely be close to orthogonal to $ \\overline{\\Delta_t} $. A good choice would be a vector $ \\xi $ correlated to $ \\overline{\\Delta_t} $. A heuristic choice is to take $ \\overline{\\Delta_{t_0}} $ (after scaling it to norm 1), i.e., the update vector right before the current round started. All nodes can estimate this without communication, as $ \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} $, the difference of the last two models pushed by the Server. Hence, \n",
    "\n",
    "$$ \\xi = \\frac{\\overline{w_{t_0}} - \\overline{w_{t_{-1}}}}{\\lVert \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} \\rVert_2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbfb12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ksi_unit_fn(w_t0, w_tminus1):\n",
    "    \n",
    "    if tf.reduce_all(tf.equal(w_t0, w_tminus1)):\n",
    "        # if equal then ksi becomes a random vector (will only happen in round 1)\n",
    "        ksi = tf.random.normal(shape=w_t0.shape)\n",
    "    else:\n",
    "        ksi = w_t0 - w_tminus1\n",
    "\n",
    "    # Normalize and return\n",
    "    return tf.divide(ksi, tf.norm(ksi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0ad6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_linear(cnn_tminus, cnn_t0, client_cnn, client_dataset):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - cnn_t0.trainable_vars_as_vector()\n",
    "    \n",
    "    #||D(t)_i||^2 , shape = (1,) \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    # heuristic unit vector ksi\n",
    "    ksi = ksi_unit_fn(cnn_t0.trainable_vars_as_vector(), cnn_tminus.trainable_vars_as_vector())\n",
    "    \n",
    "    # ksi * Delta_i (* is dot) , shape = ()\n",
    "    ksi_Delta_i = tf.reduce_sum(tf.multiply(ksi, Delta_i))\n",
    "    \n",
    "    return Delta_i_euc_norm_squared, ksi_Delta_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c734b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d8dd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_linear(S_1, S_2):\n",
    "    return S_1 - S_2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e87dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_linear(previous_server_cnn, server_cnn, client_cnns, federated_dataset,\n",
    "                                   num_epochs, theta, epoch_fda_steps):\n",
    "    \n",
    "    print(\"retracing naive\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    S_2 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_linear(S_1, S_2) <= theta:\n",
    "            euc_norm_squared_clients = []\n",
    "            ksi_delta_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared, ksi_Delta_i = steps_linear(\n",
    "                    previous_server_cnn, server_cnn, client_cnn, client_dataset\n",
    "                )\n",
    "                \n",
    "                euc_norm_squared_clients.append(Delta_i_euc_norm_squared)\n",
    "                ksi_delta_clients.append(ksi_Delta_i)\n",
    "            \n",
    "            S_1 = tf.reduce_mean(euc_norm_squared_clients)\n",
    "            S_2 = tf.reduce_mean(ksi_delta_clients)\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        #tf.print(\"\\n sync : \", output_stream=sys.stdout)\n",
    "        Delta_i_clients = [\n",
    "            tf.subtract(client_cnn.trainable_vars_as_vector(), server_cnn.trainable_vars_as_vector()) \n",
    "            for client_cnn in client_cnns\n",
    "        ] #test\n",
    "        testing_approx_0 = tf.reduce_sum(tf.square(tf.reduce_mean(Delta_i_clients, axis=0))) #test\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # last server model (previous sync)\n",
    "        previous_server_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        #loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "        #tf.print(\"acc (after) : \", acc, output_stream=sys.stdout)\n",
    "        tf.print(\"Naive Epoch count: \", epoch_count, \" Total fda steps: \", total_fda_steps, output_stream=sys.stdout)\n",
    "        #tf.print(\"Naive Epoch count: \", epoch_count, output_stream=sys.stdout)\n",
    "        tf.print(\"Est var: \", F_linear(S_1, S_2), \" Assumed 0: \", testing_approx_0, \" Actual var: \", variance(client_cnns, server_cnn), output_stream=sys.stdout)\n",
    "        tf.print(\"\\n\", output_stream=sys.stdout)\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "        S_2 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b28f1a",
   "metadata": {},
   "source": [
    "## 3️⃣ Sketch FDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a6434",
   "metadata": {},
   "source": [
    "An optimal estimator for $ \\lVert \\overline{\\Delta_t} \\rVert_2^2  $ can be obtained by employing AMS sketches. An AMS sketch of a vector $ v \\in \\mathbb{R}^M $ is a $ d \\times m $ real matrix\n",
    "\n",
    "$$ \\Xi = \\text{sk}(v) = \\begin{bmatrix}\n",
    "           \\Xi_1 \\\\\n",
    "           \\Xi_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           \\Xi_d \n",
    "         \\end{bmatrix} $$\n",
    "         \n",
    "where $ d \\cdot m \\ll M$. Operator sk($ \\cdot $) is linear, i.e., let $a, b \\in \\mathbb{R}$ and $v_1, v_2 \\in \\mathbb{R}^N$ then \n",
    "\n",
    "$$ \\text{sk}(a v_1 + b v_2) = a \\; \\text{sk}(v_1) + b \\; \\text{sk}(v_2)  $$\n",
    "\n",
    "Also, sk($ v $) can be computed in $ \\mathcal{O}(dN) $ steps.\n",
    "\n",
    "The interesting property of AMS sketches is that the function \n",
    "\n",
    "$$ M(sk(\\textbf{v})) = \\underset{i=1,...,d}{\\text{median}} \\; \\lVert \\boldsymbol{\\Xi}_i \\rVert_2^2  $$ \n",
    "\n",
    "is an excellent estimator of the Euclidean norm of **v** (within relative $\\epsilon$-error):\n",
    "\n",
    "$$ M(sk(\\textbf{v})) \\; \\in (1 \\pm \\epsilon) \\lVert \\textbf{v} \\rVert_2^2 \\; \\; \\text{with probability at least} \\; (1-\\delta) $$\n",
    "\n",
    "where $m = \\mathcal{O}(\\frac{1}{\\epsilon^2})$ and $d = \\mathcal{O}(\\log \\frac{1}{\\delta})$\n",
    "            \n",
    "Moreover, let $\\boldsymbol{\\Xi} \\in \\mathbb{R}^{d \\times m}$ and $ k \\in \\mathbb{R}$. It can be proven that\n",
    "\n",
    "$$ M( \\frac{1}{k} \\boldsymbol{\\Xi}) = \\frac{1}{k^2} M(\\boldsymbol{\\Xi}) $$\n",
    "\n",
    "Let's investigate a little further on how this helps us. The $i$-th client computes $ sk(\\Delta_t^{(i)}) $ and sends it to the server. Notice\n",
    "\n",
    "$$ M\\big(sk(\\Delta_t^{(1)}) + sk(\\Delta_t^{(2)}) + ... + sk(\\Delta_t^{(k)}) \\big) = M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big)$$\n",
    "\n",
    "Remember that\n",
    "\n",
    "$$ \\overline{\\boldsymbol{\\Delta}}_t = \\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} $$\n",
    "\n",
    "Then\n",
    "            \n",
    "$$ M\\Big( \\text{sk}\\big( \\overline{\\boldsymbol{\\Delta}}_t \\big) \\Big) = M\\Big( \\text{sk}\\big( \\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) = \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) $$\n",
    "\n",
    "\n",
    "Which means that \n",
    "\n",
    "$$ \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) \\in (1 \\pm \\epsilon) \\lVert \\overline{\\boldsymbol{\\Delta}}_t \\rVert_2^2 \\; \\; \\text{w.p. at least} \\; (1-\\delta) $$\n",
    "\n",
    "In the monitoring process it is essential that we do not overestimate $ \\lVert \\overline{\\Delta_t} \\rVert_2^2 $ because we would then underestimate the variance which would potentially result in actual varience exceeding $ \\Theta$ without us noticing it. With this in mind,\n",
    "\n",
    "$$ \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big) \\leq (1+\\epsilon) \\lVert \\overline{\\Delta_t} \\rVert_2^2 \\quad \\text{with probability at least} \\; (1-\\delta)$$\n",
    "\n",
    "Which means\n",
    "\n",
    "$$ \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big) \\leq \\lVert \\overline{\\Delta_t} \\rVert_2^2 \\quad \\text{with probability at least} \\; (1-\\delta)$$\n",
    "\n",
    "Hence, the Server's estimation of $ \\lVert \\overline{\\Delta_t} \\rVert_2^2 $ is\n",
    "\n",
    "$$ \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M\\Big( sk(\\Delta_t^{(1)}) + sk(\\Delta_t^{(2)}) + ... + sk(\\Delta_t^{(k)}) \\big) \\Big) $$\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           sk(\\Delta_t^{(i)})\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^{1+d \\times m} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\Xi\n",
    "         \\end{bmatrix}) = v - \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M(\\Xi) \\quad \\text{where} \\quad \\Xi = \\sum_{i=1}^{k} sk(\\Delta_t^{(i)}) $$\n",
    "\n",
    "It follows that $ F(S(t)) \\leq \\Theta $ implies that the variance is less or equal to $ \\Theta $ with probability at least $ 1-\\delta $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db738526",
   "metadata": {},
   "source": [
    "## AMS sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e3bfa",
   "metadata": {},
   "source": [
    "We use `ExtensionType` which is the way to go in order to avoid unecessary graph retracing when passing around `AmsSketch` type 'objects'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f6b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.experimental import ExtensionType\n",
    "\n",
    "class AmsSketch(ExtensionType):\n",
    "    depth: int\n",
    "    width: int\n",
    "    F: tf.Tensor\n",
    "        \n",
    "        \n",
    "    def __init__(self, depth=7, width=1500):\n",
    "        self.depth = depth\n",
    "        self.width = width\n",
    "        self.F = tf.random.uniform(shape=(6, depth), minval=0, maxval=(1 << 31) - 1, dtype=tf.int32)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def hash31(self, x, a, b):\n",
    "\n",
    "        r = a * x + b\n",
    "        fold = tf.bitwise.bitwise_xor(tf.bitwise.right_shift(r, 31), r)\n",
    "        return tf.bitwise.bitwise_and(fold, 2147483647)\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def fourwise(self, x):\n",
    "\n",
    "        result = 2 * (tf.bitwise.right_shift(tf.bitwise.bitwise_and(self.hash31(self.hash31(self.hash31(x, self.F[2], self.F[3]), x, self.F[4]), x, self.F[5]), 32768), 15)) - 1\n",
    "        return result\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector(self, v):\n",
    "        print(\"retrace\")\n",
    "        \n",
    "        \"\"\" This is the most memory friendly/quick method for this that I can think of. `v` shape=(d,) \"\"\"\n",
    "\n",
    "        sketch = tf.zeros(shape=(self.depth, self.width), dtype=tf.float32)\n",
    "\n",
    "        for i in tf.range(tf.shape(v)[0], dtype=tf.int32):\n",
    "            pos = self.hash31(i, self.F[0], self.F[1]) % self.width\n",
    "            delta = tf.cast(self.fourwise(i), dtype=tf.float32) * v[i]\n",
    "            indices_to_update = tf.stack([tf.range(self.depth, dtype=tf.int32), pos], axis=1)\n",
    "            sketch = tf.tensor_scatter_nd_add(sketch, indices_to_update, delta)\n",
    "\n",
    "        return sketch\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector2(self, v):\n",
    "        \"\"\" ... An attempt to parallelize `sketch_for_vector`.\"\"\"\n",
    "        \n",
    "        _batch_size = 256\n",
    "        \n",
    "        sketch = tf.zeros(shape=(self.depth, self.width), dtype=tf.float32)\n",
    "\n",
    "        def update_sketch(i_v):\n",
    "            i, v_i = i_v\n",
    "            pos = self.hash31(i, self.F[0], self.F[1]) % self.width\n",
    "            delta = tf.cast(self.fourwise(i), dtype=tf.float32) * v_i\n",
    "            indices_to_update = tf.stack([tf.range(self.depth, dtype=tf.int32), pos], axis=1)\n",
    "            return tf.tensor_scatter_nd_add(tf.zeros(shape=(self.depth, self.width), dtype=tf.float32), indices_to_update, delta)\n",
    "\n",
    "        v_indices = tf.range(tf.shape(v)[0], dtype=tf.int32)\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((v_indices, v)).batch(_batch_size)\\\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        @tf.function\n",
    "        def process_batch(sketch, batch):\n",
    "            batch_indices, batch_values = batch\n",
    "            updates = tf.map_fn(update_sketch, (batch_indices, batch_values), fn_output_signature=tf.TensorSpec(shape=(self.depth, self.width), dtype=tf.float32))\n",
    "            return sketch + tf.reduce_sum(updates, axis=0)\n",
    "\n",
    "        sketch = dataset.reduce(sketch, process_batch)\n",
    "\n",
    "        return sketch\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector3(self, v):\n",
    "        \"\"\" Infeasible to run. Computes N sketches and sums them (N ~ 2,700,000 !) \"\"\"\n",
    "\n",
    "        def update_sketch(i_v):\n",
    "            i, v_i = i_v\n",
    "            pos = self.hash31(i, self.F[0], self.F[1]) % self.width\n",
    "            delta = tf.cast(self.fourwise(i), dtype=tf.float32) * v_i\n",
    "            indices_to_update = tf.stack([tf.range(self.depth, dtype=tf.int32), pos], axis=1)\n",
    "            return tf.tensor_scatter_nd_add(tf.zeros(shape=(self.depth, self.width), dtype=tf.float32), indices_to_update, delta)\n",
    "\n",
    "        v_indices = tf.range(tf.shape(v)[0], dtype=tf.int32)\n",
    "        updates = tf.map_fn(update_sketch, (v_indices, v), fn_output_signature=tf.TensorSpec(shape=(self.depth, self.width), dtype=tf.float32))\n",
    "        sketch = tf.reduce_sum(updates, axis=0)\n",
    "\n",
    "        return sketch\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def estimate_euc_norm_squared(sketch):\n",
    "\n",
    "        @tf.function\n",
    "        def _median(v):\n",
    "            \"\"\" Median of tensor `v` with shape=(n,). Note: Suboptimal O(nlogn) but it's ok bcz n = `depth`\"\"\"\n",
    "            length = tf.shape(v)[0]\n",
    "            sorted_v = tf.sort(v)\n",
    "            middle = length // 2\n",
    "\n",
    "            return tf.cond(\n",
    "                tf.equal(length % 2, 0),\n",
    "                lambda: (sorted_v[middle - 1] + sorted_v[middle]) / 2.0,\n",
    "                lambda: sorted_v[middle]\n",
    "            )\n",
    "\n",
    "        return _median(tf.reduce_sum(tf.square(sketch), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36cef89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ams_sketch = AmsSketch(\n",
    "    depth=7,\n",
    "    width=1700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb70c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tf.random.uniform(shape=(100000,), minval=0, maxval=2, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d4cd7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrace\n",
      "0.614248514175415\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sk = ams_sketch.sketch_for_vector(vec)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6b426e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f19f822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0687813758850098\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#%%timeit\n",
    "sk2 = ams_sketch.sketch_for_vector2(vec)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a925743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 1700), dtype=float32, numpy=\n",
       "array([[ 1.0736248 ,  2.3649616 ,  0.59587145, ..., -1.7317469 ,\n",
       "        -0.04054713,  1.5100734 ],\n",
       "       [-1.0901418 ,  3.4207542 , -0.4354477 , ..., -3.4073126 ,\n",
       "        -3.9412038 ,  0.40085244],\n",
       "       [ 1.8654134 ,  1.8456681 , -1.3245697 , ..., -0.4380231 ,\n",
       "        -4.5321894 ,  2.1376078 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.12635875, -5.43426   , ..., -5.236559  ,\n",
       "         4.178916  ,  0.        ],\n",
       "       [-0.55259275, -4.310498  , -4.65131   , ...,  4.6583624 ,\n",
       "        -4.0378175 , -1.5525312 ],\n",
       "       [ 0.91747046,  5.2422004 ,  3.6640456 , ..., -3.9125469 ,\n",
       "        -0.8350315 , -2.0014143 ]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27cd10e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.100456953048706\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sk3 = ams_sketch.sketch_for_vector3(vec)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7610c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sk3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb220a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sk4 = ams_sketch.sketch_for_vector4(vec)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efe7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sk4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66bf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a74388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b40835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f520f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710a62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca81deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48379f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0c953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475081e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d94680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ff94042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(375, shape=(), dtype=int32)\n",
      "retracing naive\n",
      "Naive Epoch count:  0  Total fda steps:  1\n",
      "Est var:  3113.79541  Assumed 0:  2491.76904  Actual var:  1661.4668\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  5\n",
      "Est var:  6.00482798  Assumed 0:  1.2308228  Actual var:  4.77414513\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  14\n",
      "Est var:  5.32721424  Assumed 0:  1.2800473  Actual var:  5.03975058\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  31\n",
      "Est var:  5.06042242  Assumed 0:  1.29175544  Actual var:  4.1212163\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  67\n",
      "Est var:  5.04389238  Assumed 0:  1.4845587  Actual var:  3.95188904\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  114\n",
      "Est var:  5.0469141  Assumed 0:  1.13830686  Actual var:  3.9433434\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  138\n",
      "Est var:  5.45182848  Assumed 0:  1.4219414  Actual var:  4.15044641\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  148\n",
      "Est var:  5.74335909  Assumed 0:  3.70497155  Actual var:  3.47907448\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  154\n",
      "Est var:  5.42306471  Assumed 0:  2.26138663  Actual var:  3.85537028\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  161\n",
      "Est var:  5.04876947  Assumed 0:  2.43799782  Actual var:  3.86034966\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  170\n",
      "Est var:  5.67425919  Assumed 0:  2.74087191  Actual var:  4.41506577\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  180\n",
      "Est var:  5.26234198  Assumed 0:  2.77533865  Actual var:  3.58732677\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  190\n",
      "Est var:  5.79960203  Assumed 0:  3.44668913  Actual var:  3.81441736\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  198\n",
      "Est var:  5.46286488  Assumed 0:  3.40882373  Actual var:  3.4896915\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  205\n",
      "Est var:  5.11358738  Assumed 0:  2.42703223  Actual var:  3.74323511\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  213\n",
      "Est var:  6.25290251  Assumed 0:  2.81809735  Actual var:  4.57256842\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  221\n",
      "Est var:  5.96497107  Assumed 0:  2.93041563  Actual var:  4.48766851\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  228\n",
      "Est var:  5.12777758  Assumed 0:  2.21175408  Actual var:  3.81804204\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  236\n",
      "Est var:  5.1793232  Assumed 0:  2.26742554  Actual var:  4.11182594\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  245\n",
      "Est var:  5.96109581  Assumed 0:  2.40706444  Actual var:  4.58299398\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  253\n",
      "Est var:  5.57899761  Assumed 0:  2.00458527  Actual var:  4.29521942\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  262\n",
      "Est var:  5.58366919  Assumed 0:  2.04955935  Actual var:  4.30877399\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  271\n",
      "Est var:  5.52996492  Assumed 0:  1.87342072  Actual var:  4.26419878\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  281\n",
      "Est var:  5.64291286  Assumed 0:  2.01692295  Actual var:  4.43585348\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  290\n",
      "Est var:  5.15186644  Assumed 0:  1.72905517  Actual var:  4.04565144\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  299\n",
      "Est var:  5.00914478  Assumed 0:  1.68355823  Actual var:  3.84817886\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  308\n",
      "Est var:  5.23048639  Assumed 0:  1.78984368  Actual var:  3.99283981\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  317\n",
      "Est var:  5.04426289  Assumed 0:  1.60176909  Actual var:  3.9025178\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  327\n",
      "Est var:  5.65705585  Assumed 0:  1.66948986  Actual var:  4.44437313\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  338\n",
      "Est var:  5.31995821  Assumed 0:  1.55169296  Actual var:  4.20103168\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  349\n",
      "Est var:  5.11527252  Assumed 0:  1.36087382  Actual var:  4.06563044\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  362\n",
      "Est var:  5.59565973  Assumed 0:  1.52722859  Actual var:  4.42407036\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  374\n",
      "Est var:  5.60770798  Assumed 0:  1.48479676  Actual var:  4.37838268\n",
      "\n",
      "\n",
      "Naive Epoch count:  1  Total fda steps:  375\n",
      "Est var:  0.0698529109  Assumed 0:  0.019813139  Actual var:  0.0564668402\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" One test for Naive,Linear,Sketch. Returns metrics \"\"\"\n",
    "NUM_EPOCHS = 1 # out\n",
    "THETA = 5 # out\n",
    "BATCH_SIZE = 32 # out\n",
    "NUM_STEPS_UNTIL_RTC_CHECK = 1 # out\n",
    "NUM_CLIENTS = 5 # out\n",
    "\n",
    "seed = 7\n",
    "\n",
    "\n",
    "client_slices_train = create_data_for_clients(NUM_CLIENTS) # out\n",
    "\n",
    "\n",
    "federated_dataset = create_federated_data(\n",
    "    client_slices_train=client_slices_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer_size=n_train,\n",
    "    num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = tf.constant(NUM_EPOCHS, shape=(), dtype=tf.int32)\n",
    "theta = tf.constant(THETA, shape=(), dtype=tf.float32)\n",
    "\n",
    "# for sketch\n",
    "#epsilon = tf.constant(EPSILON, shape=(), dtype=tf.float32) # new\n",
    "num_clients = tf.constant(float(NUM_CLIENTS), shape=(), dtype=tf.float32) # new\n",
    "\n",
    "\n",
    "epoch_client_batches = (n_train / BATCH_SIZE) / NUM_CLIENTS\n",
    "epoch_max_fda_steps = epoch_client_batches / NUM_STEPS_UNTIL_RTC_CHECK\n",
    "epoch_max_fda_steps = tf.constant(int(epoch_max_fda_steps), shape=(), dtype=tf.int32)\n",
    "\n",
    "basic_test_metrics = []\n",
    "\n",
    "\"\"\" --------------- Naive ----------------------------------\"\"\"\n",
    "\"\"\"\n",
    "# 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "\n",
    "federated_dataset = create_federated_data(\n",
    "    client_slices_train=client_slices_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer_size=int(n_train/NUM_CLIENTS),\n",
    "    num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# 2. Models init\n",
    "\n",
    "client_cnns = [\n",
    "    get_compiled_and_built_advanced_cnn() for _ in range(NUM_CLIENTS)\n",
    "]\n",
    "\n",
    "server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "\n",
    "# 3. Run \n",
    "\n",
    "print(epoch_max_fda_steps)\n",
    "\n",
    "total_rounds, total_fda_steps = run_federated_simulation_naive(\n",
    "    server_cnn, \n",
    "    client_cnns, \n",
    "    federated_dataset, \n",
    "    num_epochs, \n",
    "    theta,\n",
    "    epoch_max_fda_steps\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" --------------- Naive ----------------------------------\"\"\"\n",
    "\n",
    "# 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "\n",
    "federated_dataset = create_federated_data(\n",
    "    client_slices_train=client_slices_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_buffer_size=int(n_train/NUM_CLIENTS),\n",
    "    num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# 2. Models init\n",
    "\n",
    "client_cnns = [\n",
    "    get_compiled_and_built_advanced_cnn() for _ in range(NUM_CLIENTS)\n",
    "]\n",
    "\n",
    "previous_server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "\n",
    "server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "\n",
    "# 3. Run \n",
    "\n",
    "print(epoch_max_fda_steps)\n",
    "\n",
    "total_rounds, total_fda_steps = run_federated_simulation_linear(\n",
    "    previous_server_cnn,\n",
    "    server_cnn, \n",
    "    client_cnns, \n",
    "    federated_dataset, \n",
    "    num_epochs, \n",
    "    theta,\n",
    "    epoch_max_fda_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cd9e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 13s 306ms/step - loss: 0.1437 - test_accuracy: 0.9539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14370574057102203, 0.9538999795913696]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_cnn.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674cec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tff-py39] *",
   "language": "python",
   "name": "conda-env-tff-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
