{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2759993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74531322",
   "metadata": {},
   "source": [
    "## Import EMNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a550108",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc37c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c8e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_BATCH_INPUT = (None, 28, 28) # EMNIST dataset (None is used for batch size, as it varies)\n",
    "CNN_INPUT_RESHAPE = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a83a61",
   "metadata": {},
   "source": [
    "## Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb02a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "del X_train\n",
    "\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.int32)\n",
    "del y_train\n",
    "\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "del X_test\n",
    "\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.int32)\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ab8a7",
   "metadata": {},
   "source": [
    "## Prepare data for Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e112b",
   "metadata": {},
   "source": [
    "### Create centralized testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fad165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_test = (X_test_tensor, y_test_tensor)\n",
    "\n",
    "def create_tf_dataset_for_testing(batch_size):\n",
    "    return tf.data.Dataset.from_tensor_slices(slices_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84aa24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = create_tf_dataset_for_testing(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24a95c",
   "metadata": {},
   "source": [
    "### Slice the Tensors for each Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43734f7",
   "metadata": {},
   "source": [
    "We will cut the training data, i.e., (`X_train_tensor`, `y_train_tensor`) to equal parts, each part corresponding to one Client. We want to give the result back as a dictionary with key `client_id` and value the training tensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b27a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_clients(num_clients):\n",
    "    \n",
    "    client_slices_train = {}\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        # Compute the indices for this client's slice\n",
    "        start_idx = int(i * n_train / num_clients)\n",
    "        end_idx = int((i + 1) * n_train / num_clients)\n",
    "\n",
    "        # Get the slice for this client\n",
    "        X_client_train = X_train_tensor[start_idx:end_idx]\n",
    "        y_client_train = y_train_tensor[start_idx:end_idx]\n",
    "        \n",
    "        # Combine the slices into a single dataset\n",
    "        client_slices_train[f'client_{i}'] = (X_client_train, y_client_train)\n",
    "    \n",
    "    return client_slices_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88734523",
   "metadata": {},
   "source": [
    "### Create TF friendly data for each Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c58dbae",
   "metadata": {},
   "source": [
    "Given a Tensor slice (i.e. value of `client_slices_train[\"client_id\"]` we convert it to highly optimized `tf.data.Dataset` to prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794535c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset_for_client(client_tensor_slices, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed):\n",
    "    \n",
    "        return tf.data.Dataset.from_tensor_slices(client_tensor_slices) \\\n",
    "            .shuffle(buffer_size=shuffle_buffer_size, seed=seed).batch(batch_size) \\\n",
    "            .prefetch(tf.data.AUTOTUNE).take(num_steps_until_rtc_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794b1a0",
   "metadata": {},
   "source": [
    "### Create Federated Learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d770c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_federated_data(client_slices_train, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed=None):\n",
    "    \n",
    "    federated_dataset = [ \n",
    "        create_tf_dataset_for_client(client_tensor_slices, batch_size, shuffle_buffer_size, num_steps_until_rtc_check, seed)\n",
    "        for client, client_tensor_slices in client_slices_train.items()\n",
    "    ]\n",
    "    \n",
    "    return federated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a0455",
   "metadata": {},
   "source": [
    "# Miscallenious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d696f",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6aa1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def variance(cnn_list, cnn_sync):\n",
    "    \n",
    "    squared_distances = [\n",
    "        tf.reduce_sum(tf.square(cnn.trainable_vars_as_vector() - cnn_sync.trainable_vars_as_vector())) \n",
    "        for cnn in cnn_list\n",
    "    ]\n",
    "    \n",
    "    var = tf.reduce_mean(squared_distances)\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88347123",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_dict(fda_name, n_train, dataset_name, input_pixels, seed, epochs, num_clients, \n",
    "                        batch_size, steps_in_one_fda_step, theta, total_fda_steps, num_weights,\n",
    "                        total_rounds, final_accuracy, sketch_width=None, sketch_depth=None):\n",
    "    metrics = {\n",
    "            \"fda_name\" : fda_name,\n",
    "            \"theta\" : theta,\n",
    "            \"dataset_name\" : dataset_name, # new\n",
    "            \"input_pixels\" : input_pixels, # new\n",
    "            \"n_train\" : n_train, # new\n",
    "            \"num_weights\" : num_weights, # new\n",
    "            \"seed\" : seed,\n",
    "            \"epochs\" : epochs,\n",
    "            \"num_clients\" : num_clients,\n",
    "            \"batch_size\" : batch_size,\n",
    "            \"steps_in_one_fda_step\" : steps_in_one_fda_step,\n",
    "            \"sketch_width\" : sketch_width,\n",
    "            \"sketch_depth\" : sketch_depth\n",
    "        }\n",
    "    \n",
    "    # one batch bytes\n",
    "    metrics[\"one_sample_bytes\"] = 4 * (metrics[\"input_pixels\"] + 1)  # 4 bytes float32\n",
    "    \n",
    "    # training dataset size\n",
    "    metrics[\"training_dataset_bytes\"] = metrics[\"one_sample_bytes\"] * metrics[\"n_train\"]\n",
    "    \n",
    "    # model bytes\n",
    "    metrics[\"model_bytes\"] = metrics[\"num_weights\"] * 4\n",
    "    \n",
    "    \n",
    "    # local state bytes (i.e. S_i), for one client\n",
    "    if fda_name == \"naive\":\n",
    "        metrics[\"local_state_bytes\"] = 4\n",
    "    elif fda_name == \"linear\":\n",
    "        metrics[\"local_state_bytes\"] = 8\n",
    "    else:\n",
    "        metrics[\"local_state_bytes\"] = sketch_width * sketch_depth * 4 + 4\n",
    "        \n",
    "    # accuracy (already computed in parameter)\n",
    "    metrics[\"final_accuracy\"] = final_accuracy\n",
    "    \n",
    "    # total fda steps from algo\n",
    "    metrics[\"total_fda_steps\"] = total_fda_steps\n",
    "    \n",
    "    # total steps (a single fda step might have many normal SGD steps, batch steps)\n",
    "    metrics[\"total_steps\"] = metrics[\"total_fda_steps\"] * metrics[\"steps_in_one_fda_step\"]\n",
    "    \n",
    "    # total rounds in algo. Reason why we differentiate from the hardcoded NUM_ROUNDS\n",
    "    # is because we might run less rounds in the future (i.e. stop on 10^7 samples idk)\n",
    "    metrics[\"total_rounds\"] = total_rounds\n",
    "    \n",
    "    # bytes exchanged for synchronizing weights (x2 because server sends back)\n",
    "    metrics[\"model_bytes_exchanged\"] = metrics[\"total_rounds\"] * metrics[\"model_bytes\"] \\\n",
    "        * metrics[\"num_clients\"] * 2\n",
    "    \n",
    "    # bytes exchanged for monitoring the variance (communication)\n",
    "    metrics[\"monitoring_bytes_exchanged\"] = metrics[\"local_state_bytes\"] * metrics[\"total_fda_steps\"] \\\n",
    "        * metrics[\"num_clients\"]\n",
    "    \n",
    "    # total communication bytes (for both monitoring and model synchronization)\n",
    "    metrics[\"total_communication_bytes\"] = metrics[\"model_bytes_exchanged\"] + metrics[\"monitoring_bytes_exchanged\"]\n",
    "    \n",
    "    # total seen dataset bytes (across all learning, i.e., all clients)\n",
    "    metrics[\"trained_in_bytes\"] = metrics[\"batch_size\"] * metrics[\"one_sample_bytes\"] \\\n",
    "        * metrics[\"total_steps\"] * metrics[\"num_clients\"]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e079822",
   "metadata": {},
   "source": [
    "## Neural Net weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9b44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weights(model):\n",
    "    total_params = 0\n",
    "    for layer in model.layers:\n",
    "        total_params += np.sum([np.prod(weight.shape) for weight in layer.trainable_weights])\n",
    "    return int(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbfbd3",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Net (CNN) - Medium Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3a252",
   "metadata": {},
   "source": [
    "A simple Convolutional Neural Network with a single convolutional layer, followed by a max-pooling layer, and two dense layers for classification. Designed for 28x28 grayscale images. It has 692,352 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac88dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.reshape = layers.Reshape(CNN_INPUT_RESHAPE)\n",
    "        self.conv1 = layers.Conv2D(32, 3, activation='relu')\n",
    "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dense2 = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "        \n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.reshape(inputs)  # Add a channel dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, batch):\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            y_batch_pred = self(x_batch, training=True)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(\n",
    "                y_true=y_batch,\n",
    "                y_pred=y_batch_pred,\n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients to the model's trainable variables (update weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        #self.compiled_metrics.update_state(y_batch, y_batch_pred)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, dataset):\n",
    "\n",
    "        for batch in dataset:\n",
    "            self.step(batch)\n",
    "            \n",
    "    \n",
    "    def set_trainable_variables(self, trainable_vars):\n",
    "        \"\"\" Given `trainable_vars` set our `self.trainable_vars` \"\"\"\n",
    "        for model_var, var in zip(self.trainable_variables, trainable_vars):\n",
    "            model_var.assign(var)\n",
    "\n",
    "            \n",
    "    def trainable_vars_as_vector(self):\n",
    "        return tf.concat([tf.reshape(var, [-1]) for var in self.trainable_variables], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead5a6c",
   "metadata": {},
   "source": [
    "### Helper function to compile and return the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82ddd2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_and_built_cnn():\n",
    "    cnn = CNN()\n",
    "    \n",
    "    cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # we have softmax\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')]\n",
    "    )\n",
    "    \n",
    "    cnn.build(CNN_BATCH_INPUT)  # EMNIST dataset (None is used for batch size, as it varies)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f99e0",
   "metadata": {},
   "source": [
    "# Advanced Convolutional Neural Net (CNN) - Large Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259d325",
   "metadata": {},
   "source": [
    "A more complex Convolutional Neural Network with three sets of two convolutional layers, each followed by a max-pooling layer, and two dense layers with dropout for classification. Designed for 28x28 grayscale images. It has 2,592,202 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5705b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedCNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AdvancedCNN, self).__init__()\n",
    "        \n",
    "        self.reshape = layers.Reshape(CNN_INPUT_RESHAPE)\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool1 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv4 = layers.Conv2D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        \n",
    "        self.conv5 = layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')\n",
    "        self.conv6 = layers.Conv2D(256, kernel_size=3, activation='relu', padding='same')\n",
    "        self.max_pool3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(512, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.dense2 = layers.Dense(512, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "        self.dense3 = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.reshape(inputs)  # Add a channel dimension\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pool1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.max_pool2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.max_pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def step(self, batch):\n",
    "        \n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            y_batch_pred = self(x_batch, training=True)\n",
    "\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(\n",
    "                y_true=y_batch,\n",
    "                y_pred=y_batch_pred,\n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients to the model's trainable variables (update weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        #self.compiled_metrics.update_state(y_batch, y_batch_pred)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, dataset):\n",
    "\n",
    "        for batch in dataset:\n",
    "            self.step(batch)\n",
    "            \n",
    "    \n",
    "    def set_trainable_variables(self, trainable_vars):\n",
    "        \"\"\" Given `trainable_vars` set our `self.trainable_vars` \"\"\"\n",
    "        for model_var, var in zip(self.trainable_variables, trainable_vars):\n",
    "            model_var.assign(var)\n",
    "\n",
    "            \n",
    "    def trainable_vars_as_vector(self):\n",
    "        return tf.concat([tf.reshape(var, [-1]) for var in self.trainable_variables], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1289f0",
   "metadata": {},
   "source": [
    "### Helper function to compile and return the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a705fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_and_built_advanced_cnn():\n",
    "    advanced_cnn = AdvancedCNN()\n",
    "    \n",
    "    advanced_cnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # we have softmax\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')]\n",
    "    )\n",
    "    \n",
    "    advanced_cnn.build(CNN_BATCH_INPUT)  # EMNIST dataset (None is used for batch size, as it varies)\n",
    "    \n",
    "    return advanced_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c454f52",
   "metadata": {},
   "source": [
    "### Average NN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7cb196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_client_weights(client_models):\n",
    "    # client_weights[0] the trainable variables of Client 0 (a list of tf.Variable)\n",
    "    client_weights = [model.trainable_variables for model in client_models]\n",
    "\n",
    "    # concise solution. per layer. `layer_weight_tensors` corresponds to a list of tensors of a layer\n",
    "    avg_weights = [\n",
    "        tf.reduce_mean(layer_weight_tensors, axis=0)\n",
    "        for layer_weight_tensors in zip(*client_weights)\n",
    "    ]\n",
    "\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a13e6",
   "metadata": {},
   "source": [
    "# Functional Dynamic Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2a1b5",
   "metadata": {},
   "source": [
    "We follow the Functional Dynamic Averaging (FDA) scheme. Let the mean model be\n",
    "\n",
    "$$ \\overline{w_t} = \\frac{1}{k} \\sum_{i=1}^{k} w_t^{(i)} $$\n",
    "\n",
    "where $ w_t^{(i)} $ is the model at time $ t $ in some round in the $i$-th learner.\n",
    "\n",
    "Local models are trained independently and cooperatively and we want to monitor the Round Terminating Conditon (**RTC**):\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2  \\leq \\Theta $$\n",
    "\n",
    "where the left-hand side is the **model variance**, and threshold $\\Theta$ is a hyperparameter of the FDA, defined at the beginning of the round; it may change at each round. When the monitoring logic cannot guarantee the validity of RTC, the round terminates. All local models are pulled into `tff.SERVER`, and $\\bar{w_t}$ is set to their average. Then, another round begins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0ccbb",
   "metadata": {},
   "source": [
    "### Monitoring the RTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6e34b",
   "metadata": {},
   "source": [
    "FDA monitors the RTC by applying techniques from Functionary [Functional Geometric Averaging](http://users.softnet.tuc.gr/~minos/Papers/edbt19.pdf). We first restate the problem of monitoring RTC into the standard distributed stream monitoring formulation. Let\n",
    "\n",
    "$$ S(t) =  \\frac{1}{k} \\sum_{i=1}^{k} S_i(t) $$\n",
    "\n",
    "where $ S(t) \\in \\mathbb{R}^n $ be the \"global state\" of the system and $ S_i(t) \\in \\mathbb{R}^n $ the \"local states\". The goal is to monitor the threshold condition on the global state in the form $ F(S(t)) \\leq \\Theta $ where $ F : \\mathbb{R}^n \\to \\mathbb{R} $ a non-linear function. Let\n",
    "\n",
    "$$ \\Delta_t^{(i)} = w_t^{(i)} - w_{t_0}^{(i)} $$\n",
    "\n",
    "be the update at the $ i $-th learner, that is, the change to the local model at time $t$ since the beginning of the current round at time $t_0$. Let the average update be\n",
    "\n",
    "$$ \\overline{\\Delta_t} = \\frac{1}{k} \\sum_{i=1}^{k} \\Delta_t^{(i)} $$\n",
    "\n",
    "it follows that the variance can be written as\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} \\lVert w_t^{(i)} - \\overline{w_t} \\rVert_2^2 = \\Big( \\frac{1}{k} \\sum_{i=1}^{k} \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\Big) - \\lVert \\overline{\\Delta_t} \\rVert_2^2 $$\n",
    "\n",
    "So, conceptually, if we define\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\bf{x}\n",
    "         \\end{bmatrix}) = v - \\lVert \\bf{x} \\rVert_2^2 $$\n",
    "\n",
    "The RTC is equivalent to condition $$ F(S(t)) \\leq \\Theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2989181",
   "metadata": {},
   "source": [
    "### Server - Clients synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f00c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def synchronize(server_cnn, client_cnns):\n",
    "    # server average\n",
    "    server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "    \n",
    "    # synchronize clients\n",
    "    for client_cnn in client_cnns:\n",
    "        client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a0df4",
   "metadata": {},
   "source": [
    "## 1️⃣ Naive FDA\n",
    "\n",
    "In the naive approach, we eliminate the update vector from the local state (i.e. recuce the dimension to 0). Define local state as\n",
    "\n",
    "$$ S_i(t) = \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\in \\mathbb{R}$$ \n",
    "\n",
    "and the identity function\n",
    "\n",
    "$$ F(v) = v $$\n",
    "\n",
    "It is trivial that $ F(S(t)) \\leq \\Theta $ implies the RTC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6875d47",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0cf704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_naive(last_sync_cnn, client_cnn, client_dataset):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - last_sync_cnn.trainable_vars_as_vector()\n",
    "    \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    return Delta_i_euc_norm_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2013ea",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4780782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_naive(S):\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2c6b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_naive(server_cnn, client_cnns, federated_dataset,\n",
    "                                   num_epochs, theta, epoch_fda_steps):\n",
    "    \n",
    "    print(\"retracing naive\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_naive(S) <= theta:\n",
    "            S_i_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared = steps_naive(server_cnn, client_cnn, client_dataset)\n",
    "                S_i_clients.append(Delta_i_euc_norm_squared)\n",
    "                \n",
    "            S = tf.reduce_mean(S_i_clients)\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        Delta_i_clients = [\n",
    "            tf.subtract(client_cnn.trainable_vars_as_vector(), server_cnn.trainable_vars_as_vector()) \n",
    "            for client_cnn in client_cnns\n",
    "        ] #test\n",
    "        actual_S_2 = tf.reduce_sum(tf.square(tf.reduce_mean(Delta_i_clients, axis=0))) #test\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        tf.print(\"Naive Epoch count: \", epoch_count, \" Total fda steps: \", total_fda_steps, output_stream=sys.stdout)\n",
    "        tf.print(\"Est var: \", S, \" Actual S_2 (Assumed 0): \", actual_S_2, \" Actual var: \", variance(client_cnns, server_cnn), output_stream=sys.stdout)\n",
    "        tf.print(\"\\n\", output_stream=sys.stdout)\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6d3f1",
   "metadata": {},
   "source": [
    "## 2️⃣ Linear FDA\n",
    "\n",
    "In the linear case, we reduce the update vector to a scalar, $ \\xi \\Delta_t^{(i)} \\in \\mathbb{R}$, where $ \\xi $ is any unit vector.\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           \\xi \\Delta_t^{(i)}\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^2 $$\n",
    "\n",
    "Also, define \n",
    "\n",
    "$$ F(v, x) = v - x^2 $$\n",
    "\n",
    "The RTC is equivalent to condition \n",
    "\n",
    "$$ F(S(t)) \\leq \\Theta $$\n",
    "\n",
    "A random choice of $ \\xi $ is likely to perform poorly (terminate round prematurely), as it wil likely be close to orthogonal to $ \\overline{\\Delta_t} $. A good choice would be a vector $ \\xi $ correlated to $ \\overline{\\Delta_t} $. A heuristic choice is to take $ \\overline{\\Delta_{t_0}} $ (after scaling it to norm 1), i.e., the update vector right before the current round started. All nodes can estimate this without communication, as $ \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} $, the difference of the last two models pushed by the Server. Hence, \n",
    "\n",
    "$$ \\xi = \\frac{\\overline{w_{t_0}} - \\overline{w_{t_{-1}}}}{\\lVert \\overline{w_{t_0}} - \\overline{w_{t_{-1}}} \\rVert_2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbfb12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ksi_unit_fn(w_t0, w_tminus1):\n",
    "    \n",
    "    if tf.reduce_all(tf.equal(w_t0, w_tminus1)):\n",
    "        # if equal then ksi becomes a random vector (will only happen in round 1)\n",
    "        ksi = tf.random.normal(shape=w_t0.shape)\n",
    "    else:\n",
    "        ksi = w_t0 - w_tminus1\n",
    "\n",
    "    # Normalize and return\n",
    "    return tf.divide(ksi, tf.norm(ksi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b66f11",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0ad6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_linear(cnn_tminus, cnn_t0, client_cnn, client_dataset):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - cnn_t0.trainable_vars_as_vector()\n",
    "    \n",
    "    #||D(t)_i||^2 , shape = (1,) \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    # heuristic unit vector ksi\n",
    "    ksi = ksi_unit_fn(cnn_t0.trainable_vars_as_vector(), cnn_tminus.trainable_vars_as_vector())\n",
    "    \n",
    "    # ksi * Delta_i (* is dot) , shape = ()\n",
    "    ksi_Delta_i = tf.reduce_sum(tf.multiply(ksi, Delta_i))\n",
    "    \n",
    "    return Delta_i_euc_norm_squared, ksi_Delta_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c734b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d8dd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_linear(S_1, S_2):\n",
    "    return S_1 - S_2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e87dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_linear(previous_server_cnn, server_cnn, client_cnns, federated_dataset,\n",
    "                                   num_epochs, theta, epoch_fda_steps):\n",
    "    \n",
    "    print(\"retracing linear\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    S_2 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_linear(S_1, S_2) <= theta:\n",
    "            euc_norm_squared_clients = []\n",
    "            ksi_delta_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared, ksi_Delta_i = steps_linear(\n",
    "                    previous_server_cnn, server_cnn, client_cnn, client_dataset\n",
    "                )\n",
    "                \n",
    "                euc_norm_squared_clients.append(Delta_i_euc_norm_squared)\n",
    "                ksi_delta_clients.append(ksi_Delta_i)\n",
    "            \n",
    "            S_1 = tf.reduce_mean(euc_norm_squared_clients)\n",
    "            S_2 = tf.reduce_mean(ksi_delta_clients)\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        Delta_i_clients = [\n",
    "            tf.subtract(client_cnn.trainable_vars_as_vector(), server_cnn.trainable_vars_as_vector()) \n",
    "            for client_cnn in client_cnns\n",
    "        ] #test\n",
    "        actual_S_2 = tf.reduce_sum(tf.square(tf.reduce_mean(Delta_i_clients, axis=0))) #test\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # last server model (previous sync)\n",
    "        previous_server_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        tf.print(\"Linear Epoch count: \", epoch_count, \" Total fda steps: \", total_fda_steps, output_stream=sys.stdout)\n",
    "        tf.print(\"Est var: \", F_linear(S_1, S_2), \" Actual S_2: \", actual_S_2, \" Approx S_2: \", S_2**2, \" Actual var: \", variance(client_cnns, server_cnn), output_stream=sys.stdout)\n",
    "        tf.print(\"\\n\", output_stream=sys.stdout)\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "        S_2 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b28f1a",
   "metadata": {},
   "source": [
    "## 3️⃣ Sketch FDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a6434",
   "metadata": {},
   "source": [
    "An optimal estimator for $ \\lVert \\overline{\\Delta_t} \\rVert_2^2  $ can be obtained by employing AMS sketches. An AMS sketch of a vector $ v \\in \\mathbb{R}^M $ is a $ d \\times m $ real matrix\n",
    "\n",
    "$$ \\Xi = \\text{sk}(v) = \\begin{bmatrix}\n",
    "           \\Xi_1 \\\\\n",
    "           \\Xi_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           \\Xi_d \n",
    "         \\end{bmatrix} $$\n",
    "         \n",
    "where $ d \\cdot m \\ll M$. Operator sk($ \\cdot $) is linear, i.e., let $a, b \\in \\mathbb{R}$ and $v_1, v_2 \\in \\mathbb{R}^N$ then \n",
    "\n",
    "$$ \\text{sk}(a v_1 + b v_2) = a \\; \\text{sk}(v_1) + b \\; \\text{sk}(v_2)  $$\n",
    "\n",
    "Also, sk($ v $) can be computed in $ \\mathcal{O}(dN) $ steps.\n",
    "\n",
    "The interesting property of AMS sketches is that the function \n",
    "\n",
    "$$ M(sk(\\textbf{v})) = \\underset{i=1,...,d}{\\text{median}} \\; \\lVert \\boldsymbol{\\Xi}_i \\rVert_2^2  $$ \n",
    "\n",
    "is an excellent estimator of the Euclidean norm of **v** (within relative $\\epsilon$-error):\n",
    "\n",
    "$$ M(sk(\\textbf{v})) \\; \\in (1 \\pm \\epsilon) \\lVert \\textbf{v} \\rVert_2^2 \\; \\; \\text{with probability at least} \\; (1-\\delta) $$\n",
    "\n",
    "where $m = \\mathcal{O}(\\frac{1}{\\epsilon^2})$ and $d = \\mathcal{O}(\\log \\frac{1}{\\delta})$\n",
    "            \n",
    "Moreover, let $\\boldsymbol{\\Xi} \\in \\mathbb{R}^{d \\times m}$ and $ k \\in \\mathbb{R}$. It can be proven that\n",
    "\n",
    "$$ M( \\frac{1}{k} \\boldsymbol{\\Xi}) = \\frac{1}{k^2} M(\\boldsymbol{\\Xi}) $$\n",
    "\n",
    "Let's investigate a little further on how this helps us. The $i$-th client computes $ sk(\\Delta_t^{(i)}) $ and sends it to the server. Notice\n",
    "\n",
    "$$ M\\big(sk(\\Delta_t^{(1)}) + sk(\\Delta_t^{(2)}) + ... + sk(\\Delta_t^{(k)}) \\big) = M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big)$$\n",
    "\n",
    "Remember that\n",
    "\n",
    "$$ \\overline{\\boldsymbol{\\Delta}}_t = \\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} $$\n",
    "\n",
    "Then\n",
    "            \n",
    "$$ M\\Big( \\text{sk}\\big( \\overline{\\boldsymbol{\\Delta}}_t \\big) \\Big) = M\\Big( \\text{sk}\\big( \\frac{1}{k} \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) = \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) $$\n",
    "\n",
    "\n",
    "Which means that \n",
    "\n",
    "$$ \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\boldsymbol{\\Delta}_t^{(i)} \\big) \\Big) \\in (1 \\pm \\epsilon) \\lVert \\overline{\\boldsymbol{\\Delta}}_t \\rVert_2^2 \\; \\; \\text{w.p. at least} \\; (1-\\delta) $$\n",
    "\n",
    "In the monitoring process it is essential that we do not overestimate $ \\lVert \\overline{\\Delta_t} \\rVert_2^2 $ because we would then underestimate the variance which would potentially result in actual varience exceeding $ \\Theta$ without us noticing it. With this in mind,\n",
    "\n",
    "$$ \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big) \\leq (1+\\epsilon) \\lVert \\overline{\\Delta_t} \\rVert_2^2 \\quad \\text{with probability at least} \\; (1-\\delta)$$\n",
    "\n",
    "Which means\n",
    "\n",
    "$$ \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M\\Big( \\text{sk}\\big( \\sum_{i=1}^{k} \\Delta_t^{(i)} \\big) \\Big) \\leq \\lVert \\overline{\\Delta_t} \\rVert_2^2 \\quad \\text{with probability at least} \\; (1-\\delta)$$\n",
    "\n",
    "Hence, the Server's estimation of $ \\lVert \\overline{\\Delta_t} \\rVert_2^2 $ is\n",
    "\n",
    "$$ \\frac{1}{(1+\\epsilon)} \\frac{1}{k^2} M\\Big( sk(\\Delta_t^{(1)}) + sk(\\Delta_t^{(2)}) + ... + sk(\\Delta_t^{(k)}) \\big) \\Big) $$\n",
    "\n",
    "Define the local state to be \n",
    "\n",
    "$$ S_i(t) = \\begin{bmatrix}\n",
    "           \\lVert \\Delta_t^{(i)} \\rVert_2^2 \\\\\n",
    "           sk(\\Delta_t^{(i)})\n",
    "         \\end{bmatrix} \\in \\mathbb{R}^{1+d \\times m} \\quad \\text{and} \\quad\n",
    "         F(\\begin{bmatrix}\n",
    "           v \\\\\n",
    "           \\Xi\n",
    "         \\end{bmatrix}) = v - \\frac{1}{(1+\\epsilon)}  M(\\Xi) \\quad \\text{where} \\quad \\Xi = \\frac{1}{k} \\sum_{i=1}^{k} sk(\\Delta_t^{(i)}) $$\n",
    "\n",
    "It follows that $ F(S(t)) \\leq \\Theta $ implies that the variance is less or equal to $ \\Theta $ with probability at least $ 1-\\delta $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db738526",
   "metadata": {},
   "source": [
    "## AMS sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e3bfa",
   "metadata": {},
   "source": [
    "We use `ExtensionType` which is the way to go in order to avoid unecessary graph retracing when passing around `AmsSketch` type 'objects'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7f6b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.experimental import ExtensionType\n",
    "\n",
    "class AmsSketch(ExtensionType):\n",
    "    depth: int\n",
    "    width: int\n",
    "    F: tf.Tensor\n",
    "        \n",
    "        \n",
    "    def __init__(self, depth=7, width=1500):\n",
    "        self.depth = depth\n",
    "        self.width = width\n",
    "        self.F = tf.random.uniform(shape=(6, depth), minval=0, maxval=(1 << 31) - 1, dtype=tf.int32)\n",
    "\n",
    "        \n",
    "    @tf.function\n",
    "    def hash31(self, x, a, b):\n",
    "\n",
    "        r = a * x + b\n",
    "        fold = tf.bitwise.bitwise_xor(tf.bitwise.right_shift(r, 31), r)\n",
    "        return tf.bitwise.bitwise_and(fold, 2147483647)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def tensor_hash31(self, x, a, b): # GOOD\n",
    "        \"\"\" Assumed that x is tensor shaped (d,) , i.e., a vector (for example, indices, i.e., tf.range(d)) \"\"\"\n",
    "\n",
    "        # Reshape x to have an extra dimension, resulting in a shape of (k, 1)\n",
    "        x_reshaped = tf.expand_dims(x, axis=-1)\n",
    "\n",
    "        # shape=(`v_dim`, 7)\n",
    "        r = tf.multiply(a, x_reshaped) + b\n",
    "\n",
    "        fold = tf.bitwise.bitwise_xor(tf.bitwise.right_shift(r, 31), r)\n",
    "        \n",
    "        return tf.bitwise.bitwise_and(fold, 2147483647)\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def tensor_fourwise(self, x):\n",
    "        \"\"\" Assumed that x is tensor shaped (d,) , i.e., a vector (for example, indices, i.e., tf.range(d)) \"\"\"\n",
    "        # 1st use the tensor hash31\n",
    "        in1 = self.tensor_hash31(x, self.F[2], self.F[3])  # (`x_dim`, 7)\n",
    "        \n",
    "        # 2nd (notice we swap the first two params, no change really)\n",
    "        in2 = self.tensor_hash31(x, in1, self.F[4])  # (`x_dim`, 7)\n",
    "        \n",
    "        in3 = self.tensor_hash31(x, in2, self.F[5])  # (`x_dim`, 7)\n",
    "        \n",
    "        in4 = tf.bitwise.bitwise_and(in3, 32768)  # (`x_dim`, 7)\n",
    "        \n",
    "        return 2 * (tf.bitwise.right_shift(in4, 15)) - 1  # (`x_dim`, 7)\n",
    "        \n",
    "        \n",
    "    @tf.function\n",
    "    def fourwise(self, x):\n",
    "\n",
    "        result = 2 * (tf.bitwise.right_shift(tf.bitwise.bitwise_and(self.hash31(self.hash31(self.hash31(x, self.F[2], self.F[3]), x, self.F[4]), x, self.F[5]), 32768), 15)) - 1\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector(self, v):\n",
    "        \"\"\" Extremely efficient computation of sketch with only using tensors. \"\"\"\n",
    "        \n",
    "        sketch = tf.zeros(shape=(self.depth, self.width), dtype=tf.float32)\n",
    "        \n",
    "        len_v = v.shape[0]\n",
    "        \n",
    "        pos_tensor = self.tensor_hash31(tf.range(len_v), self.F[0], self.F[1]) % self.width\n",
    "        \n",
    "        v_expand = tf.expand_dims(v, axis=-1)\n",
    "        \n",
    "        deltas_tensor = tf.multiply(tf.cast(self.tensor_fourwise(tf.range(len_v)), dtype=tf.float32), v_expand)\n",
    "        \n",
    "        range_tensor = tf.range(self.depth)\n",
    "        \n",
    "        # Expand dimensions to create a 2D tensor with shape (1, depth)\n",
    "        range_tensor_expanded = tf.expand_dims(range_tensor, 0)\n",
    "\n",
    "        # Use tf.tile to repeat the range `len_v` times\n",
    "        repeated_range_tensor = tf.tile(range_tensor_expanded, [len_v, 1])\n",
    "        \n",
    "        # shape=(`len_v`, 7, 2)\n",
    "        indices = tf.stack([repeated_range_tensor, pos_tensor], axis=-1)\n",
    "        \n",
    "        sketch = tf.tensor_scatter_nd_add(sketch, indices, deltas_tensor)\n",
    "        \n",
    "        return sketch\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def sketch_for_vector2(self, v):\n",
    "        \"\"\" Bad implementation for tensorflow. \"\"\"\n",
    "\n",
    "        sketch = tf.zeros(shape=(self.depth, self.width), dtype=tf.float32)\n",
    "\n",
    "        for i in tf.range(tf.shape(v)[0], dtype=tf.int32):\n",
    "            pos = self.hash31(i, self.F[0], self.F[1]) % self.width\n",
    "            delta = tf.cast(self.fourwise(i), dtype=tf.float32) * v[i]\n",
    "            indices_to_update = tf.stack([tf.range(self.depth, dtype=tf.int32), pos], axis=1)\n",
    "            sketch = tf.tensor_scatter_nd_add(sketch, indices_to_update, delta)\n",
    "\n",
    "        return sketch\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def estimate_euc_norm_squared(sketch):\n",
    "\n",
    "        @tf.function\n",
    "        def _median(v):\n",
    "            \"\"\" Median of tensor `v` with shape=(n,). Note: Suboptimal O(nlogn) but it's ok bcz n = `depth`\"\"\"\n",
    "            length = tf.shape(v)[0]\n",
    "            sorted_v = tf.sort(v)\n",
    "            middle = length // 2\n",
    "\n",
    "            return tf.cond(\n",
    "                tf.equal(length % 2, 0),\n",
    "                lambda: (sorted_v[middle - 1] + sorted_v[middle]) / 2.0,\n",
    "                lambda: sorted_v[middle]\n",
    "            )\n",
    "\n",
    "        return _median(tf.reduce_sum(tf.square(sketch), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe92e3b",
   "metadata": {},
   "source": [
    "### Client Steps\n",
    "\n",
    "The number of steps depends on the dataset, i.e., `.take(num)` call on `tf.data.Dataset` creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34a74388",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def steps_sketch(last_sync_cnn, client_cnn, client_dataset, ams_sketch):\n",
    "    # number of steps depend on `.take()` from `dataset`\n",
    "    client_cnn.train(client_dataset)\n",
    "    \n",
    "    Delta_i = client_cnn.trainable_vars_as_vector() - last_sync_cnn.trainable_vars_as_vector()\n",
    "    \n",
    "    #||D(t)_i||^2 , shape = (1,) \n",
    "    Delta_i_euc_norm_squared = tf.reduce_sum(tf.square(Delta_i)) # ||D(t)_i||^2\n",
    "    \n",
    "    # sketch approx\n",
    "    sketch = ams_sketch.sketch_for_vector(Delta_i)\n",
    "    \n",
    "    return Delta_i_euc_norm_squared, sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ba6cf",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6b0043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_sketch(S_1, S_2, epsilon):\n",
    "    \"\"\" `S_1` is mean || ||^2 as usual, S_2 is the `Ξ` as defined in the theoretical analysis above \"\"\"\n",
    "    \n",
    "    return S_1 - (1. / (1. + epsilon)) * AmsSketch.estimate_euc_norm_squared(S_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd3e3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(S_2, epsilon):\n",
    "    \n",
    "    return (1. / (1. + epsilon)) * AmsSketch.estimate_euc_norm_squared(S_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9f520f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_federated_simulation_sketch(server_cnn, client_cnns, federated_dataset, num_epochs, \n",
    "                                    theta, epoch_fda_steps, ams_sketch, epsilon):\n",
    "    \n",
    "    print(\"retracing sketch\")\n",
    "    \n",
    "    total_rounds = 0\n",
    "    total_fda_steps = 0\n",
    "    \n",
    "    round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    epoch_count = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "    \n",
    "    S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "    S_2 = tf.zeros(shape=(ams_sketch.depth, ams_sketch.width), dtype=tf.float32)\n",
    "    \n",
    "    while epoch_count < num_epochs:\n",
    "        \n",
    "        while F_sketch(S_1, S_2, epsilon) <= theta:\n",
    "            euc_norm_squared_clients = []\n",
    "            sketch_clients = []\n",
    "\n",
    "            # client steps (number depends on `federated_dataset`, i.e., `.take(num)`)\n",
    "            for client_cnn, client_dataset in zip(client_cnns, federated_dataset):\n",
    "                Delta_i_euc_norm_squared, sketch = steps_sketch(\n",
    "                    server_cnn, client_cnn, client_dataset, ams_sketch\n",
    "                )\n",
    "                \n",
    "                euc_norm_squared_clients.append(Delta_i_euc_norm_squared)\n",
    "                sketch_clients.append(sketch)\n",
    "            \n",
    "            S_1 = tf.reduce_mean(euc_norm_squared_clients)\n",
    "            S_2 = tf.reduce_mean(sketch_clients, axis=0)  # shape=(`depth`, width`). See `Ξ` in theoretical analysis\n",
    "            \n",
    "            round_fda_steps += 1\n",
    "            total_fda_steps += 1\n",
    "            \n",
    "            if round_fda_steps == epoch_fda_steps:\n",
    "                epoch_count += 1\n",
    "                round_fda_steps = tf.constant(0, shape=(), dtype=tf.int32)\n",
    "                \n",
    "                if epoch_count == num_epochs:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        Delta_i_clients = [\n",
    "            tf.subtract(client_cnn.trainable_vars_as_vector(), server_cnn.trainable_vars_as_vector()) \n",
    "            for client_cnn in client_cnns\n",
    "        ] #test\n",
    "        actual_S_2 = tf.reduce_sum(tf.square(tf.reduce_mean(Delta_i_clients, axis=0))) #test\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # server average\n",
    "        server_cnn.set_trainable_variables(average_client_weights(client_cnns))\n",
    "        \n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        tf.print(\"Sketch Epoch count: \", epoch_count, \" Total fda steps: \", total_fda_steps, output_stream=sys.stdout)\n",
    "        #tf.print(\"Naive Epoch count: \", epoch_count, output_stream=sys.stdout)\n",
    "        tf.print(\"Est var: \", F_sketch(S_1, S_2, epsilon), \" Actual S_2: \", actual_S_2, \" Apprxo S_2\", t(S_2, epsilon),  \" Actual var: \", variance(client_cnns, server_cnn), output_stream=sys.stdout)\n",
    "        tf.print(\"\\n\", output_stream=sys.stdout)\n",
    "        \"\"\"------------------------------test--------------------------------------------\"\"\"\n",
    "        \n",
    "        # reset variance approx\n",
    "        S_1 = tf.constant(0., shape=(), dtype=tf.float32)\n",
    "        S_2 = tf.zeros(shape=(ams_sketch.depth, ams_sketch.width), dtype=tf.float32)\n",
    "\n",
    "        # synchronize clients\n",
    "        for client_cnn in client_cnns:\n",
    "            client_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "            \n",
    "        total_rounds += 1\n",
    "    \n",
    "    return total_rounds, total_fda_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d998e29",
   "metadata": {},
   "source": [
    "# Simulation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615142b0",
   "metadata": {},
   "source": [
    "###  Basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94d94680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_test(NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, BATCH_SIZE, \n",
    "               THETA, EPSILON, ams_sketch, client_slices_train, seed):\n",
    "    \"\"\" One test for Naive,Linear,Sketch. Returns metrics \"\"\"\n",
    "    \n",
    "    num_epochs = tf.constant(NUM_EPOCHS, shape=(), dtype=tf.int32)\n",
    "    theta = tf.constant(THETA, shape=(), dtype=tf.float32)\n",
    "    \n",
    "    # for sketch\n",
    "    epsilon = tf.constant(EPSILON, shape=(), dtype=tf.float32) # new\n",
    "    \n",
    "    \n",
    "    epoch_client_batches = (n_train / BATCH_SIZE) / NUM_CLIENTS\n",
    "    epoch_max_fda_steps = epoch_client_batches / NUM_STEPS_UNTIL_RTC_CHECK\n",
    "    epoch_max_fda_steps = tf.constant(int(epoch_max_fda_steps), shape=(), dtype=tf.int32)\n",
    "    \n",
    "    basic_test_metrics = []\n",
    "    \n",
    "    \"\"\" --------------- Naive ----------------------------------\"\"\"\n",
    "    \n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "\n",
    "    federated_dataset = create_federated_data(\n",
    "        client_slices_train=client_slices_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_buffer_size=int(n_train/NUM_CLIENTS),\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Models init - Sync\n",
    "\n",
    "    client_cnns = [\n",
    "        get_compiled_and_built_advanced_cnn() for _ in range(NUM_CLIENTS)\n",
    "    ]\n",
    "\n",
    "    server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "    \n",
    "    #synchronize before starting\n",
    "    synchronize(server_cnn, client_cnns)\n",
    "\n",
    "    # 3. Run \n",
    "\n",
    "    total_rounds, total_fda_steps = run_federated_simulation_naive(\n",
    "        server_cnn, \n",
    "        client_cnns, \n",
    "        federated_dataset, \n",
    "        num_epochs, \n",
    "        theta,\n",
    "        epoch_max_fda_steps\n",
    "    )\n",
    "    \n",
    "    # compute metrics\n",
    "    \n",
    "    _, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"naive\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc, \n",
    "        sketch_width=None, \n",
    "        sketch_depth=None\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "\n",
    "    del federated_dataset, server_cnn, client_cnns, total_rounds, total_fda_steps, acc\n",
    "    \n",
    "    \n",
    "    \"\"\" --------------- Linear ----------------------------------\"\"\"\n",
    "\n",
    "    # 1. tf.data.Dataset (we create it again because we want determinism)\n",
    "\n",
    "    federated_dataset = create_federated_data(\n",
    "        client_slices_train=client_slices_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_buffer_size=int(n_train/NUM_CLIENTS),\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Models init - Sync\n",
    "\n",
    "    client_cnns = [\n",
    "        get_compiled_and_built_advanced_cnn() for _ in range(NUM_CLIENTS)\n",
    "    ]\n",
    "\n",
    "    previous_server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "\n",
    "    server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "    \n",
    "    #synchronize before starting\n",
    "    synchronize(server_cnn, client_cnns)\n",
    "    previous_server_cnn.set_trainable_variables(server_cnn.trainable_variables)\n",
    "\n",
    "    # 3. Run \n",
    "\n",
    "    total_rounds, total_fda_steps = run_federated_simulation_linear(\n",
    "        previous_server_cnn,\n",
    "        server_cnn, \n",
    "        client_cnns, \n",
    "        federated_dataset, \n",
    "        num_epochs, \n",
    "        theta,\n",
    "        epoch_max_fda_steps\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 4. compute metrics\n",
    "    \n",
    "    loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"linear\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc, \n",
    "        sketch_width=None, \n",
    "        sketch_depth=None\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "\n",
    "    del federated_dataset, server_cnn, previous_server_cnn, client_cnns, total_rounds, total_fda_steps, acc\n",
    "\n",
    "    \n",
    "    \"\"\" ------------------------ Sketch ----------------------\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    federated_dataset = create_federated_data(\n",
    "        client_slices_train=client_slices_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle_buffer_size=int(n_train/NUM_CLIENTS),\n",
    "        num_steps_until_rtc_check=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 2. Models init - Sync\n",
    "\n",
    "    client_cnns = [\n",
    "        get_compiled_and_built_advanced_cnn() for _ in range(NUM_CLIENTS)\n",
    "    ]\n",
    "\n",
    "    server_cnn = get_compiled_and_built_advanced_cnn()\n",
    "    \n",
    "    #synchronize before starting\n",
    "    synchronize(server_cnn, client_cnns)\n",
    "\n",
    "    # 3. Run \n",
    "\n",
    "    total_rounds, total_fda_steps = run_federated_simulation_sketch(\n",
    "        server_cnn=server_cnn, \n",
    "        client_cnns=client_cnns, \n",
    "        federated_dataset=federated_dataset,\n",
    "        num_epochs=num_epochs, \n",
    "        theta=theta, \n",
    "        epoch_fda_steps=epoch_max_fda_steps, \n",
    "        ams_sketch=ams_sketch, \n",
    "        epsilon=epsilon\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 4. compute metrics\n",
    "    \n",
    "    loss, acc = server_cnn.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    metrics = create_metrics_dict(\n",
    "        fda_name=\"sketch\", \n",
    "        n_train=n_train, \n",
    "        dataset_name=\"EMNIST\", \n",
    "        input_pixels=784, \n",
    "        seed=seed, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        num_clients=NUM_CLIENTS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        steps_in_one_fda_step=NUM_STEPS_UNTIL_RTC_CHECK, \n",
    "        theta=THETA, \n",
    "        total_fda_steps=total_fda_steps.numpy(),\n",
    "        num_weights=count_weights(server_cnn),\n",
    "        total_rounds=total_rounds.numpy(), \n",
    "        final_accuracy=acc, \n",
    "        sketch_width=ams_sketch.width, \n",
    "        sketch_depth=ams_sketch.depth\n",
    "    )\n",
    "    \n",
    "    basic_test_metrics.append(metrics)\n",
    "\n",
    "    del federated_dataset, server_cnn, client_cnns, total_rounds, total_fda_steps, acc\n",
    "    \n",
    "    return basic_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f8f8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info_current_test(NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, THETA):\n",
    "    print()\n",
    "    print(f\"----------- Current Test --------------\")\n",
    "    print(f\"Num Epochs : {NUM_EPOCHS}\")\n",
    "    print(f\"Num Clients : {NUM_CLIENTS}\")\n",
    "    print(f\"Theta : {THETA}\")\n",
    "    print(f\"Number of steps until we check RTC : {NUM_STEPS_UNTIL_RTC_CHECK}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b2078c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt # new\n",
    "\n",
    "def run_tests(NUM_CLIENTS_LIST, NUM_EPOCHS_LIST, NUM_STEPS_UNTIL_RTC_CHECK_LIST,\n",
    "              BATCH_SIZE_LIST, THETA_LIST, SKETCH_DEPTH, SKETCH_WIDTH, SEED=None):\n",
    "    \n",
    "    \"\"\" --------------- Fixed configurations -------------------\"\"\"\n",
    "\n",
    "    ams_sketch = AmsSketch(\n",
    "        depth=SKETCH_DEPTH,\n",
    "        width=SKETCH_WIDTH\n",
    "    )\n",
    "\n",
    "    EPSILON = 1. / sqrt(SKETCH_WIDTH)\n",
    "    \n",
    "    \n",
    "    \"\"\" --------------- Metrics list ----------------------\"\"\"\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        \"\"\" --------------- Run tests -------------------\"\"\"\n",
    "        for NUM_CLIENTS in NUM_CLIENTS_LIST:\n",
    "            \n",
    "            client_slices_train = create_data_for_clients(NUM_CLIENTS)  # new sliced dataset (diff NUM_CLIENTS)\n",
    "            \n",
    "            for NUM_EPOCHS in NUM_EPOCHS_LIST:\n",
    "                \n",
    "                for NUM_STEPS_UNTIL_RTC_CHECK in NUM_STEPS_UNTIL_RTC_CHECK_LIST:\n",
    "                    \n",
    "                    for BATCH_SIZE in BATCH_SIZE_LIST:\n",
    "                        \n",
    "                        for THETA in THETA_LIST:\n",
    "                            \n",
    "                            print_info_current_test(NUM_EPOCHS, NUM_STEPS_UNTIL_RTC_CHECK, NUM_CLIENTS, THETA)\n",
    "                            \n",
    "                            basic_test_metrics = basic_test(\n",
    "                                NUM_EPOCHS=NUM_EPOCHS, \n",
    "                                NUM_STEPS_UNTIL_RTC_CHECK=NUM_STEPS_UNTIL_RTC_CHECK,\n",
    "                                NUM_CLIENTS=NUM_CLIENTS,\n",
    "                                BATCH_SIZE=BATCH_SIZE, \n",
    "                                THETA=THETA, \n",
    "                                EPSILON=EPSILON,\n",
    "                                ams_sketch=ams_sketch,\n",
    "                                client_slices_train=client_slices_train,\n",
    "                                seed=SEED\n",
    "                            )\n",
    "                            \n",
    "                            all_metrics.extend(basic_test_metrics)\n",
    "                            \n",
    "            del client_slices_train\n",
    "            \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"shit\")\n",
    "    \n",
    "    finally:\n",
    "        return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e61e5",
   "metadata": {},
   "source": [
    "# Run Simulation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a5b5b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Current Test --------------\n",
      "Num Epochs : 1\n",
      "Num Clients : 5\n",
      "Theta : 1.0\n",
      "Number of steps until we check RTC : 1\n",
      "----------------------------------------\n",
      "\n",
      "retracing naive\n",
      "Naive Epoch count:  0  Total fda steps:  6\n",
      "Est var:  1.12512529  Actual S_2 (Assumed 0):  0.233064085  Actual var:  0.892061234\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  12\n",
      "Est var:  1.09180915  Actual S_2 (Assumed 0):  0.219386309  Actual var:  0.872422874\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  20\n",
      "Est var:  1.20854175  Actual S_2 (Assumed 0):  0.272792488  Actual var:  0.935749412\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  27\n",
      "Est var:  1.15332496  Actual S_2 (Assumed 0):  0.26872316  Actual var:  0.884601772\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  33\n",
      "Est var:  1.36139154  Actual S_2 (Assumed 0):  0.299967855  Actual var:  1.06142366\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  40\n",
      "Est var:  1.13467848  Actual S_2 (Assumed 0):  0.292517304  Actual var:  0.842161179\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  47\n",
      "Est var:  1.28537  Actual S_2 (Assumed 0):  0.542372286  Actual var:  0.742997646\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  52\n",
      "Est var:  1.385831  Actual S_2 (Assumed 0):  0.734544873  Actual var:  0.651286244\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  56\n",
      "Est var:  1.33311212  Actual S_2 (Assumed 0):  0.495021433  Actual var:  0.838090718\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  60\n",
      "Est var:  1.64703107  Actual S_2 (Assumed 0):  0.561669409  Actual var:  1.08536148\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  63\n",
      "Est var:  1.18036056  Actual S_2 (Assumed 0):  0.38459003  Actual var:  0.795770526\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  66\n",
      "Est var:  1.30502617  Actual S_2 (Assumed 0):  0.445591182  Actual var:  0.859434783\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  69\n",
      "Est var:  1.46581447  Actual S_2 (Assumed 0):  0.685208  Actual var:  0.780606389\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  72\n",
      "Est var:  1.68832839  Actual S_2 (Assumed 0):  0.87646  Actual var:  0.81186837\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  75\n",
      "Est var:  1.51790226  Actual S_2 (Assumed 0):  0.611431718  Actual var:  0.906470478\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  78\n",
      "Est var:  1.42725527  Actual S_2 (Assumed 0):  0.552016735  Actual var:  0.875238538\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  81\n",
      "Est var:  1.25588858  Actual S_2 (Assumed 0):  0.533904433  Actual var:  0.721984148\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  84\n",
      "Est var:  1.1688906  Actual S_2 (Assumed 0):  0.534620047  Actual var:  0.634270549\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  87\n",
      "Est var:  1.19248235  Actual S_2 (Assumed 0):  0.641379654  Actual var:  0.551102757\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  90\n",
      "Est var:  1.10015857  Actual S_2 (Assumed 0):  0.532125473  Actual var:  0.568033159\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  93\n",
      "Est var:  1.07068884  Actual S_2 (Assumed 0):  0.531221688  Actual var:  0.539467096\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  96\n",
      "Est var:  1.04644954  Actual S_2 (Assumed 0):  0.533295274  Actual var:  0.513154387\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  100\n",
      "Est var:  1.56260812  Actual S_2 (Assumed 0):  0.70720911  Actual var:  0.855399\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  103\n",
      "Est var:  1.01594138  Actual S_2 (Assumed 0):  0.43595618  Actual var:  0.579985142\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  106\n",
      "Est var:  1.028458  Actual S_2 (Assumed 0):  0.443794399  Actual var:  0.58466351\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  109\n",
      "Est var:  1.0798192  Actual S_2 (Assumed 0):  0.464985728  Actual var:  0.614833474\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  112\n",
      "Est var:  1.18216634  Actual S_2 (Assumed 0):  0.499132097  Actual var:  0.683034122\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  115\n",
      "Est var:  1.23670638  Actual S_2 (Assumed 0):  0.522810221  Actual var:  0.713896155\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  118\n",
      "Est var:  1.16283834  Actual S_2 (Assumed 0):  0.474502474  Actual var:  0.688335776\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  121\n",
      "Est var:  1.17806613  Actual S_2 (Assumed 0):  0.46537593  Actual var:  0.712690175\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  124\n",
      "Est var:  1.20604253  Actual S_2 (Assumed 0):  0.478813052  Actual var:  0.727229416\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  127\n",
      "Est var:  1.21042335  Actual S_2 (Assumed 0):  0.464413136  Actual var:  0.746010184\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  130\n",
      "Est var:  1.12866092  Actual S_2 (Assumed 0):  0.42846781  Actual var:  0.700193048\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  133\n",
      "Est var:  1.05140805  Actual S_2 (Assumed 0):  0.379408  Actual var:  0.672000051\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  136\n",
      "Est var:  1.01557648  Actual S_2 (Assumed 0):  0.402989328  Actual var:  0.612587094\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  139\n",
      "Est var:  1.06612957  Actual S_2 (Assumed 0):  0.441775888  Actual var:  0.624353647\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  143\n",
      "Est var:  1.63917828  Actual S_2 (Assumed 0):  0.632621408  Actual var:  1.00655687\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  146\n",
      "Est var:  1.01320517  Actual S_2 (Assumed 0):  0.388312697  Actual var:  0.624892533\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  150\n",
      "Est var:  1.41784692  Actual S_2 (Assumed 0):  0.540890694  Actual var:  0.876956165\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  154\n",
      "Est var:  1.53941691  Actual S_2 (Assumed 0):  0.59954834  Actual var:  0.93986845\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  158\n",
      "Est var:  1.45861554  Actual S_2 (Assumed 0):  0.528957129  Actual var:  0.929658413\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  162\n",
      "Est var:  1.30516684  Actual S_2 (Assumed 0):  0.455645055  Actual var:  0.849521816\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  166\n",
      "Est var:  1.22797704  Actual S_2 (Assumed 0):  0.407593906  Actual var:  0.820383072\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  170\n",
      "Est var:  1.21848297  Actual S_2 (Assumed 0):  0.410929203  Actual var:  0.807553649\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  174\n",
      "Est var:  1.18994796  Actual S_2 (Assumed 0):  0.336755037  Actual var:  0.853193\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  178\n",
      "Est var:  1.14707816  Actual S_2 (Assumed 0):  0.359713256  Actual var:  0.787364841\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  182\n",
      "Est var:  1.10436225  Actual S_2 (Assumed 0):  0.344136804  Actual var:  0.760225415\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  186\n",
      "Est var:  1.05375791  Actual S_2 (Assumed 0):  0.327128857  Actual var:  0.726629138\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  190\n",
      "Est var:  1.0983485  Actual S_2 (Assumed 0):  0.321323  Actual var:  0.777025461\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  194\n",
      "Est var:  1.17218471  Actual S_2 (Assumed 0):  0.318959206  Actual var:  0.853225589\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  198\n",
      "Est var:  1.22503507  Actual S_2 (Assumed 0):  0.328971684  Actual var:  0.896063447\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  202\n",
      "Est var:  1.36826301  Actual S_2 (Assumed 0):  0.496233523  Actual var:  0.872029424\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  206\n",
      "Est var:  1.21754575  Actual S_2 (Assumed 0):  0.364215016  Actual var:  0.853330612\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  210\n",
      "Est var:  1.07405519  Actual S_2 (Assumed 0):  0.377396435  Actual var:  0.69665879\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  215\n",
      "Est var:  1.5131892  Actual S_2 (Assumed 0):  0.486403942  Actual var:  1.02678525\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  219\n",
      "Est var:  1.14715958  Actual S_2 (Assumed 0):  0.345464706  Actual var:  0.801695\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  223\n",
      "Est var:  1.22850335  Actual S_2 (Assumed 0):  0.375066191  Actual var:  0.853437245\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  227\n",
      "Est var:  1.09693074  Actual S_2 (Assumed 0):  0.302096  Actual var:  0.794834673\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  231\n",
      "Est var:  1.01117539  Actual S_2 (Assumed 0):  0.286076128  Actual var:  0.725099206\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  235\n",
      "Est var:  1.06050944  Actual S_2 (Assumed 0):  0.281137258  Actual var:  0.779372096\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  239\n",
      "Est var:  1.06307447  Actual S_2 (Assumed 0):  0.300278366  Actual var:  0.762796223\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est var:  1.13054979  Actual S_2 (Assumed 0):  0.295838803  Actual var:  0.834711075\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  247\n",
      "Est var:  1.05150127  Actual S_2 (Assumed 0):  0.304031193  Actual var:  0.74747\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  251\n",
      "Est var:  1.01593435  Actual S_2 (Assumed 0):  0.291358829  Actual var:  0.724575639\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  256\n",
      "Est var:  1.37802553  Actual S_2 (Assumed 0):  0.376792222  Actual var:  1.00123334\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  261\n",
      "Est var:  1.15755975  Actual S_2 (Assumed 0):  0.315811723  Actual var:  0.841748059\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  266\n",
      "Est var:  1.11141384  Actual S_2 (Assumed 0):  0.323850393  Actual var:  0.787563443\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  271\n",
      "Est var:  1.31579185  Actual S_2 (Assumed 0):  0.348878413  Actual var:  0.966913342\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  275\n",
      "Est var:  1.02803791  Actual S_2 (Assumed 0):  0.25144279  Actual var:  0.776595175\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  280\n",
      "Est var:  1.47949052  Actual S_2 (Assumed 0):  0.376719534  Actual var:  1.10277104\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  285\n",
      "Est var:  1.35208535  Actual S_2 (Assumed 0):  0.347192377  Actual var:  1.00489306\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  290\n",
      "Est var:  1.16521049  Actual S_2 (Assumed 0):  0.297033906  Actual var:  0.86817646\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  295\n",
      "Est var:  1.13213956  Actual S_2 (Assumed 0):  0.292838335  Actual var:  0.839301288\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  300\n",
      "Est var:  1.27390921  Actual S_2 (Assumed 0):  0.323939413  Actual var:  0.949969649\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  304\n",
      "Est var:  1.12400448  Actual S_2 (Assumed 0):  0.30479154  Actual var:  0.819212914\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  309\n",
      "Est var:  1.40441585  Actual S_2 (Assumed 0):  0.337893665  Actual var:  1.06652212\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  314\n",
      "Est var:  1.44161892  Actual S_2 (Assumed 0):  0.36381  Actual var:  1.07780898\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  318\n",
      "Est var:  1.15171456  Actual S_2 (Assumed 0):  0.256008089  Actual var:  0.895706475\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  322\n",
      "Est var:  1.04313266  Actual S_2 (Assumed 0):  0.208569765  Actual var:  0.834562957\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  327\n",
      "Est var:  1.27541232  Actual S_2 (Assumed 0):  0.302547961  Actual var:  0.972864449\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  332\n",
      "Est var:  1.22992408  Actual S_2 (Assumed 0):  0.283584923  Actual var:  0.94633925\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  337\n",
      "Est var:  1.11176836  Actual S_2 (Assumed 0):  0.260530084  Actual var:  0.85123837\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  342\n",
      "Est var:  1.46923685  Actual S_2 (Assumed 0):  0.34071818  Actual var:  1.1285187\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  346\n",
      "Est var:  1.08514047  Actual S_2 (Assumed 0):  0.288860351  Actual var:  0.79628\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  350\n",
      "Est var:  1.02774405  Actual S_2 (Assumed 0):  0.258580357  Actual var:  0.769163728\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  355\n",
      "Est var:  1.28968143  Actual S_2 (Assumed 0):  0.35988307  Actual var:  0.929798424\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  360\n",
      "Est var:  1.46416581  Actual S_2 (Assumed 0):  0.410495788  Actual var:  1.05366993\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  365\n",
      "Est var:  1.20853531  Actual S_2 (Assumed 0):  0.309491515  Actual var:  0.899043858\n",
      "\n",
      "\n",
      "Naive Epoch count:  0  Total fda steps:  370\n",
      "Est var:  1.40775645  Actual S_2 (Assumed 0):  0.386920452  Actual var:  1.02083588\n",
      "\n",
      "\n",
      "Naive Epoch count:  1  Total fda steps:  375\n",
      "Est var:  1.28994679  Actual S_2 (Assumed 0):  0.295579523  Actual var:  0.994367242\n",
      "\n",
      "\n",
      "retracing linear\n",
      "Linear Epoch count:  0  Total fda steps:  7\n",
      "Est var:  1.31666768  Actual S_2:  0.289037943  Approx S_2:  7.1018242e-08  Actual var:  1.02763009\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  15\n",
      "Est var:  1.05100942  Actual S_2:  0.253107756  Approx S_2:  0.128728896  Actual var:  0.926630497\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  28\n",
      "Est var:  1.07469141  Actual S_2:  0.293777674  Approx S_2:  0.124283962  Actual var:  0.90519774\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  41\n",
      "Est var:  1.02448356  Actual S_2:  0.172390312  Approx S_2:  0.0352822132  Actual var:  0.887375474\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  54\n",
      "Est var:  1.04837692  Actual S_2:  0.261971235  Approx S_2:  0.0495317951  Actual var:  0.8359375\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  64\n",
      "Est var:  1.15718865  Actual S_2:  0.225644737  Approx S_2:  0.0453358889  Actual var:  0.976879895\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  73\n",
      "Est var:  1.01332176  Actual S_2:  0.297425687  Approx S_2:  0.106935956  Actual var:  0.822832108\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  80\n",
      "Est var:  1.0680896  Actual S_2:  0.493477762  Approx S_2:  0.184768319  Actual var:  0.759380281\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  84\n",
      "Est var:  1.23135757  Actual S_2:  0.624437869  Approx S_2:  0.0865233392  Actual var:  0.693443\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  88\n",
      "Est var:  1.56569481  Actual S_2:  0.906876  Approx S_2:  0.175934613  Actual var:  0.834753394\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  91\n",
      "Est var:  1.2305063  Actual S_2:  0.535518885  Approx S_2:  0.269575596  Actual var:  0.964562893\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  94\n",
      "Est var:  1.42537189  Actual S_2:  0.594940245  Approx S_2:  0.295887  Actual var:  1.12631869\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  97\n",
      "Est var:  1.40837049  Actual S_2:  0.835142553  Approx S_2:  0.516085863  Actual var:  1.08931398\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  100\n",
      "Est var:  1.89443064  Actual S_2:  1.39025438  Approx S_2:  0.530610144  Actual var:  1.03478646\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  103\n",
      "Est var:  1.54089475  Actual S_2:  0.776650071  Approx S_2:  0.344050825  Actual var:  1.10829568\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  106\n",
      "Est var:  1.70112646  Actual S_2:  0.907172143  Approx S_2:  0.331713825  Actual var:  1.12566817\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  109\n",
      "Est var:  1.35059071  Actual S_2:  0.940430284  Approx S_2:  0.638112485  Actual var:  1.04827273\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  112\n",
      "Est var:  1.09856081  Actual S_2:  0.920111656  Approx S_2:  0.611373842  Actual var:  0.789822757\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  115\n",
      "Est var:  1.11796129  Actual S_2:  1.07158613  Approx S_2:  0.656507373  Actual var:  0.702882349\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  118\n",
      "Est var:  1.09755182  Actual S_2:  1.04306531  Approx S_2:  0.63652724  Actual var:  0.691013515\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  121\n",
      "Est var:  1.29757011  Actual S_2:  0.751305342  Approx S_2:  0.320865393  Actual var:  0.86713016\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  124\n",
      "Est var:  1.27931869  Actual S_2:  0.729028702  Approx S_2:  0.360224158  Actual var:  0.910514057\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  127\n",
      "Est var:  1.16225672  Actual S_2:  0.661381125  Approx S_2:  0.386258274  Actual var:  0.887133777\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  131\n",
      "Est var:  1.5421977  Actual S_2:  0.97113961  Approx S_2:  0.678624868  Actual var:  1.24968266\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  135\n",
      "Est var:  1.52113426  Actual S_2:  0.874715209  Approx S_2:  0.454212636  Actual var:  1.10063171\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  139\n",
      "Est var:  1.40977478  Actual S_2:  0.765769  Approx S_2:  0.413361  Actual var:  1.05736685\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  143\n",
      "Est var:  1.4464618  Actual S_2:  0.725898087  Approx S_2:  0.316734761  Actual var:  1.03729856\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  147\n",
      "Est var:  1.22535348  Actual S_2:  0.631749034  Approx S_2:  0.42025423  Actual var:  1.01385868\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  151\n",
      "Est var:  1.27635467  Actual S_2:  0.668119729  Approx S_2:  0.382200718  Actual var:  0.990435719\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  155\n",
      "Est var:  1.33344889  Actual S_2:  0.699117064  Approx S_2:  0.315880567  Actual var:  0.950212359\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  159\n",
      "Est var:  1.27032042  Actual S_2:  0.608787894  Approx S_2:  0.298120558  Actual var:  0.95965308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  163\n",
      "Est var:  1.19282794  Actual S_2:  0.629924059  Approx S_2:  0.450803339  Actual var:  1.01370728\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  167\n",
      "Est var:  1.35701525  Actual S_2:  0.594142616  Approx S_2:  0.253714234  Actual var:  1.01658678\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  171\n",
      "Est var:  1.44583714  Actual S_2:  0.570670485  Approx S_2:  0.271186441  Actual var:  1.14635313\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  175\n",
      "Est var:  1.40221298  Actual S_2:  0.546267509  Approx S_2:  0.323462218  Actual var:  1.17940772\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  179\n",
      "Est var:  1.49695194  Actual S_2:  0.503790677  Approx S_2:  0.242132083  Actual var:  1.23529339\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  183\n",
      "Est var:  1.40783167  Actual S_2:  0.444066525  Approx S_2:  0.222416669  Actual var:  1.18618178\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  187\n",
      "Est var:  1.36541867  Actual S_2:  0.462408066  Approx S_2:  0.257920563  Actual var:  1.16093111\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  191\n",
      "Est var:  1.27417088  Actual S_2:  0.436730355  Approx S_2:  0.226756915  Actual var:  1.06419754\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  195\n",
      "Est var:  1.12935913  Actual S_2:  0.41016373  Approx S_2:  0.248090118  Actual var:  0.967285514\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  199\n",
      "Est var:  1.07816494  Actual S_2:  0.373423427  Approx S_2:  0.210663646  Actual var:  0.915405095\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  203\n",
      "Est var:  1.14968467  Actual S_2:  0.427405059  Approx S_2:  0.179489568  Actual var:  0.90176928\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  207\n",
      "Est var:  1.23467731  Actual S_2:  0.43967694  Approx S_2:  0.113275208  Actual var:  0.908275485\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  211\n",
      "Est var:  1.30193329  Actual S_2:  0.470953465  Approx S_2:  0.126034394  Actual var:  0.957014263\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  215\n",
      "Est var:  1.33728218  Actual S_2:  0.453741372  Approx S_2:  0.121138908  Actual var:  1.0046798\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  219\n",
      "Est var:  1.13095176  Actual S_2:  0.401764482  Approx S_2:  0.214808539  Actual var:  0.943995953\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  223\n",
      "Est var:  1.10640144  Actual S_2:  0.372095764  Approx S_2:  0.195875093  Actual var:  0.930180669\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  228\n",
      "Est var:  1.36002254  Actual S_2:  0.456411809  Approx S_2:  0.218722045  Actual var:  1.12233281\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  233\n",
      "Est var:  1.41191602  Actual S_2:  0.467552602  Approx S_2:  0.190610588  Actual var:  1.13497376\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  237\n",
      "Est var:  1.05300844  Actual S_2:  0.374055982  Approx S_2:  0.124352179  Actual var:  0.803304672\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  241\n",
      "Est var:  1.02180672  Actual S_2:  0.292655647  Approx S_2:  0.143068492  Actual var:  0.872219682\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  245\n",
      "Est var:  1.27207553  Actual S_2:  0.387265682  Approx S_2:  0.113542393  Actual var:  0.99835223\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  250\n",
      "Est var:  1.2466476  Actual S_2:  0.381033033  Approx S_2:  0.20168668  Actual var:  1.06730127\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  255\n",
      "Est var:  1.20634127  Actual S_2:  0.374733686  Approx S_2:  0.153487891  Actual var:  0.985095382\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  260\n",
      "Est var:  1.24398303  Actual S_2:  0.360810727  Approx S_2:  0.125864297  Actual var:  1.00903642\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  265\n",
      "Est var:  1.24687362  Actual S_2:  0.375977278  Approx S_2:  0.191662252  Actual var:  1.06255853\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  270\n",
      "Est var:  1.37002361  Actual S_2:  0.401309818  Approx S_2:  0.113919653  Actual var:  1.08263338\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  275\n",
      "Est var:  1.40250015  Actual S_2:  0.390781969  Approx S_2:  0.150890619  Actual var:  1.16260886\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  280\n",
      "Est var:  1.24058485  Actual S_2:  0.423927903  Approx S_2:  0.243991151  Actual var:  1.06064808\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  285\n",
      "Est var:  1.13768041  Actual S_2:  0.321069  Approx S_2:  0.116293959  Actual var:  0.932905376\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  290\n",
      "Est var:  1.11385429  Actual S_2:  0.337126791  Approx S_2:  0.171015188  Actual var:  0.947742581\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  295\n",
      "Est var:  1.13422441  Actual S_2:  0.320989609  Approx S_2:  0.094862178  Actual var:  0.908096969\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  300\n",
      "Est var:  1.11727488  Actual S_2:  0.320961982  Approx S_2:  0.131394014  Actual var:  0.927707\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  305\n",
      "Est var:  1.11106896  Actual S_2:  0.332350403  Approx S_2:  0.12750715  Actual var:  0.906225801\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  311\n",
      "Est var:  1.26172221  Actual S_2:  0.393359661  Approx S_2:  0.180271238  Actual var:  1.04863393\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  317\n",
      "Est var:  1.31613445  Actual S_2:  0.429602325  Approx S_2:  0.157974958  Actual var:  1.04450727\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  322\n",
      "Est var:  1.23938084  Actual S_2:  0.313489348  Approx S_2:  0.0691195726  Actual var:  0.995011\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  328\n",
      "Est var:  1.29700041  Actual S_2:  0.349657416  Approx S_2:  0.174704805  Actual var:  1.12204778\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  334\n",
      "Est var:  1.19809198  Actual S_2:  0.313488215  Approx S_2:  0.0883558467  Actual var:  0.972959816\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  340\n",
      "Est var:  1.14094603  Actual S_2:  0.278904557  Approx S_2:  0.109669954  Actual var:  0.971711457\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  346\n",
      "Est var:  1.33771789  Actual S_2:  0.324899852  Approx S_2:  0.123948023  Actual var:  1.13676608\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  351\n",
      "Est var:  1.0774498  Actual S_2:  0.271779776  Approx S_2:  0.120111749  Actual var:  0.925781608\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  356\n",
      "Est var:  1.14938331  Actual S_2:  0.301635861  Approx S_2:  0.079526  Actual var:  0.927273393\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  361\n",
      "Est var:  1.05462754  Actual S_2:  0.3045398  Approx S_2:  0.146334723  Actual var:  0.896422505\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  367\n",
      "Est var:  1.29151905  Actual S_2:  0.32192713  Approx S_2:  0.129608944  Actual var:  1.09920084\n",
      "\n",
      "\n",
      "Linear Epoch count:  0  Total fda steps:  372\n",
      "Est var:  1.11085486  Actual S_2:  0.287399888  Approx S_2:  0.0994668603  Actual var:  0.922921777\n",
      "\n",
      "\n",
      "Linear Epoch count:  1  Total fda steps:  375\n",
      "Est var:  0.430401385  Actual S_2:  0.110072  Approx S_2:  0.054203134  Actual var:  0.374532521\n",
      "\n",
      "\n",
      "retracing sketch\n",
      "WARNING:tensorflow:From /home/miketheologitis/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Sketch Epoch count:  0  Total fda steps:  7\n",
      "Est var:  1.02309227  Actual S_2:  0.27971077  Apprxo S_2 0.269894332  Actual var:  1.01327574\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  16\n",
      "Est var:  1.06323814  Actual S_2:  0.293219328  Apprxo S_2 0.286100835  Actual var:  1.0561198\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  29\n",
      "Est var:  1.00603175  Actual S_2:  0.318090886  Apprxo S_2 0.31078136  Actual var:  0.998722255\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  44\n",
      "Est var:  1.09951138  Actual S_2:  0.280120492  Apprxo S_2 0.274089754  Actual var:  1.09348083\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  59\n",
      "Est var:  1.04729271  Actual S_2:  0.338920176  Apprxo S_2 0.343019783  Actual var:  1.05139232\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  73\n",
      "Est var:  1.09522665  Actual S_2:  0.321805  Apprxo S_2 0.318897843  Actual var:  1.09231937\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  81\n",
      "Est var:  1.0284791  Actual S_2:  0.605074942  Apprxo S_2 0.592899859  Actual var:  1.01630378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  86\n",
      "Est var:  1.41371346  Actual S_2:  0.796175718  Apprxo S_2 0.782193  Actual var:  1.39973092\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  90\n",
      "Est var:  1.45827866  Actual S_2:  0.85400188  Apprxo S_2 0.841438532  Actual var:  1.44571531\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  93\n",
      "Est var:  1.17379832  Actual S_2:  0.770785153  Apprxo S_2 0.757650197  Actual var:  1.16066337\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  96\n",
      "Est var:  1.12000501  Actual S_2:  1.08014572  Apprxo S_2 1.07288706  Actual var:  1.11274624\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  99\n",
      "Est var:  1.48924184  Actual S_2:  0.719597697  Apprxo S_2 0.708323479  Actual var:  1.4779675\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  102\n",
      "Est var:  1.20193946  Actual S_2:  0.679900706  Apprxo S_2 0.683169723  Actual var:  1.2052083\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  105\n",
      "Est var:  1.03881717  Actual S_2:  0.58905381  Apprxo S_2 0.594625413  Actual var:  1.04438901\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  109\n",
      "Est var:  1.23597288  Actual S_2:  0.88888669  Apprxo S_2 0.888088  Actual var:  1.23517418\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  113\n",
      "Est var:  1.01783979  Actual S_2:  0.886636138  Apprxo S_2 0.882961154  Actual var:  1.01416481\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  117\n",
      "Est var:  1.04717827  Actual S_2:  1.05323  Apprxo S_2 1.01125216  Actual var:  1.00520039\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  121\n",
      "Est var:  1.08553863  Actual S_2:  0.993727744  Apprxo S_2 0.960831821  Actual var:  1.05264258\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  125\n",
      "Est var:  1.1756711  Actual S_2:  0.9352386  Apprxo S_2 0.893818676  Actual var:  1.13425088\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  129\n",
      "Est var:  1.2209996  Actual S_2:  1.0221554  Apprxo S_2 0.986192584  Actual var:  1.1850369\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  133\n",
      "Est var:  1.39204884  Actual S_2:  1.02983117  Apprxo S_2 0.971455336  Actual var:  1.33367276\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  137\n",
      "Est var:  1.40997386  Actual S_2:  0.992182314  Apprxo S_2 0.952179253  Actual var:  1.36997068\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  141\n",
      "Est var:  1.37889361  Actual S_2:  0.926674247  Apprxo S_2 0.908762276  Actual var:  1.3609817\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  145\n",
      "Est var:  1.38685036  Actual S_2:  0.86627686  Apprxo S_2 0.850626767  Actual var:  1.37120008\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  149\n",
      "Est var:  1.3797102  Actual S_2:  0.910249352  Apprxo S_2 0.873091221  Actual var:  1.34255183\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  153\n",
      "Est var:  1.36728013  Actual S_2:  0.87980783  Apprxo S_2 0.841540217  Actual var:  1.32901263\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  157\n",
      "Est var:  1.24822474  Actual S_2:  0.790317774  Apprxo S_2 0.769589424  Actual var:  1.22749639\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  161\n",
      "Est var:  1.13229358  Actual S_2:  0.761249542  Apprxo S_2 0.754253864  Actual var:  1.12529778\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  165\n",
      "Est var:  1.09181356  Actual S_2:  0.668534815  Apprxo S_2 0.638255775  Actual var:  1.06153464\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  169\n",
      "Est var:  1.09426045  Actual S_2:  0.594701052  Apprxo S_2 0.567777395  Actual var:  1.0673368\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  173\n",
      "Est var:  1.05950141  Actual S_2:  0.570369244  Apprxo S_2 0.543697894  Actual var:  1.03283\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  177\n",
      "Est var:  1.14293766  Actual S_2:  0.516383111  Apprxo S_2 0.485709548  Actual var:  1.11226428\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  181\n",
      "Est var:  1.29392505  Actual S_2:  0.591193259  Apprxo S_2 0.567015648  Actual var:  1.2697475\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  185\n",
      "Est var:  1.1784699  Actual S_2:  0.582627296  Apprxo S_2 0.567916632  Actual var:  1.16375923\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  189\n",
      "Est var:  1.02733457  Actual S_2:  0.483262032  Apprxo S_2 0.464414805  Actual var:  1.00848734\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  194\n",
      "Est var:  1.35651743  Actual S_2:  0.689501882  Apprxo S_2 0.661230922  Actual var:  1.32824636\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  198\n",
      "Est var:  1.0178194  Actual S_2:  0.417955279  Apprxo S_2 0.409330428  Actual var:  1.00919461\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  203\n",
      "Est var:  1.45242536  Actual S_2:  0.686395049  Apprxo S_2 0.661941648  Actual var:  1.42797208\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  208\n",
      "Est var:  1.32503474  Actual S_2:  0.569951653  Apprxo S_2 0.559251785  Actual var:  1.31433487\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  213\n",
      "Est var:  1.25261462  Actual S_2:  0.64046  Apprxo S_2 0.632876158  Actual var:  1.24503076\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  218\n",
      "Est var:  1.23800325  Actual S_2:  0.601475  Apprxo S_2 0.575371623  Actual var:  1.2119\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  222\n",
      "Est var:  1.01760113  Actual S_2:  0.400662184  Apprxo S_2 0.383443713  Actual var:  1.00038266\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  227\n",
      "Est var:  1.26935697  Actual S_2:  0.544576705  Apprxo S_2 0.518340766  Actual var:  1.24312115\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  232\n",
      "Est var:  1.03784025  Actual S_2:  0.427926272  Apprxo S_2 0.421806574  Actual var:  1.03172052\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  237\n",
      "Est var:  1.1386404  Actual S_2:  0.437119186  Apprxo S_2 0.436754376  Actual var:  1.1382755\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  242\n",
      "Est var:  1.17202  Actual S_2:  0.416926503  Apprxo S_2 0.407445639  Actual var:  1.16253901\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  247\n",
      "Est var:  1.24762547  Actual S_2:  0.516757905  Apprxo S_2 0.488186121  Actual var:  1.21905363\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  252\n",
      "Est var:  1.13760352  Actual S_2:  0.456477  Apprxo S_2 0.442092836  Actual var:  1.12321961\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  257\n",
      "Est var:  1.03860688  Actual S_2:  0.402373821  Apprxo S_2 0.388153136  Actual var:  1.02438605\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  263\n",
      "Est var:  1.26146305  Actual S_2:  0.454037905  Apprxo S_2 0.432891935  Actual var:  1.24031711\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  269\n",
      "Est var:  1.11268187  Actual S_2:  0.440450758  Apprxo S_2 0.444117248  Actual var:  1.11634827\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  275\n",
      "Est var:  1.2241323  Actual S_2:  0.408067733  Apprxo S_2 0.407773316  Actual var:  1.22383785\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  281\n",
      "Est var:  1.24621391  Actual S_2:  0.396571189  Apprxo S_2 0.391411036  Actual var:  1.24105382\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  287\n",
      "Est var:  1.32805908  Actual S_2:  0.411547601  Apprxo S_2 0.380113125  Actual var:  1.29662454\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  293\n",
      "Est var:  1.22510183  Actual S_2:  0.397331357  Apprxo S_2 0.377828926  Actual var:  1.20559943\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  299\n",
      "Est var:  1.01625264  Actual S_2:  0.32407558  Apprxo S_2 0.313849807  Actual var:  1.00602686\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  305\n",
      "Est var:  1.22195017  Actual S_2:  0.439333797  Apprxo S_2 0.433268666  Actual var:  1.21588492\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  311\n",
      "Est var:  1.14637733  Actual S_2:  0.395286918  Apprxo S_2 0.384366393  Actual var:  1.1354568\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  317\n",
      "Est var:  1.20284581  Actual S_2:  0.43094337  Apprxo S_2 0.417301208  Actual var:  1.1892035\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  323\n",
      "Est var:  1.30051696  Actual S_2:  0.402043194  Apprxo S_2 0.39137736  Actual var:  1.28985107\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  329\n",
      "Est var:  1.15720451  Actual S_2:  0.404477447  Apprxo S_2 0.39618814  Actual var:  1.14891517\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  335\n",
      "Est var:  1.12204027  Actual S_2:  0.365148872  Apprxo S_2 0.35585168  Actual var:  1.11274302\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  341\n",
      "Est var:  1.22383237  Actual S_2:  0.410421669  Apprxo S_2 0.40814206  Actual var:  1.22155261\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  346\n",
      "Est var:  1.01963782  Actual S_2:  0.338742405  Apprxo S_2 0.334050953  Actual var:  1.01494622\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  352\n",
      "Est var:  1.33099496  Actual S_2:  0.40955019  Apprxo S_2 0.398341566  Actual var:  1.31978655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  358\n",
      "Est var:  1.09660304  Actual S_2:  0.366713822  Apprxo S_2 0.354406714  Actual var:  1.08429587\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  364\n",
      "Est var:  1.11743975  Actual S_2:  0.385153145  Apprxo S_2 0.37044251  Actual var:  1.10272908\n",
      "\n",
      "\n",
      "Sketch Epoch count:  0  Total fda steps:  371\n",
      "Est var:  1.27010417  Actual S_2:  0.447949409  Apprxo S_2 0.434430182  Actual var:  1.256585\n",
      "\n",
      "\n",
      "Sketch Epoch count:  1  Total fda steps:  375\n",
      "Est var:  0.45269835  Actual S_2:  0.135831401  Apprxo S_2 0.133349821  Actual var:  0.45021677\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_metrics = run_tests(\n",
    "    NUM_CLIENTS_LIST=[5],\n",
    "    NUM_EPOCHS_LIST=[1],\n",
    "    NUM_STEPS_UNTIL_RTC_CHECK_LIST=[1],\n",
    "    BATCH_SIZE_LIST=[32],\n",
    "    THETA_LIST=[1.],\n",
    "    SKETCH_DEPTH=7,\n",
    "    SKETCH_WIDTH=1700,\n",
    "    SEED=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93aa398",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    \n",
    "1. Check why no change in accuracy between steps. Do the updates happen at all? What the fuck is going on here?\n",
    "2. Check accuracy final\n",
    "3. `synchronize` retracing\n",
    "\n",
    "5. Approach on sketch should be `reduce_mean`, change it in PA-I.\n",
    "6. Approach on global tests `for` loop PA-I\n",
    "7. remove `one` as a `tf.constant(1)` PA-I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4b2b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da382e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_results/advanced_cnn_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e3cd8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fda_name': 'naive',\n",
       "  'theta': 1.0,\n",
       "  'dataset_name': 'EMNIST',\n",
       "  'input_pixels': 784,\n",
       "  'n_train': 60000,\n",
       "  'num_weights': 2592202,\n",
       "  'seed': 7,\n",
       "  'epochs': 1,\n",
       "  'num_clients': 5,\n",
       "  'batch_size': 32,\n",
       "  'steps_in_one_fda_step': 1,\n",
       "  'sketch_width': None,\n",
       "  'sketch_depth': None,\n",
       "  'one_sample_bytes': 3140,\n",
       "  'training_dataset_bytes': 188400000,\n",
       "  'model_bytes': 10368808,\n",
       "  'local_state_bytes': 4,\n",
       "  'final_accuracy': 0.9605000019073486,\n",
       "  'total_fda_steps': 375,\n",
       "  'total_steps': 375,\n",
       "  'total_rounds': 90,\n",
       "  'model_bytes_exchanged': 9331927200,\n",
       "  'monitoring_bytes_exchanged': 7500,\n",
       "  'total_communication_bytes': 9331934700,\n",
       "  'trained_in_bytes': 188400000},\n",
       " {'fda_name': 'linear',\n",
       "  'theta': 1.0,\n",
       "  'dataset_name': 'EMNIST',\n",
       "  'input_pixels': 784,\n",
       "  'n_train': 60000,\n",
       "  'num_weights': 2592202,\n",
       "  'seed': 7,\n",
       "  'epochs': 1,\n",
       "  'num_clients': 5,\n",
       "  'batch_size': 32,\n",
       "  'steps_in_one_fda_step': 1,\n",
       "  'sketch_width': None,\n",
       "  'sketch_depth': None,\n",
       "  'one_sample_bytes': 3140,\n",
       "  'training_dataset_bytes': 188400000,\n",
       "  'model_bytes': 10368808,\n",
       "  'local_state_bytes': 8,\n",
       "  'final_accuracy': 0.9692999720573425,\n",
       "  'total_fda_steps': 375,\n",
       "  'total_steps': 375,\n",
       "  'total_rounds': 77,\n",
       "  'model_bytes_exchanged': 7983982160,\n",
       "  'monitoring_bytes_exchanged': 15000,\n",
       "  'total_communication_bytes': 7983997160,\n",
       "  'trained_in_bytes': 188400000},\n",
       " {'fda_name': 'sketch',\n",
       "  'theta': 1.0,\n",
       "  'dataset_name': 'EMNIST',\n",
       "  'input_pixels': 784,\n",
       "  'n_train': 60000,\n",
       "  'num_weights': 2592202,\n",
       "  'seed': 7,\n",
       "  'epochs': 1,\n",
       "  'num_clients': 5,\n",
       "  'batch_size': 32,\n",
       "  'steps_in_one_fda_step': 1,\n",
       "  'sketch_width': 1700,\n",
       "  'sketch_depth': 7,\n",
       "  'one_sample_bytes': 3140,\n",
       "  'training_dataset_bytes': 188400000,\n",
       "  'model_bytes': 10368808,\n",
       "  'local_state_bytes': 47604,\n",
       "  'final_accuracy': 0.972599983215332,\n",
       "  'total_fda_steps': 375,\n",
       "  'total_steps': 375,\n",
       "  'total_rounds': 69,\n",
       "  'model_bytes_exchanged': 7154477520,\n",
       "  'monitoring_bytes_exchanged': 89257500,\n",
       "  'total_communication_bytes': 7243735020,\n",
       "  'trained_in_bytes': 188400000}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1435c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tff-py39] *",
   "language": "python",
   "name": "conda-env-tff-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
