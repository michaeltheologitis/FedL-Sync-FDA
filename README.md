# Abstract

The ever-growing volume and decentralized nature of data, coupled with the need to harness it and extract knowledge, have led to the extensive use of distributed deep learning (DDL) techniques for training. These techniques rely on local training performed at distributed nodes using locally collected data, followed by a periodic synchronization process that combines these models to create a unified global model. However, the frequent synchronization of deep learning models, encompassing millions to many billions of parameters, creates a communication bottleneck, severely hindering scalability. Worse yet, DDL algorithms typically waste valuable bandwidth and render themselves less practical in bandwidth-constrained federated settings by relying on overly simplistic, periodic, and rigid synchronization schedules. These inefficiencies make the training process increasingly impractical as they demand excessive time for data communication. To address these shortcomings, we propose Federated Dynamic Averaging (FDA), a communication-efficient DDL strategy that dynamically triggers synchronization based on the value of the model variance. In essence, the costly synchronization step is triggered only if the local models---initialized from a common global model after each synchronization---have significantly diverged. This decision is facilitated by the transmission of a small local state from each distributed node. Through extensive experiments across a wide range of learning tasks we demonstrate that FDA reduces communication cost by orders of magnitude, compared to both traditional and cutting-edge communication-efficient algorithms. Additionally, we show that FDA maintains robust performance across diverse data heterogeneity settings.

# Cite
```python
@inproceedings{theologitis2025fda,
  author       = {Michail Theologitis and
                  Georgios Frangias and
                  Georgios Anestis and
                  Vasilis Samoladas and
                  Antonios Deligiannakis},
  title        = {Communication-Efficient Distributed Deep Learning via Federated Dynamic
                  Averaging},
  booktitle    = {Proceedings 28th International Conference on Extending Database Technology,
                  {EDBT} 2025, Barcelona, Spain, March 25-28, 2025},
  pages        = {411--424},
  publisher    = {OpenProceedings.org},
  year         = {2025},
  url          = {https://doi.org/10.48786/edbt.2025.33},
  doi          = {10.48786/EDBT.2025.33}
}
```

# Paper Experiments

## Create Enviroment
**TensorFlow** version: **v2.15**
```bash
conda create -n tf-2.15 python==3.9
conda activate tf-2.15
```

If you are planning on using GPU:
```bash
pip install --extra-index-url https://pypi.nvidia.com tensorrt-bindings==8.6.1 tensorrt-libs==8.6.1
pip install -U tensorflow[and-cuda]==2.15.0 pandas pyarrow
```

If you are planning on **not** using GPU:
```bash
pip install tensorflow==2.15.0 pandas pyarrow
```

## Create Experiments
We provide the workflow to run all of our experiments. Note that they required 200K GPU hours so hopefully there are many available GPUs.

We will create one json file, `0.json`, encompassing all 1434 unique experiments of the paper.
1. **LeNet-5 - MNIST**: `612` unique simulations.
2. **VGG16\* - MNIST**: `612` unique simulations.
3. **DenseNet121 - CIFAR-10**: `96` unique simulations.
4. **DenseNet201 - CIFAR-10**: `96` unique simulations.
5. **ConvNeXtLarge - CIFAR-100** (fine-tuning): `18` unique simulations.

```bash
bash paper_experiments.sh
```

## Clone repo
Clone and materialize LFS files (deep pre-trained models ~2GB).
```bash
git clone https://github.com/miketheologitis/FedL-Sync-FDA
cd FedL-Sync-FDA/
git lfs fetch --all
git lfs pull
```

## Run Experiments
```bash
python -m local_simulator --n_sims 1434 --comb_file_id 0 --n_gpus 2
```




# Example WorkFlow
1. Using the `create_combinations.py` script, create all the experiments you want to run.
2. Simulate the experiments you created utilizing GPUs, in **SLURM**, in a local machine, or in a server cluster provided they are visible to `nvidia-smi`. For the `Kafka` WorkFlow go to the end of this README.

## 1. Combination Script

Go to project directory `/FedL-Sync-FDA` and create a combinations file:
```bash
python -m fdavg.utils.create_combinations --fda linear sketch --nn DenseNet121 --ds_name CIFAR10 --b 32 --e 100 --th 350 400 --num_clients 5 10 15 20 --comb_file_id 0
```
**Output:**
```
OK! Created 16 combinations in 0.json, i.e., `n_sims` = 16.
```
**Help**:
```shell
python -m fdavg.utils.create_combinations --help
```

## 2. Simulation
Be careful to use the same `--comb_file_id` for both scripts and use the same number of simulations (`--n_sims`) 
as the number of combinations generated by the combinations script. 
\
\
When an experiment finishes you will see the `.csv` or `.parquet` file in `/metrics/tmp/epoch_metrcis/`.
### Local
If `--n_gpus` is given and no GPUs are found, we continue on CPUs.

**Example**: Go to project directory `/FedL-Sync-FDA` and run the following command:
```bash
python -m local_simulator --n_gpus 2 --n_sims 12 --comb_file_id 0
```
**Tip**: Press `enter` to send as many simulations on each GPU as possible. In parallel, monitor the GPUs with 
```nvidia-smi``` so you know that there is available RAM (wait 5 minutes so that each simulation has enough time to 
allocate the needed RAM).

**Help**:
```shell
python -m local_simulator --help
```
You will see the `.out` and `.err` files in `/metrics/tmp/local_out/` with the following convention: `cX_simY.out`, where 
`X` is the `comb_file_id` and `Y` is the simulation ID (out of `n_sims`).

### SLURM

**Example**: Go to project directory `/FedL-Sync-FDA` and run the following command:
```shell
python -m slurm_submitter --gpu_mem 5120 --gpus_per_node 2 --sims_per_gpu 2 --mem_per_sim 10240 --cpus_per_sim 2 --nodes_per_submit 2 --n_sims 16 --comb_file_id 0 --walltime 24:00:00
```
**Help**:
```shell
python -m slurm_submitter --help
```
You will see the `.out` and `.err` files in `/metrics/tmp/slurm_out/`.

## 3. Results

After each simulation ends, a `.parquet` or `.csv` file will be created in `/FedL-Sync-FDA/metrics/tmp/epoch_metrics`. Then,
we must move it to `/FedL-Sync-FDA/metrics/epoch_metrics` where all the metrics are kept.

If the files are `.parquet`:
```bash
cp /metrics/tmp/epoch_metrics/* /metrics/epoch_metrics
```
```bash
rm /metrics/tmp/epoch_metrics/*
```

If the files are `.csv`:
```bash
cd /metrics/
```
```bash
python from_csv_to_parquet.py
```
```bash
rm /metrics/tmp/epoch_metrics/*
```
Then, run the notebook `/FedL-Sync-FDA/notebooks/data_analysis/epoch_metrics_analysis.ipynb` to see the results (plots) in
`/FedL-Sync-FDA/metrics/plots`.
