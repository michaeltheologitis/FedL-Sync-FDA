{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "996933fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "45873535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 100\n",
    "d = 10\n",
    "noise_factor = 0.01\n",
    "\n",
    "# Create (noisy) testing data for binary classification.\n",
    "X, y = make_classification(\n",
    "    n_samples=n, \n",
    "    n_features=d,\n",
    "    n_informative=d,\n",
    "    n_redundant=0, \n",
    "    n_classes=2,\n",
    "    class_sep=-1,\n",
    "    flip_y=noise_factor\n",
    ")\n",
    "\n",
    "# We will work with label values -1, +1 and not 0, +1 (convert)\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9fc3d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to TensorFlow tensors\n",
    "X_train_tensor = tf.constant(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.constant(y_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "y_test_tensor = tf.constant(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f99a6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the X and y tensors into a single dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train_tensor))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_tensor, y_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "963ea1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 5\n",
    "SHUFFLE_BUFFER = 1\n",
    "PREFETCH_BUFFER = 2\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8da7bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the training dataset\n",
    "preprocessed_train_dataset = preprocess(train_dataset)\n",
    "\n",
    "# Preprocess the testing dataset\n",
    "preprocessed_test_dataset = preprocess(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0a0d62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_pa1(x_batch, loss, C):\n",
    "        return tf.minimum(C, tf.divide(loss, tf.reduce_sum(tf.square(x_batch), axis=1)))\n",
    "\n",
    "def _train_on_batch(weights, batch, C, t_fn):\n",
    "        x_batch, y_batch = batch\n",
    "        \n",
    "        # predict y\n",
    "        y_predict_batch = tf.sign(tf.reduce_sum(tf.multiply(weights, x_batch), axis=0))\n",
    "        # suffer loss\n",
    "        loss = tf.maximum(0, tf.subtract(1, tf.multiply(y_predict_batch, tf.reduce_sum(tf.multiply(weights, x_batch), axis=0))))\n",
    "        # update 1. Set\n",
    "        t = t_fn(loss, x_batch, C)\n",
    "        # update 1. Update\n",
    "        weights = tf.reduce_mean(tf.multiply(tf.expand_dims(tf.multiply(t, y_batch), axis=1), x_batch), axis=0)\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2159b2a0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [1,10], In[1]: [5,10] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(d,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      3\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(preprocessed_test_dataset))\n\u001b[0;32m----> 5\u001b[0m \u001b[43m_train_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_pa1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[153], line 10\u001b[0m, in \u001b[0;36m_train_on_batch\u001b[0;34m(weights, batch, C, t_fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m y_predict_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msign(tf\u001b[38;5;241m.\u001b[39mtensordot(x_batch, weights, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# suffer loss\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, tf\u001b[38;5;241m.\u001b[39msubtract(\u001b[38;5;241m1\u001b[39m, tf\u001b[38;5;241m.\u001b[39mmultiply(y, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# update 1. Set\u001b[39;00m\n\u001b[1;32m     12\u001b[0m t \u001b[38;5;241m=\u001b[39m t_fn(loss, x, C)\n",
      "File \u001b[0;32m~/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [1,10], In[1]: [5,10] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "model = tf.zeros(shape=(d,), dtype=tf.float32)\n",
    "\n",
    "batch = next(iter(preprocessed_test_dataset))\n",
    "\n",
    "_train_on_batch(model, batch, 0.01, t_pa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "51c50e68",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [10] vs. [5] [Op:RealDiv]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[204], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, tf\u001b[38;5;241m.\u001b[39msubtract(\u001b[38;5;241m1\u001b[39m, tf\u001b[38;5;241m.\u001b[39mmultiply(y_predict_batch, tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39mmultiply(model, x_batch), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# update 1. Set\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mminimum(\u001b[38;5;241m0.01\u001b[39m, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# update 1. Update\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mmultiply(tf\u001b[38;5;241m.\u001b[39mexpand_dims(tf\u001b[38;5;241m.\u001b[39mmultiply(t, y_batch), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), x_batch), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [10] vs. [5] [Op:RealDiv]"
     ]
    }
   ],
   "source": [
    "# predict y\n",
    "y_predict_batch = tf.sign(tf.reduce_sum(tf.multiply(model, x_batch), axis=0))\n",
    "# suffer loss\n",
    "loss = tf.maximum(0, tf.subtract(1, tf.multiply(y_predict_batch, tf.reduce_sum(tf.multiply(model, x_batch), axis=0))))\n",
    "\n",
    "# update 1. Set\n",
    "t = tf.minimum(0.01, tf.divide(loss, tf.reduce_sum(tf.square(x_batch), axis=1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# update 1. Update\n",
    "model = tf.reduce_mean(tf.multiply(tf.expand_dims(tf.multiply(t, y_batch), axis=1), x_batch), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d382198f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([40.169853, 26.641253, 52.715706, 23.691893, 49.462044],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.square(x_batch), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b47276d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1fc42013",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [10] vs. [5] [Op:RealDiv]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [10] vs. [5] [Op:RealDiv]"
     ]
    }
   ],
   "source": [
    "tf.divide(loss, tf.reduce_sum(tf.square(x_batch), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8abf2e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[2.7227235e+00, 7.8841519e-01, 3.3305879e+00, 2.5806174e+00,\n",
       "        2.8236456e+00, 2.1111879e+01, 2.4590995e+00, 7.8797305e-01,\n",
       "        2.4283993e+00, 1.1365116e+00],\n",
       "       [3.9215775e+00, 1.5762302e+00, 6.6056151e+00, 1.1644359e-01,\n",
       "        5.6876745e+00, 9.0521502e-01, 4.3535156e+00, 2.0891049e+00,\n",
       "        1.0273392e+00, 3.5853818e-01],\n",
       "       [9.7422391e-01, 1.2565459e-01, 3.8885303e-02, 9.2241801e-02,\n",
       "        1.5771321e+01, 2.3519553e-01, 3.0575569e+00, 8.9186935e+00,\n",
       "        1.3531401e+01, 9.9705334e+00],\n",
       "       [8.3538862e-03, 5.7021980e+00, 1.4575380e+00, 7.8815589e+00,\n",
       "        3.7042454e-02, 1.1443394e-03, 4.7315583e-01, 7.8459969e+00,\n",
       "        1.4129360e-04, 2.8476232e-01],\n",
       "       [5.3385362e-02, 2.4166159e-01, 2.2355444e+00, 6.5107121e+00,\n",
       "        8.3945742e+00, 2.3514077e-02, 2.4994141e+01, 4.3666854e+00,\n",
       "        2.2740734e+00, 3.6775467e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91d808",
   "metadata": {},
   "source": [
    "## PA-Classiers (binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e391eb",
   "metadata": {},
   "source": [
    "![PA](images/PA_binary_classifiers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "70cd53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_train(initial_weights, batch, C):\n",
    "    def t_pa(loss, x):\n",
    "        return tf.divide(loss, tf.tensordot(x, x, axes=1))\n",
    "    def t_pa1(x_batch, loss, C):\n",
    "        return tf.minimum(C, tf.divide(loss, tf.reduce_sum(tf.square(x_batch), axis=1)))\n",
    "    def t_pa2(loss, x, C):\n",
    "        return tf.divide(loss, tf.add(tf.tensordot(x, x, axes=1), tf.divide(1, tf.multiply(2, C))))\n",
    "\n",
    "    def _train_on_batch(weights, batch, C, t_fn):\n",
    "        x_batch, y_batch = batch\n",
    "        \n",
    "        # predict y\n",
    "        y_predict_batch = tf.sign(tf.tensordot(x_batch, weights, axes=1))\n",
    "        # suffer loss\n",
    "        loss = tf.maximum(0, tf.subtract(1, tf.multiply(y, tf.tensordot(x_batch, model, axes=1))))\n",
    "        # update 1. Set\n",
    "        t = t_fn(loss, x, C)\n",
    "        # update 1. Update\n",
    "        weights = tf.reduce_mean(tf.multiply(tf.expand_dims(tf.multiply(t, y_batch), axis=1), x_batch), axis=0)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    return _train_on_batch(initial_weights, batch, C, t_pa1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "996bc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def accuracy_test(weights, test_dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in iter(test_dataset):\n",
    "        for x, y in zip(batch[0], batch[1]):\n",
    "            y_predict = tf.sign(tf.tensordot(weights, x, axes=1))\n",
    "            if y_predict.numpy() == y.numpy(): correct += 1\n",
    "            total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "043a3f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [100] vs. [10] [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[184], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(preprocessed_test_dataset):\n\u001b[0;32m----> 7\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[183], line 23\u001b[0m, in \u001b[0;36mbatch_train\u001b[0;34m(initial_weights, batch, C)\u001b[0m\n\u001b[1;32m     19\u001b[0m     weights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mmultiply(tf\u001b[38;5;241m.\u001b[39mexpand_dims(tf\u001b[38;5;241m.\u001b[39mmultiply(t, y_batch), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), x_batch), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_train_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_pa1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[183], line 15\u001b[0m, in \u001b[0;36mbatch_train.<locals>._train_on_batch\u001b[0;34m(weights, batch, C, t_fn)\u001b[0m\n\u001b[1;32m     13\u001b[0m y_predict_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msign(tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39mmultiply(model, x_batch), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# suffer loss\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, tf\u001b[38;5;241m.\u001b[39msubtract(\u001b[38;5;241m1\u001b[39m, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# update 1. Set\u001b[39;00m\n\u001b[1;32m     17\u001b[0m t \u001b[38;5;241m=\u001b[39m t_fn(loss, x, C)\n",
      "File \u001b[0;32m~/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tff-py39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [100] vs. [10] [Op:Mul]"
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 -r 1\n",
    "\n",
    "model = tf.zeros(shape=(d,), dtype=tf.float32)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch in iter(preprocessed_test_dataset):\n",
    "        model = batch_train(model, batch, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test(model, preprocessed_test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tff-py39] *",
   "language": "python",
   "name": "conda-env-tff-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
